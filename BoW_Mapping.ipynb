{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "d22uW_pcdDeE"
   },
   "source": [
    "## Installing necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "# Dependencies\n",
    "pip install docx2txt\n",
    "pip install strsimpy\n",
    "pip install python-docx\n",
    "pip install pandas\n",
    "\n",
    "if ls docx2csv >/dev/null 2>&1; then\n",
    "    echo \"docx2csv exists.\"\n",
    "else\n",
    "    echo \"Folder does not exist. Cloning docx2csv.\"\n",
    "    git clone https://github.com/ivbeg/docx2csv.git\n",
    "fi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "source .env\n",
    "cd docx2csv && echo \"$PASSWORD\" | sudo -S python3 setup.py install"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----- TEST DATA INPUT -----\n",
    "\n",
    "# Computer Science Test Data.\n",
    "# CURRENT_MAPPING=\"Lists_ComputerScience.docx\"\n",
    "# ORIGINAL_MAPPING=\"Original-Mapping-ComputerScience.csv\"\n",
    "\n",
    "# InformationSecurity Test Data.\n",
    "CURRENT_MAPPING=\"Lists_MonashEngineering.docx\"\n",
    "ORIGINAL_MAPPING=\"Original-Mapping-MonashEngineering.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bpNT07kAXk01"
   },
   "outputs": [],
   "source": [
    "# importing necessary libraries\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "A_eRLnUVbzGk"
   },
   "outputs": [],
   "source": [
    "# extract tables from word document\n",
    "from docx2csv import extract_tables, extract\n",
    "tables = extract_tables(CURRENT_MAPPING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3ypxqU-pbzC7"
   },
   "outputs": [],
   "source": [
    "from docx import Document\n",
    "document = Document(CURRENT_MAPPING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "C3UwEI5hby9k"
   },
   "outputs": [],
   "source": [
    "def read_docx_table(document,table_num):\n",
    "  table = document.tables[table_num-1]\n",
    "  data = [[cell.text for cell in row.cells] for row in table.rows]\n",
    "  df = pd.DataFrame(data)\n",
    "  return df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "VugavvsYYLPG"
   },
   "source": [
    "## PLO TABLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "7ZpBU1LeXqiG",
    "outputId": "f021d7e4-aebe-430c-9b07-ad265896aff9"
   },
   "outputs": [],
   "source": [
    "# Creating a dataframe for PLOs and it will accept 'n' number of PLOs\n",
    "table_num=1\n",
    "df = read_docx_table(document,table_num)\n",
    "df.head(n=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "U28-N6sjbQkc"
   },
   "outputs": [],
   "source": [
    "q1 = df.copy()\n",
    "q1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BzoHBy9RXqlF"
   },
   "outputs": [],
   "source": [
    "# assigning count vectorizer\n",
    "count_vectorizer = CountVectorizer(stop_words='english', min_df=0.005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xUftpLLZXqoV"
   },
   "outputs": [],
   "source": [
    "# Remove integers\n",
    "\n",
    "# Data preprocessing for PLO dataframe\n",
    "q1[1] = q1[1].str.lower()\n",
    "corpus = q1[1].tolist()\n",
    "corpii = count_vectorizer.fit_transform(corpus)\n",
    "corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "n20hDeKkQX9t",
    "outputId": "eac9548d-3148-4260-8b80-17f19079e3bd"
   },
   "outputs": [],
   "source": [
    "corpii"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mySN4lHeXqrV",
    "outputId": "7eb5f419-6cd3-467b-d40c-f7930edd9b61"
   },
   "outputs": [],
   "source": [
    "# extracting features names from PLO table\n",
    "feature_names = count_vectorizer.get_feature_names_out()\n",
    "feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oRxKuvJkQ0Mk",
    "outputId": "80f0fee1-eebe-4487-dfd9-946ea7c8b797"
   },
   "outputs": [],
   "source": [
    "len(feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0K-RSe6xXqul"
   },
   "outputs": [],
   "source": [
    "# Converting features to vector form and create a dataframe\n",
    "X1 = pd.DataFrame(corpii.toarray(), columns=feature_names)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "i8mtCAEZYPIe"
   },
   "source": [
    "## CLO TABLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "clX4_RyxXqxF"
   },
   "outputs": [],
   "source": [
    "# Creating a dataframe for CLOs and it will accept 'n' number of CLOs\n",
    "table_num=2\n",
    "df1 = read_docx_table(document,table_num)\n",
    "p1 = df1.copy()\n",
    "p1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gWp0jpPVXq0F"
   },
   "outputs": [],
   "source": [
    "# Data preprocessing for CLO dataframe\n",
    "p1[1] = p1[1].str.lower()\n",
    "corpus11 = p1[1].tolist()\n",
    "corpii11 = count_vectorizer.fit_transform(corpus11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1Zg_ZIN7Xq3e",
    "outputId": "6e4c8724-732a-4399-c269-259f23c56e15"
   },
   "outputs": [],
   "source": [
    "# extracting features names from CLO table\n",
    "feature_names1 = count_vectorizer.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tCXVAkg3mSgL",
    "outputId": "ad41e6d3-8677-4673-9531-71f6d9e78ee0"
   },
   "outputs": [],
   "source": [
    "len(feature_names1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PPI9g37tXq6l"
   },
   "outputs": [],
   "source": [
    "# Converting features to vector form and create a dataframe\n",
    "X2 = pd.DataFrame(corpii11.toarray(), columns=feature_names1)\n",
    "X2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 299
    },
    "id": "U5C18Iysmlbb",
    "outputId": "f18da737-d812-4a7d-8763-fe816dec3e3c"
   },
   "outputs": [],
   "source": [
    "X2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b6W1EAfRXq91"
   },
   "outputs": [],
   "source": [
    "# adding column index to the CLO table\n",
    "U2 = pd.concat([df1[0], X2], axis=1)\n",
    "U2.set_index(0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 331
    },
    "id": "1RAJJ1I_mxiL",
    "outputId": "bc32ea6a-e597-4362-d2fa-aff4d2bbb47d"
   },
   "outputs": [],
   "source": [
    "U2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PwmpreCjXrBN"
   },
   "outputs": [],
   "source": [
    "# adding column index to the PLO table\n",
    "U1 = pd.concat([df[0], X1], axis=1)\n",
    "U1.set_index(0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 362
    },
    "id": "rrmS-OWgm1BK",
    "outputId": "6390c385-b086-4fd4-cad1-59b1da009925"
   },
   "outputs": [],
   "source": [
    "U1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "KITnhyrLYo13"
   },
   "source": [
    "## Intersection method for both CLOs and PLOs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "aS2D2OmEpueu"
   },
   "source": [
    "### Generalised list of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Wu1iwUnkn0cP"
   },
   "outputs": [],
   "source": [
    "append_words = list(map(str.lower,['Cite', 'Define', 'Describe', 'Draw', 'Enumerate', 'Identify' 'Index', 'Indicate', 'Label', 'List', 'Match', 'Meet', 'Name', 'Outline', 'Point', 'Quote', 'Read', 'Recall', 'Recite', 'Recognize', 'Record', 'Repeat', 'Reproduce','Review',\n",
    "'Select', 'State', 'Study', 'Tabulate', 'Trace', 'Write', 'Add', 'Approximate', 'Articulate', 'Associate', 'Characterize', 'Clarify', 'Classify', 'Compare', 'Compute', 'Contrast', 'Convert', 'Defend', 'Detail', 'Differentiate',\n",
    "'Discuss', 'Distinguish', 'Elaborate', 'Estimate', 'Example', 'Explain', 'Express', 'Extend', 'Extrapolate', 'Factor', 'Generalize', 'Give', 'Infer', 'Interact', 'Interpolate', 'Interpret', 'Observe', 'Paraphrase', 'Picture graphically',\n",
    "'Predict', 'Rewrite', 'Subtract', 'Summarize', 'Translate', 'Visualize', 'Acquire', 'Adapt', 'Allocate', 'Alphabetize', 'Apply', 'Ascertain', 'Assign', 'Attain', 'Avoid', 'Back up', 'Calculate', 'Capture', 'Change', 'Complete', 'Construct', \n",
    "'Customize', 'Demonstrate', 'Depreciate', 'Derive', 'Determine', 'Diminish', 'Discover', 'Employ', 'Examine', 'Exercise', 'Explore', 'Expose', 'Figure', 'Graph', 'Handle', 'Illustrate', 'Interconvert', 'Investigate', 'Manipulate', 'Modify', \n",
    "'Operate', 'Personalize', 'Plot','Practice', 'Prepare', 'Price', 'Process', 'Produce', 'Project', 'Provide', 'Relate', 'Round off', 'Sequence', 'Show', 'Simulate', 'Sketch', 'Solve', 'Subscribe', 'Transcribe', 'Use', 'Analyze', 'Audit', \n",
    "'Blueprint', 'Breadboard', 'Break down', 'Confirm', 'Correlate', 'Detect', 'Diagnose', 'Diagram', 'Discriminate', 'Dissect', 'Document', 'Ensure', 'Figure out', 'File', 'Group', 'Interrupt', 'Inventory', 'Layout', 'Manage', 'Maximize', \n",
    "'Minimize', 'Optimize', 'Order', 'Point out', 'Prioritize', 'Proofread', 'Query', 'Separate', 'Subdivide', 'Train', 'Transform', 'Appraise', 'Assess', 'Conclude', 'Counsel', 'Criticize', 'Critique', 'Evaluate', 'Grade', 'Hire', 'Judge', \n",
    "'Justify', 'Measure', 'Prescribe', 'Rank', 'Rate', 'Recommend', 'Release', 'Support', 'Test', 'Validate', 'Verify', 'Abstract', 'Animate', 'Arrange', 'Assemble', 'Budget', 'Categorize', 'Code', 'Combine', 'Compile', 'Compose', 'Cope', \n",
    "'Correspond', 'Create', 'Cultivate', 'Debug', 'Depict', 'Design', 'Develop', 'Devise', 'Dictate', 'Enhance', 'Facilitate', 'Format', 'Formulate', 'Generate', 'Import', 'Improve', 'Incorporate', 'Integrate', 'Interface', 'Join', 'Lecture', \n",
    "'Model', 'Network', 'Organize', 'Overhaul', 'Plan', 'Portray', 'Program', 'Rearrange', 'Reconstruct', 'Reorganize', 'Revise', 'Specify']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "v1Oy5S1Gn7G-"
   },
   "outputs": [],
   "source": [
    "# using + operator to concat the generalised list of words to the PLO list\n",
    "train_column = list(feature_names) + append_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xFcACqqen7OO"
   },
   "outputs": [],
   "source": [
    "# CLO list of words\n",
    "test_column = feature_names1\n",
    "test_column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f4mKQFEyXrEe"
   },
   "outputs": [],
   "source": [
    "# Intersection method for extracting common column names from the tables (both CLO AND PLO)\n",
    "# comparing whether the CLO column name is present in the PLO column names or not\n",
    "train_column = list(feature_names) + append_words # (PLO table ) (# using + operator to concat PLO words and list of generalized words)\n",
    "test_column = list(feature_names1)   # (CLO table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GXFTvsnDXrHV",
    "outputId": "5df371f0-585d-4945-9342-c21959bf533b"
   },
   "outputs": [],
   "source": [
    "# This is the column names from both the tables (using intersection)\n",
    "common_column = list(set(train_column).intersection(set(test_column)))\n",
    "common_column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dxFuq2Glm4lK",
    "outputId": "ab3f8b6d-b942-4957-c206-0669f5d79e35"
   },
   "outputs": [],
   "source": [
    "print(common_column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iVuJ4Fmbm-cM",
    "outputId": "8d3e8bb4-58ca-4b2e-d0ef-046eadb161fd"
   },
   "outputs": [],
   "source": [
    "len(common_column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Q8J85w03XrKF"
   },
   "outputs": [],
   "source": [
    "# Filter the common column values from the CLO table\n",
    "U3 = U2.filter(list(common_column), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 331
    },
    "id": "AeqnATgZnh6y",
    "outputId": "d904ef48-d05d-4193-dffa-c27e040fd887"
   },
   "outputs": [],
   "source": [
    "U3.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sYTk-s3xXrM9"
   },
   "outputs": [],
   "source": [
    "# extracting first row from PLO table and make a dataframe\n",
    "Cs = []\n",
    "for x in range(len(df)):\n",
    "    Cs.append(U1.loc[['PO'+str(x+1)]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Gvd0fUR7XrPt"
   },
   "outputs": [],
   "source": [
    "# Concatenating these extracted each PLOs with 'n' number of CLOs\n",
    "Dds = []\n",
    "for x in range(len(df)):\n",
    "    Dds.append(pd.concat([Cs[x],U3], sort=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "U5FwB5bFXrS9"
   },
   "outputs": [],
   "source": [
    "# Filling the nan values of the concatenated dataframes\n",
    "Ds = []\n",
    "for x in range(len(df)):\n",
    "    Ds.append(Dds[x].fillna(0))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "WLA3uqpeY9ZI"
   },
   "source": [
    "## Calculate Cosine similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AyzKblfgY725"
   },
   "outputs": [],
   "source": [
    "# Calculate cosine similarity for concatenated dataframes and create a new dataframe\n",
    "for x in range(len(df)):\n",
    "    Dds[x] = pd.DataFrame(cosine_similarity(Ds[x], dense_output=True))\n",
    "Ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Y7bh2VScY76R"
   },
   "outputs": [],
   "source": [
    "# Extract the '0'th column because it has the CLO-PLO  cosine similarity values. We are neglecting the remaining ones.\n",
    "# Renaming the '0'th column name to 'Pn' ['P1, P2, P3, P4, ... 'Pn']\n",
    "for x in range(len(df)):\n",
    "    Dds[x].rename(columns = {0 :'PO'+str(x+1)}, inplace = True)\n",
    "\n",
    "Dds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fJssCZmHZGDR"
   },
   "outputs": [],
   "source": [
    "# Concatenating each  '0'th column from different cosine similarity dataframes\n",
    "Ddn = []\n",
    "for x in range(len(df)):\n",
    "    Ddn.append(Dds[x]['PO'+str(x+1)])\n",
    "\n",
    "d = pd.concat(Ddn, axis=1)\n",
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "K5psrdOTZGGZ"
   },
   "outputs": [],
   "source": [
    "# '0'th column gives us 1 which means each PLO map with own PLO.\n",
    "# So we are removing that column.\n",
    "dd = d[1:]\n",
    "dd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KTxJ9O07ZGJr",
    "outputId": "5af1e75c-4145-463b-9d9a-3ffd84287cc1"
   },
   "outputs": [],
   "source": [
    "# resetting index\n",
    "dd.reset_index(inplace = True)\n",
    "dd.drop(['index'], axis=1, inplace = True)\n",
    "dd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FfqgcF23j3-r",
    "outputId": "1e948e88-7d70-42bc-a5bf-8f26c9f6eda5"
   },
   "outputs": [],
   "source": [
    "# print the matrix\n",
    "print(dd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## This code loads the vector file into the word_vectors variable\n",
    "## Download the vector file from https://fasttext.cc/docs/en/english-vectors.html (first file on the website), unzip the file and store in your local development folder\n",
    "## Note: This piece of code may take upto an hour or two to run depending on your pc specs.\n",
    "## My i5 8th gen with 8gig ram took 58mins to run.\n",
    "\n",
    "# from gensim.models import KeyedVectors\n",
    "\n",
    "# # Path to the downloaded .vec file\n",
    "# path_to_vectors = 'wiki-news-300d-1M.vec'\n",
    "# # path_to_vectors = 'wiki.en.vec'\n",
    "# # Load the word vectors\n",
    "# word_vectors = KeyedVectors.load_word2vec_format(path_to_vectors)\n",
    "\n",
    "# # Find similar words\n",
    "# similar_words = word_vectors.most_similar('cat')\n",
    "\n",
    "# # Calculate word similarity\n",
    "# similarity = word_vectors.similarity('cat', 'dog')\n",
    "\n",
    "# # Perform vector arithmetic\n",
    "# result = word_vectors['king'] - word_vectors['man'] + word_vectors['woman']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import spacy\n",
    "\n",
    "file_path = \"Bloom and SOLO Verbs.xlsx\"\n",
    "\n",
    "xls = pd.ExcelFile(file_path)\n",
    "sheet_names = xls.sheet_names\n",
    "# print(sheet_names)\n",
    "\n",
    "sources = {} # Copy verbs from excel into var to maniupulate easier\n",
    "verbs = {   # Complete list of verbs sorted into their taxonomy. Includes # of occurrences and the potential sources and levels the verb could be mapped to\n",
    "    \"Cognitive\": {},\n",
    "    \"Affective\": {},\n",
    "    \"Psychomotor\": {},\n",
    "    \"SOLO\": {}\n",
    "}\n",
    "\n",
    "domain_templates = {    # Template structure for each taxonomy\n",
    "    \"Cognitive\": {\n",
    "        \"Remembering\": set(),\n",
    "        \"Understanding\": set(),\n",
    "        \"Applying\": set(),\n",
    "        \"Analysing\": set(),\n",
    "        \"Evaluating\": set(),\n",
    "        \"Creating\": set()\n",
    "    },\n",
    "    \"Affective\": {\n",
    "        \"Receiving\": set(),\n",
    "        \"Responding\": set(),\n",
    "        \"Valuing\": set(),\n",
    "        \"Organisation\": set(),\n",
    "        \"Characterisation\": set(),\n",
    "    },\n",
    "    \"Psychomotor\": {\n",
    "        \"Perception\": set(),\n",
    "        \"Set\": set(),\n",
    "        \"Guided Response\": set(),\n",
    "        \"Mechanism\": set(),\n",
    "        \"Complex Overt Response\": set(),\n",
    "        \"Adaptation\": set(),\n",
    "        \"Origination\": set()    \n",
    "    },\n",
    "    \"SOLO\": {\n",
    "        \"Prestructural\": set(),\n",
    "        \"Unistructural\": set(),\n",
    "        \"Multistructural\": set(),\n",
    "        \"Relational\": set(),\n",
    "        \"Extended Abstract\": set()\n",
    "    }\n",
    "}\n",
    "\n",
    "# Var to store final mappings of verbs for each taxonomy\n",
    "mapped_verbs = domain_templates\n",
    "\n",
    "# Read all sources and store count of verbs into var\n",
    "for domain_key, domain_item in mapped_verbs.items():    # For each taxonomy\n",
    "    for current_sheet_name in sheet_names:      # For each source sheet\n",
    "        if domain_key in current_sheet_name:    # If the current taxonomy matches the current source sheet\n",
    "            current_sheet = pd.read_excel(xls, current_sheet_name)  # Read the sheet into a var\n",
    "            sources[current_sheet_name] = domain_templates.get(domain_key)  # Add the appropriate taxonomy template structure\n",
    "            for j in range(len(domain_item.keys())):        # For each level of the taxonomy\n",
    "                current_column = current_sheet.columns[j]\n",
    "                column_values = current_sheet[current_column].values.tolist()\n",
    "                for verb in range(len(column_values)):      # For each verb in the level\n",
    "                    if type(column_values[verb])!= str or len(str(column_values[verb]).strip()) == 0:   # Validate the verb is a string\n",
    "                        continue\n",
    "                    v = str(column_values[verb]).strip().lower()    # Format verb\n",
    "                    sources[current_sheet_name][current_column].add(v)\n",
    "                    if v not in verbs:      # If this is the first instance of the verb for the taxonomy init the dict\n",
    "                        verbs[domain_key][v] = {\n",
    "                            \"count\": 1,\n",
    "                            \"potentials\": [{\n",
    "                                \"level\": current_column, \n",
    "                                \"source\": current_sheet_name\n",
    "                                }]\n",
    "                        }\n",
    "                    else:   # Else add vars\n",
    "                        verbs[domain_key][v][\"count\"] += 1\n",
    "                        verbs[domain_key][v][\"potentials\"].append({\n",
    "                                \"level\": current_column, \n",
    "                                \"source\": current_sheet_name\n",
    "                                })\n",
    "\n",
    "# print(sources)\n",
    "# print(verbs)\n",
    "# test = verbs\n",
    "# solo = verbs[\"SOLO\"]\n",
    "# # print(solo)\n",
    "# for verb in solo.items():\n",
    "#     print(verb)\n",
    "\n",
    "\n",
    "# Classify each verb in a taxonomy into a single level\n",
    "for domain_key, domain_item in verbs.items():   # For each taxonomy\n",
    "    for v in domain_item:   # For each verb\n",
    "        if verbs[domain_key][v][\"count\"] == 1:  # If there is only one instance of the verb, map it\n",
    "            mapped_verbs[domain_key][verbs[domain_key][v][\"potentials\"][0][\"level\"]].add(v)\n",
    "        else:\n",
    "            chosen_source = None\n",
    "            chosen_level = None\n",
    "            potential_sources = []\n",
    "            potential_levels = []\n",
    "            for l in verbs[domain_key][v][\"potentials\"]:    # Iterate through the potential level mappings\n",
    "                if l[\"source\"] not in potential_sources or l[\"level\"] not in potential_levels:\n",
    "                    potential_sources.append(l[\"source\"])\n",
    "                    potential_levels.append(l[\"level\"])\n",
    "            for sheet in sheet_names:   # For each source, map verb to first matching source (Assumes sources are sorted in order of priority for mapping)\n",
    "                if sheet in potential_sources:\n",
    "                    chosen_source = sheet\n",
    "                    chosen_level = potential_levels[potential_sources.index(sheet)]\n",
    "                    break\n",
    "            mapped_verbs[domain_key][chosen_level].add(v)\n",
    "\n",
    "## Function to identify verbs in a sentence\n",
    "def identify_verbs(sentence):\n",
    "    # Load the English language model in spaCy\n",
    "    nlp = spacy.load('en_core_web_sm')\n",
    "    \n",
    "    # Process the sentence using spaCy\n",
    "    doc = nlp(sentence)\n",
    "    \n",
    "    # Extract the verbs from the processed sentence\n",
    "    verbs = [token.lemma_ for token in doc if token.pos_ == 'VERB']\n",
    "    \n",
    "    return verbs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bloom_mapping(sentences):\n",
    "\n",
    "    mappings = []\n",
    "\n",
    "    final_level = None\n",
    "    passed_mappings = 0\n",
    "    failed_mappings = 0\n",
    "    total_mappings = len(sentences)\n",
    "    for i in range(len(sentences)): # Iterates over the LOs\n",
    "        identified_verbs = identify_verbs(sentences[i])\n",
    "\n",
    "        score_list = {\n",
    "            \"Cognitive\": {\n",
    "                \"Remembering\": 0,\n",
    "                \"Understanding\": 0,\n",
    "                \"Applying\": 0,\n",
    "                \"Analysing\": 0,\n",
    "                \"Evaluating\": 0,\n",
    "                \"Creating\": 0\n",
    "            },\n",
    "            \"Affective\": {\n",
    "                \"Receiving\": 0,\n",
    "                \"Responding\": 0,\n",
    "                \"Valuing\": 0,\n",
    "                \"Organisation\": 0,\n",
    "                \"Characterisation\": 0\n",
    "            },\n",
    "            \"Psychomotor\": {\n",
    "                \"Perception\": 0,\n",
    "                \"Set\": 0,\n",
    "                \"Guided Response\": 0,\n",
    "                \"Mechanism\": 0,\n",
    "                \"Complex Overt Response\": 0,\n",
    "                \"Adaptation\": 0,\n",
    "                \"Origination\": 0    \n",
    "            },\n",
    "            \"SOLO\": {\n",
    "                \"Prestructural\": 0,\n",
    "                \"Unistructural\": 0,\n",
    "                \"Multistructural\": 0,\n",
    "                \"Relational\": 0,\n",
    "                \"Extended Abstract\": 0\n",
    "            }\n",
    "        }\n",
    "        for taxonomy_key, taxonomy_item in mapped_verbs.items():\n",
    "            for level in taxonomy_item.keys(): # Level\n",
    "                for verb in mapped_verbs[taxonomy_key][level]: # Verb\n",
    "                    similarity_score = 0\n",
    "                    for l in range(len(identified_verbs)):\n",
    "                        try:    # Currently some of the 'verbs' identified are phrases rather than words and it was throwing errors so this is a temp solution \n",
    "                            similarity_score += word_vectors.similarity(identified_verbs[l], verb)\n",
    "                        except:\n",
    "                            pass\n",
    "                    score_list[taxonomy_key][level] += similarity_score\n",
    "\n",
    "        # Identify level based on similarity\n",
    "        max_score = {\n",
    "            \"Cognitive\": { \"Level\": None, \"Score\": 0 },\n",
    "            \"Affective\": { \"Level\": None, \"Score\": 0 },\n",
    "            \"Psychomotor\": { \"Level\": None, \"Score\": 0 },\n",
    "            \"SOLO\": { \"Level\": None, \"Score\": 0 }\n",
    "        }\n",
    "        for t_key, t_item in score_list.items():\n",
    "            for l in t_item:\n",
    "                if max_score[t_key][\"Score\"] < score_list[t_key][l]:\n",
    "                    max_score[t_key] = { \"Level\": l, \"Score\": score_list[t_key][l] }\n",
    "        mappings.append(max_score)\n",
    "\n",
    "    return mappings\n",
    "\n",
    "# TODO: Classify the verbs in each of the learning outcomes\n",
    "ulo_sentences = p1[1].to_list()\n",
    "plo_sentences = df[1].to_list()\n",
    "\n",
    "ulo_classifications = bloom_mapping(ulo_sentences)\n",
    "plo_classifications = bloom_mapping(plo_sentences)\n",
    "\n",
    "# Iteration 1: Assume both CLOs and POs are classifiable into Cognitive, Affective, Psychomotor Levels. \n",
    "# If levels match, boost the CLO to PO coefficient by 0.1\n",
    "# If levels different, don't boost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for classification in ulo_classifications:\n",
    "#     print(classification)\n",
    "for x in range(len(df)):\n",
    "    for i in range(len(ulo_sentences)):\n",
    "        if ulo_classifications[i]['Cognitive']['Level'] == plo_classifications[x]['Cognitive']['Level']:\n",
    "            if dd['PO'+str(x+1)][i] + 0.15 <= 1:\n",
    "                dd['PO'+str(x+1)][i] += 0.15 # Add Offset\n",
    "            else:\n",
    "                dd['PO'+str(x+1)][i] = 1\n",
    "        else:\n",
    "            if dd['PO'+str(x+1)][i] - 0.15 >= 0:\n",
    "                dd['PO'+str(x+1)][i] -= 0.15 # Add Offset\n",
    "            else:\n",
    "                dd['PO'+str(x+1)][i] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store the matrix into csv file\n",
    "dd.to_csv('pseudocodematrix.csv', index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "JYhDRATaZU3Y"
   },
   "source": [
    "## Setting threshold value (taking min and max of each column and divided by 2)\n",
    "## threshold value = (min +max)/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OUqMq9LoZGMj",
    "outputId": "4c008335-10b4-41b8-a2b6-bdbc6c2ebe89"
   },
   "outputs": [],
   "source": [
    "\n",
    "# TODO: Alter the threshold based on matching hierarchy type & bloom verb instead of simply using (column_max+column_min)/2\n",
    "\n",
    "# Setting threshold value \n",
    "# Taking min max average of each column and set that as a threshold value\n",
    "\n",
    "# This will change the coefficients into 0 or 1 mappings in the dd dataframe\n",
    "for x in range(len(df)):\n",
    "    tes = dd['PO'+str(x+1)].values.min()\n",
    "    tes1 = dd['PO'+str(x+1)].values.max()\n",
    "    tt1 = (tes+tes1)/2\n",
    "    \n",
    "    if tt1 == 0:\n",
    "      dd['PO'+str(x+1)] = dd['PO'+str(x+1)] \n",
    "    else:\n",
    "      dd['PO'+str(x+1)] = dd['PO'+str(x+1)].apply(lambda x: 1 if x >= tt1 else 0)\n",
    "# dd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "t1V7uvAxZGPR"
   },
   "outputs": [],
   "source": [
    "# dd\n",
    "\n",
    "dd.to_csv('PLO-CLOmapping.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "0MmjZwIBo05H",
    "outputId": "f1c5ee61-074f-4df0-aa58-19e4a6fc2566"
   },
   "outputs": [],
   "source": [
    "dd.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "nQWRhdsfo1pX",
    "outputId": "c75e374d-17a3-4005-d3d1-49438be48ddf"
   },
   "outputs": [],
   "source": [
    "# human generated output\n",
    "d= pd.read_csv(ORIGINAL_MAPPING)\n",
    "d.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_I05ih9go1s-"
   },
   "outputs": [],
   "source": [
    "df3 = d.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "n6YEK04Ko1we"
   },
   "outputs": [],
   "source": [
    "for x in range(len(df)):\n",
    "  df3['PO'+str(x+1)] = np.where(dd['PO'+str(x+1)] == df3['PO'+str(x+1)], 'True', 'False')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "8FOKLSmFo1zO",
    "outputId": "8383f47b-bd17-4cdb-9bc2-a2cd491cd082"
   },
   "outputs": [],
   "source": [
    "df3.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "J9fhnvqeo12P"
   },
   "outputs": [],
   "source": [
    "for x in range(len(df)):\n",
    "  df3['PO'+str(x+1)] = df3['PO'+str(x+1)].replace('True', 1)\n",
    "  df3['PO'+str(x+1)] = df3['PO'+str(x+1)].replace('False', 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "vML-4JNUo15u",
    "outputId": "c32db157-b88b-43f8-d0cc-d3632146294c"
   },
   "outputs": [],
   "source": [
    "df3.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "tANDI1-6pBeH",
    "outputId": "e815045e-ceab-45b5-ef45-7f3db3a7ce34"
   },
   "outputs": [],
   "source": [
    "# calculating accuracy of the table\n",
    "df3['acc'] = df3.mean(axis=1)\n",
    "df3.head(n=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "NSSFJ-0ppBhX",
    "outputId": "70654783-735c-474e-f592-dc5605a085a0"
   },
   "outputs": [],
   "source": [
    "df4 = pd.concat([df1[0], df3], axis=1)\n",
    "df4.head(n=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 237
    },
    "id": "MXK949BmpBkX",
    "outputId": "05f31e23-74d6-4796-bba9-2fb7d04ec44c"
   },
   "outputs": [],
   "source": [
    "df4.set_index(0, inplace=True)\n",
    "df4.head(n=100)\n",
    "\n",
    "df4.to_csv('WasMappingSuccessful.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gT1jyiUtpBnf",
    "outputId": "32484ddf-a355-4f14-c865-b22e05e9b9dd"
   },
   "outputs": [],
   "source": [
    "df4['acc'].mean()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "CLOPLO_using_(minmax_2)_threshold_value_(pseudocode).ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3.9.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "11938c6bc6919ae2720b4d5011047913343b08a43b18698fd82dedb0d4417594"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
