{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PLO Mapper"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "d22uW_pcdDeE"
   },
   "source": [
    "## Installing necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%bash\n",
    "\n",
    "# # Dependencies\n",
    "# pip install docx2txt\n",
    "# pip install strsimpy\n",
    "# pip install python-docx\n",
    "# pip install pandas\n",
    "\n",
    "# if ls docx2csv >/dev/null 2>&1; then\n",
    "#     echo \"docx2csv exists.\"\n",
    "# else\n",
    "#     echo \"Folder does not exist. Cloning docx2csv.\"\n",
    "#     git clone https://github.com/ivbeg/docx2csv.git\n",
    "# fi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%bash\n",
    "\n",
    "# source .env\n",
    "# cd docx2csv && echo \"$PASSWORD\" | sudo -S python3 setup.py install"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----- TEST DATA INPUT -----\n",
    "\n",
    "# Computer Science Test Data.\n",
    "# CURRENT_MAPPING=\"Lists_ComputerScience.docx\"\n",
    "# ORIGINAL_MAPPING=\"Original-Mapping-ComputerScience.csv\"\n",
    "# PO_LABEL=\"P\"\n",
    "\n",
    "# InformationSecurity Test Data.\n",
    "# CURRENT_MAPPING=\"Lists_InformationSecurity.docx\"\n",
    "# ORIGINAL_MAPPING=\"Original-Mapping-InfoSecurity.csv\"\n",
    "# PO_LABEL=\"P\"\n",
    "ADJUSTMENT_THRESHOLD=0.35\n",
    "\n",
    "# Monash Engineering Test Data.\n",
    "CURRENT_MAPPING=\"Lists_MonashEngineering.docx\"\n",
    "ORIGINAL_MAPPING=\"Original-Mapping-MonashEngineering.csv\"\n",
    "PO_LABEL=\"PO\"\n",
    "\n",
    "# Program Outcome & Regulatory Body Requirements Table.\n",
    "PROGRAM_OUTCOME_TABLE=\"Professional Body PO Comparisons.docx\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bpNT07kAXk01"
   },
   "outputs": [],
   "source": [
    "# importing necessary libraries\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "A_eRLnUVbzGk"
   },
   "outputs": [],
   "source": [
    "# extract tables from word document\n",
    "from docx2csv import extract_tables, extract\n",
    "tables = extract_tables(CURRENT_MAPPING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3ypxqU-pbzC7"
   },
   "outputs": [],
   "source": [
    "from docx import Document\n",
    "document = Document(CURRENT_MAPPING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "C3UwEI5hby9k"
   },
   "outputs": [],
   "source": [
    "def read_docx_table(document,table_num):\n",
    "  table = document.tables[table_num-1]\n",
    "  data = [[cell.text for cell in row.cells] for row in table.rows]\n",
    "  df = pd.DataFrame(data)\n",
    "  return df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "VugavvsYYLPG"
   },
   "source": [
    "## PLO TABLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "7ZpBU1LeXqiG",
    "outputId": "f021d7e4-aebe-430c-9b07-ad265896aff9"
   },
   "outputs": [],
   "source": [
    "# Creating a dataframe for PLOs and it will accept 'n' number of PLOs\n",
    "table_num=1\n",
    "df = read_docx_table(document,table_num)\n",
    "df.head(n=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "U28-N6sjbQkc"
   },
   "outputs": [],
   "source": [
    "q1 = df.copy()\n",
    "df_po = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BzoHBy9RXqlF"
   },
   "outputs": [],
   "source": [
    "# assigning count vectorizer\n",
    "count_vectorizer = CountVectorizer(stop_words='english', min_df=0.005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xUftpLLZXqoV"
   },
   "outputs": [],
   "source": [
    "# Remove integers\n",
    "\n",
    "# Data preprocessing for PLO dataframe\n",
    "q1[1] = q1[1].str.lower()\n",
    "corpus = q1[1].tolist()\n",
    "corpii = count_vectorizer.fit_transform(corpus)\n",
    "corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "n20hDeKkQX9t",
    "outputId": "eac9548d-3148-4260-8b80-17f19079e3bd"
   },
   "outputs": [],
   "source": [
    "corpii"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mySN4lHeXqrV",
    "outputId": "7eb5f419-6cd3-467b-d40c-f7930edd9b61"
   },
   "outputs": [],
   "source": [
    "# extracting features names from PLO table\n",
    "feature_names = count_vectorizer.get_feature_names_out()\n",
    "feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oRxKuvJkQ0Mk",
    "outputId": "80f0fee1-eebe-4487-dfd9-946ea7c8b797"
   },
   "outputs": [],
   "source": [
    "len(feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0K-RSe6xXqul"
   },
   "outputs": [],
   "source": [
    "# Converting features to vector form and create a dataframe\n",
    "X1 = pd.DataFrame(corpii.toarray(), columns=feature_names)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "i8mtCAEZYPIe"
   },
   "source": [
    "## CLO TABLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "clX4_RyxXqxF"
   },
   "outputs": [],
   "source": [
    "# Creating a dataframe for CLOs and it will accept 'n' number of CLOs\n",
    "table_num=2\n",
    "df1 = read_docx_table(document,table_num)\n",
    "p1 = df1.copy()\n",
    "df_clo = df1.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gWp0jpPVXq0F"
   },
   "outputs": [],
   "source": [
    "# Data preprocessing for CLO dataframe\n",
    "p1[1] = p1[1].str.lower()\n",
    "corpus11 = p1[1].tolist()\n",
    "corpii11 = count_vectorizer.fit_transform(corpus11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1Zg_ZIN7Xq3e",
    "outputId": "6e4c8724-732a-4399-c269-259f23c56e15"
   },
   "outputs": [],
   "source": [
    "# extracting features names from CLO table\n",
    "feature_names1 = count_vectorizer.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tCXVAkg3mSgL",
    "outputId": "ad41e6d3-8677-4673-9531-71f6d9e78ee0"
   },
   "outputs": [],
   "source": [
    "len(feature_names1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PPI9g37tXq6l"
   },
   "outputs": [],
   "source": [
    "# Converting features to vector form and create a dataframe\n",
    "X2 = pd.DataFrame(corpii11.toarray(), columns=feature_names1)\n",
    "X2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 299
    },
    "id": "U5C18Iysmlbb",
    "outputId": "f18da737-d812-4a7d-8763-fe816dec3e3c"
   },
   "outputs": [],
   "source": [
    "X2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b6W1EAfRXq91"
   },
   "outputs": [],
   "source": [
    "# adding column index to the CLO table\n",
    "U2 = pd.concat([df1[0], X2], axis=1)\n",
    "U2.set_index(0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 331
    },
    "id": "1RAJJ1I_mxiL",
    "outputId": "bc32ea6a-e597-4362-d2fa-aff4d2bbb47d"
   },
   "outputs": [],
   "source": [
    "U2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PwmpreCjXrBN"
   },
   "outputs": [],
   "source": [
    "# adding column index to the PLO table\n",
    "U1 = pd.concat([df[0], X1], axis=1)\n",
    "U1.set_index(0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 362
    },
    "id": "rrmS-OWgm1BK",
    "outputId": "6390c385-b086-4fd4-cad1-59b1da009925"
   },
   "outputs": [],
   "source": [
    "U1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "KITnhyrLYo13"
   },
   "source": [
    "## Intersection method for both CLOs and PLOs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "aS2D2OmEpueu"
   },
   "source": [
    "### Generalised list of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Wu1iwUnkn0cP"
   },
   "outputs": [],
   "source": [
    "append_words = list(map(str.lower,['Cite', 'Define', 'Describe', 'Draw', 'Enumerate', 'Identify' 'Index', 'Indicate', 'Label', 'List', 'Match', 'Meet', 'Name', 'Outline', 'Point', 'Quote', 'Read', 'Recall', 'Recite', 'Recognize', 'Record', 'Repeat', 'Reproduce','Review',\n",
    "'Select', 'State', 'Study', 'Tabulate', 'Trace', 'Write', 'Add', 'Approximate', 'Articulate', 'Associate', 'Characterize', 'Clarify', 'Classify', 'Compare', 'Compute', 'Contrast', 'Convert', 'Defend', 'Detail', 'Differentiate',\n",
    "'Discuss', 'Distinguish', 'Elaborate', 'Estimate', 'Example', 'Explain', 'Express', 'Extend', 'Extrapolate', 'Factor', 'Generalize', 'Give', 'Infer', 'Interact', 'Interpolate', 'Interpret', 'Observe', 'Paraphrase', 'Picture graphically',\n",
    "'Predict', 'Rewrite', 'Subtract', 'Summarize', 'Translate', 'Visualize', 'Acquire', 'Adapt', 'Allocate', 'Alphabetize', 'Apply', 'Ascertain', 'Assign', 'Attain', 'Avoid', 'Back up', 'Calculate', 'Capture', 'Change', 'Complete', 'Construct',\n",
    "'Customize', 'Demonstrate', 'Depreciate', 'Derive', 'Determine', 'Diminish', 'Discover', 'Employ', 'Examine', 'Exercise', 'Explore', 'Expose', 'Figure', 'Graph', 'Handle', 'Illustrate', 'Interconvert', 'Investigate', 'Manipulate', 'Modify',\n",
    "'Operate', 'Personalize', 'Plot','Practice', 'Prepare', 'Price', 'Process', 'Produce', 'Project', 'Provide', 'Relate', 'Round off', 'Sequence', 'Show', 'Simulate', 'Sketch', 'Solve', 'Subscribe', 'Transcribe', 'Use', 'Analyze', 'Audit',\n",
    "'Blueprint', 'Breadboard', 'Break down', 'Confirm', 'Correlate', 'Detect', 'Diagnose', 'Diagram', 'Discriminate', 'Dissect', 'Document', 'Ensure', 'Figure out', 'File', 'Group', 'Interrupt', 'Inventory', 'Layout', 'Manage', 'Maximize',\n",
    "'Minimize', 'Optimize', 'Order', 'Point out', 'Prioritize', 'Proofread', 'Query', 'Separate', 'Subdivide', 'Train', 'Transform', 'Appraise', 'Assess', 'Conclude', 'Counsel', 'Criticize', 'Critique', 'Evaluate', 'Grade', 'Hire', 'Judge',\n",
    "'Justify', 'Measure', 'Prescribe', 'Rank', 'Rate', 'Recommend', 'Release', 'Support', 'Test', 'Validate', 'Verify', 'Abstract', 'Animate', 'Arrange', 'Assemble', 'Budget', 'Categorize', 'Code', 'Combine', 'Compile', 'Compose', 'Cope',\n",
    "'Correspond', 'Create', 'Cultivate', 'Debug', 'Depict', 'Design', 'Develop', 'Devise', 'Dictate', 'Enhance', 'Facilitate', 'Format', 'Formulate', 'Generate', 'Import', 'Improve', 'Incorporate', 'Integrate', 'Interface', 'Join', 'Lecture',\n",
    "'Model', 'Network', 'Organize', 'Overhaul', 'Plan', 'Portray', 'Program', 'Rearrange', 'Reconstruct', 'Reorganize', 'Revise', 'Specify']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "v1Oy5S1Gn7G-"
   },
   "outputs": [],
   "source": [
    "# using + operator to concat the generalised list of words to the PLO list\n",
    "train_column = list(feature_names) + append_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xFcACqqen7OO"
   },
   "outputs": [],
   "source": [
    "# CLO list of words\n",
    "test_column = feature_names1\n",
    "test_column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f4mKQFEyXrEe"
   },
   "outputs": [],
   "source": [
    "# Intersection method for extracting common column names from the tables (both CLO AND PLO)\n",
    "# comparing whether the CLO column name is present in the PLO column names or not\n",
    "train_column = list(feature_names) + append_words # (PLO table ) (# using + operator to concat PLO words and list of generalized words)\n",
    "test_column = list(feature_names1)   # (CLO table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GXFTvsnDXrHV",
    "outputId": "5df371f0-585d-4945-9342-c21959bf533b"
   },
   "outputs": [],
   "source": [
    "# This is the column names from both the tables (using intersection)\n",
    "common_column = list(set(train_column).intersection(set(test_column)))\n",
    "common_column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dxFuq2Glm4lK",
    "outputId": "ab3f8b6d-b942-4957-c206-0669f5d79e35"
   },
   "outputs": [],
   "source": [
    "print(common_column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iVuJ4Fmbm-cM",
    "outputId": "8d3e8bb4-58ca-4b2e-d0ef-046eadb161fd"
   },
   "outputs": [],
   "source": [
    "len(common_column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Q8J85w03XrKF"
   },
   "outputs": [],
   "source": [
    "# Filter the common column values from the CLO table\n",
    "U3 = U2.filter(list(common_column), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 331
    },
    "id": "AeqnATgZnh6y",
    "outputId": "d904ef48-d05d-4193-dffa-c27e040fd887"
   },
   "outputs": [],
   "source": [
    "U3.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sYTk-s3xXrM9"
   },
   "outputs": [],
   "source": [
    "# extracting first row from PLO table and make a dataframe\n",
    "Cs = []\n",
    "for x in range(len(df)):\n",
    "    Cs.append(U1.loc[[PO_LABEL+str(x+1)]])\n",
    "# U1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Gvd0fUR7XrPt"
   },
   "outputs": [],
   "source": [
    "# Concatenating these extracted each PLOs with 'n' number of CLOs\n",
    "Dds = []\n",
    "for x in range(len(df)):\n",
    "    Dds.append(pd.concat([Cs[x],U3], sort=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "U5FwB5bFXrS9"
   },
   "outputs": [],
   "source": [
    "# Filling the nan values of the concatenated dataframes\n",
    "Ds = []\n",
    "for x in range(len(df)):\n",
    "    Ds.append(Dds[x].fillna(0))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "WLA3uqpeY9ZI"
   },
   "source": [
    "## Calculate Cosine similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AyzKblfgY725"
   },
   "outputs": [],
   "source": [
    "# Calculate cosine similarity for concatenated dataframes and create a new dataframe\n",
    "for x in range(len(df)):\n",
    "    Dds[x] = pd.DataFrame(cosine_similarity(Ds[x], dense_output=True))\n",
    "Ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Y7bh2VScY76R"
   },
   "outputs": [],
   "source": [
    "# Extract the '0'th column because it has the CLO-PLO  cosine similarity values. We are neglecting the remaining ones.\n",
    "# Renaming the '0'th column name to 'Pn' ['P1, P2, P3, P4, ... 'Pn']\n",
    "for x in range(len(df)):\n",
    "    Dds[x].rename(columns = {0 :PO_LABEL+str(x+1)}, inplace = True)\n",
    "\n",
    "Dds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fJssCZmHZGDR"
   },
   "outputs": [],
   "source": [
    "# Concatenating each  '0'th column from different cosine similarity dataframes\n",
    "Ddn = []\n",
    "for x in range(len(df)):\n",
    "    Ddn.append(Dds[x][PO_LABEL+str(x+1)])\n",
    "\n",
    "d = pd.concat(Ddn, axis=1)\n",
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "K5psrdOTZGGZ"
   },
   "outputs": [],
   "source": [
    "# '0'th column gives us 1 which means each PLO map with own PLO.\n",
    "# So we are removing that column.\n",
    "dd = d[1:]\n",
    "dd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KTxJ9O07ZGJr",
    "outputId": "5af1e75c-4145-463b-9d9a-3ffd84287cc1"
   },
   "outputs": [],
   "source": [
    "# resetting index\n",
    "dd.reset_index(inplace = True)\n",
    "dd.drop(['index'], axis=1, inplace = True)\n",
    "dd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FfqgcF23j3-r",
    "outputId": "1e948e88-7d70-42bc-a5bf-8f26c9f6eda5"
   },
   "outputs": [],
   "source": [
    "# print the matrix\n",
    "print(dd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## This code loads the vector file into the word_vectors variable\n",
    "## Download the vector file from https://fasttext.cc/docs/en/english-vectors.html (first file on the website), unzip the file and store in your local development folder\n",
    "## Note: This piece of code may take upto an hour or two to run depending on your pc specs.\n",
    "## My i5 8th gen with 8gig ram took 58mins to run.\n",
    "\n",
    "# from gensim.models import KeyedVectors\n",
    "\n",
    "# # Path to the downloaded .vec file\n",
    "# path_to_vectors = 'wiki-news-300d-1M.vec'\n",
    "# # path_to_vectors = 'wiki.en.vec'\n",
    "# # Load the word vectors\n",
    "# word_vectors = KeyedVectors.load_word2vec_format(path_to_vectors)\n",
    "\n",
    "# # Find similar words\n",
    "# similar_words = word_vectors.most_similar('cat')\n",
    "\n",
    "# # Calculate word similarity\n",
    "# similarity = word_vectors.similarity('cat', 'dog')\n",
    "\n",
    "# # Perform vector arithmetic\n",
    "# result = word_vectors['king'] - word_vectors['man'] + word_vectors['woman']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LO Mapper Setup:\n",
    "Setup section for the LO mapper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.test.utils import common_texts\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "import pandas as pd\n",
    "import csv\n",
    "import math\n",
    "\n",
    "# import pprint\n",
    "import re\n",
    "import json\n",
    "\n",
    "import spacy\n",
    "\n",
    "# Initialises dictionary containing American to UK spelling translations\n",
    "american_to_british_dict = {}\n",
    "american_to_british_path = \"American-British-English-Translator.json\"\n",
    "with open(american_to_british_path, \"r\") as file:\n",
    "    data = file.read()\n",
    "american_to_british_dict = json.loads(data)\n",
    "\n",
    "\n",
    "def britishise(sentence):\n",
    "    \"\"\"\n",
    "    Convert words in a sentence to UK spelling to ensure consistency\n",
    "\n",
    "    Input:\n",
    "        sentence: An array of strings\n",
    "\n",
    "    Output:\n",
    "        sentence: An array of strings, which have been converted to UK spelling\n",
    "    \"\"\"\n",
    "\n",
    "    for j in range(len(sentence)):\n",
    "        try:\n",
    "            sentence[j] = american_to_british_dict[sentence[j].lower()]\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    return sentence\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "\n",
    "def verb_classifier(verbs_file_path):  #\n",
    "    \"\"\"\n",
    "    Takes an excel spreadsheet containing verbs, classifies the verbs and stores it into a Dataframe.\n",
    "\n",
    "\n",
    "    Inputs:\n",
    "        verbs_file_path: A string that contains the path to the excel spreadsheet to be read\n",
    "\n",
    "    Outputs:\n",
    "        domain_levels: A DataFrame which contains all the verbs from the spreadsheet classified into their respective levels\n",
    "    \"\"\"\n",
    "\n",
    "    xlsx = pd.ExcelFile(verbs_file_path, engine=\"openpyxl\")\n",
    "\n",
    "    sheet_names = xlsx.sheet_names  # Get a list of sheet names\n",
    "\n",
    "    # Create an empty dictionary to store DataFrames for each sheet\n",
    "    dfs = {}\n",
    "\n",
    "    for sheet_name in sheet_names:\n",
    "        df = xlsx.parse(sheet_name)  # For XLSX files\n",
    "\n",
    "        # Store the DataFrame in the dictionary\n",
    "        dfs[sheet_name] = df\n",
    "\n",
    "    domain_levels = pd.concat(dfs)\n",
    "    duplicate_checklist = []\n",
    "\n",
    "    # Iterate over all values in the spreadsheet\n",
    "    for i in range(domain_levels.shape[0]):\n",
    "        for j in range(domain_levels.shape[1]):\n",
    "            cell_value = domain_levels.iloc[i, j]\n",
    "            if not pd.isna(\n",
    "                cell_value\n",
    "            ):  # Format verbs (lower case, UK spelling, lemmatised format)\n",
    "                cell_value_lower = cell_value.lower()\n",
    "                verb_brit = britishise([cell_value_lower])[0]\n",
    "                verb = lemmatizer.lemmatize(verb_brit, pos=\"v\")\n",
    "\n",
    "                if (\n",
    "                    verb not in duplicate_checklist\n",
    "                ):  # Check if the verb is already mapped\n",
    "                    domain_levels.iloc[i, j] = verb\n",
    "                    duplicate_checklist.append(verb)\n",
    "                else:\n",
    "                    domain_levels.iloc[i, j] = float(\"nan\")\n",
    "\n",
    "    domain_levels = domain_levels.dropna(how=\"all\")\n",
    "    return domain_levels\n",
    "\n",
    "# Convert mapped_verbs DataFrames which is used to find verbs at a certain level, to a 'dictionary' to lookup the level of a verb\n",
    "def generate_verb_list(mapped_verbs):\n",
    "    tp_arr = []\n",
    "    levels = []\n",
    "    for t_key, t_item in mapped_verbs.items():\n",
    "        print(t_item)\n",
    "        columns = t_item.columns.values\n",
    "        for x in range(t_item.shape[0]):\n",
    "            for y in range(t_item.shape[1]):\n",
    "                verb = t_item.iloc[x, y]\n",
    "                if not pd.isna(verb):\n",
    "                    tp_arr.append((t_key, verb))\n",
    "                    levels.append(columns[y])\n",
    "\n",
    "    index = pd.MultiIndex.from_tuples(tp_arr)\n",
    "    verb_list = pd.DataFrame(levels, index=index, columns=[\"Level\"])\n",
    "    return verb_list\n",
    "\n",
    "# Load the English language model in spaCy\n",
    "# nlp = spacy.load('en_core_web_sm')\n",
    "nlp = spacy.load(\"en_core_web_trf\", exclude=[\"ner\"])\n",
    "\n",
    "\n",
    "## Function to identify verbs in a sentence\n",
    "def identify_verbs(sentence):\n",
    "    \"\"\"\n",
    "    Identify verbs within a sentence and lemmatise them (convert them into their base word)\n",
    "\n",
    "    Inputs:\n",
    "        sentence: A string\n",
    "\n",
    "    Outputs:\n",
    "        verbs: An array of strings representing identified verbs in their lemmatised form\n",
    "    \"\"\"\n",
    "\n",
    "    # Process the sentence using spaCy\n",
    "    doc = nlp(sentence)\n",
    "\n",
    "    # Extract the verbs from the processed sentence\n",
    "    verbs = [token.lemma_ for token in doc if token.pos_ == \"VERB\"]\n",
    "\n",
    "    return verbs\n",
    "\n",
    "\n",
    "def extract_columns(file_path, columns):\n",
    "    extracted_data = {}\n",
    "\n",
    "    with open(file_path, \"r\") as file:\n",
    "        reader = csv.reader(file)\n",
    "        headers = next(reader)  # Read the headers\n",
    "\n",
    "        # Check if all specified columns exist in the CSV file\n",
    "        for column in columns:\n",
    "            if column not in headers:\n",
    "                raise ValueError(f\"Column '{column}' not found in the CSV file.\")\n",
    "\n",
    "        # Initialize separate arrays for each column\n",
    "        for column in columns:\n",
    "            extracted_data[column] = []\n",
    "\n",
    "        # Extract data from specified columns\n",
    "        for row in reader:\n",
    "            for column in columns:\n",
    "                column_index = headers.index(column)\n",
    "                extracted_data[column].append(row[column_index])\n",
    "\n",
    "    return extracted_data\n",
    "\n",
    "\n",
    "# Paths\n",
    "solo_file_path = \"SOLO.xlsx\"\n",
    "bloom_cognitive_file_path = \"Bloom_cognitive.xlsx\"\n",
    "bloom_psychomotor_file_path = \"Bloom_psychomotor.xlsx\"\n",
    "bloom_affective_file_path = \"Bloom_affective.xlsx\"\n",
    "# Verbs\n",
    "mapped_verbs = {\n",
    "    \"Cognitive\": verb_classifier(bloom_cognitive_file_path),\n",
    "    \"Affective\": verb_classifier(bloom_affective_file_path),\n",
    "    \"Psychomotor\": verb_classifier(bloom_psychomotor_file_path),\n",
    "    \"SOLO\": verb_classifier(solo_file_path),\n",
    "}\n",
    "\n",
    "verb_list = generate_verb_list(mapped_verbs)\n",
    "print(verb_list)\n",
    "\n",
    "# Thresholds to filter the similarity of words to improve accuracy\n",
    "suggested_sim_threshold = 0.985\n",
    "sim_threshold = 0.997\n",
    "\n",
    "# Example usage\n",
    "csv_file = \"Learning outcomes manual mapping - Mappings.csv\"\n",
    "# csv_file = 'Learning outcomes manual mapping - Mappings - Testing.csv'\n",
    "columns_to_extract = [\"LO\", \"Cognitive\", \"Affective\", \"Psychomotor\", \"SOLO\"]\n",
    "\n",
    "extracted_data = extract_columns(csv_file, columns_to_extract)\n",
    "\n",
    "sentences = extracted_data[\"LO\"]\n",
    "final_levels = {\n",
    "    \"Cognitive\": extracted_data[\"Cognitive\"],\n",
    "    \"Affective\": extracted_data[\"Affective\"],\n",
    "    \"Psychomotor\": extracted_data[\"Psychomotor\"],\n",
    "    \"SOLO\": extracted_data[\"SOLO\"],\n",
    "}\n",
    "\n",
    "# extract tables from word document\n",
    "from docx2csv import extract_tables\n",
    "from docx import Document\n",
    "tables = extract_tables(CURRENT_MAPPING)\n",
    "document = Document(CURRENT_MAPPING)\n",
    "\n",
    "def read_docx_table(document,table_num):\n",
    "  table = document.tables[table_num-1]\n",
    "  data = [[cell.text for cell in row.cells] for row in table.rows]\n",
    "  df = pd.DataFrame(data)\n",
    "  return df\n",
    "\n",
    "training_sentences_4 = []\n",
    "\n",
    "table_num=1\n",
    "df_po = read_docx_table(document,table_num)\n",
    "df_po.head(n=12)\n",
    "for i in range(len(df_po[1])):\n",
    "  training_sentences_4.append(df_po[1][i].replace(\"\\n\",\"\"))\n",
    "\n",
    "table_num=2\n",
    "df1 = read_docx_table(document,table_num)\n",
    "df_clo = df1.copy()\n",
    "for i in range(len(df_clo[1])):\n",
    "  training_sentences_4.append(df_clo[1][i].replace(\"\\n\",\"\"))\n",
    "\n",
    "sentences = training_sentences_4\n",
    "# print(sentences)\n",
    "\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"stopwords\")\n",
    "nltk.download(\"wordnet\")\n",
    "\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "# Array of all the PLOs and ULOs (We can couple them together as we're trying to identify Bloom/Solo level here)\n",
    "lo_sentence_array = []\n",
    "\n",
    "# training_sentences = training_sentences_1\n",
    "# training_sentences = training_sentences_2\n",
    "# training_sentences = training_sentences_3\n",
    "training_sentences = training_sentences_4\n",
    "\n",
    "# TODO: train CLO classification with all data instead of just one course.\n",
    "for sentence in training_sentences:\n",
    "    sentence = re.sub(r\"[^\\w\\s]\", \"\", sentence)\n",
    "    # Process the sentence using spaCy\n",
    "    doc = nlp(sentence)\n",
    "\n",
    "    # Extract the verbs from the processed sentence\n",
    "    cleaned_tokens = [token.lemma_ for token in doc if token.lower_ not in stop_words]\n",
    "    cleaned_tokens = britishise(cleaned_tokens)\n",
    "    lo_sentence_array.append(cleaned_tokens)\n",
    "\n",
    "# build the vocabulary and train the model\n",
    "# IMPORTANT, N0TE THAT sg=1 flag specifies Word2Vec to use the Skip Gram Model as designated by the LSTM paper.\n",
    "model = Word2Vec(\n",
    "    sentences=lo_sentence_array,\n",
    "    vector_size=100,\n",
    "    window=5,\n",
    "    min_count=1,\n",
    "    workers=4,\n",
    "    sg=1,\n",
    "    epochs=30,\n",
    ")\n",
    "\n",
    "model.build_vocab(corpus_iterable=verb_list, update=True)\n",
    "model.update_weights()\n",
    "\n",
    "model_1_wv = model.wv\n",
    "\n",
    "print(\"Method 1\")\n",
    "word_vectors = model_1_wv\n",
    "\n",
    "# print(\"\\nMethod 2\")\n",
    "# word_vectors = model_2_wv\n",
    "# learning_outcome_mapping(sentences, final_levels, sim_threshold, suggested_sim_threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_dataframe(df, path, sheet_name, columns):\n",
    "# Export Failed Cases\n",
    "    # print(failed_cases)\n",
    "    with pd.ExcelWriter(path) as writer:\n",
    "        df.to_excel(\n",
    "            writer,\n",
    "            sheet_name = sheet_name,\n",
    "            columns = columns\n",
    "        )\n",
    "\n",
    "def calculate_accuracy(mappings, final_levels):\n",
    "    passed_mappings = {\"Cognitive\": 0, \"Affective\": 0, \"Psychomotor\": 0, \"SOLO\": 0}\n",
    "    failed_mappings = {\"Cognitive\": 0, \"Affective\": 0, \"Psychomotor\": 0, \"SOLO\": 0}\n",
    "    columns = mappings.columns\n",
    "    columns.append(pd.Index(data=[\"Manual Level\"]))\n",
    "    failed_cases = pd.DataFrame(columns=columns)\n",
    "\n",
    "    # Calculate Accuracy\n",
    "    total_passed = 0\n",
    "    total_failed = 0\n",
    "    for i in range(mappings.shape[0]):\n",
    "        taxonomy = mappings.at[i, \"Chosen Taxonomy\"]\n",
    "        if taxonomy and mappings.at[i, \"Mapped Level\"] == final_levels[taxonomy][i]:\n",
    "            passed_mappings[taxonomy] += 1\n",
    "            total_passed += 1\n",
    "        else:\n",
    "            total_failed += 1\n",
    "            failed_cases.loc[total_failed] = mappings.loc[i].copy()\n",
    "            if taxonomy:\n",
    "                failed_mappings[taxonomy] += 1\n",
    "                failed_cases.at[total_failed, \"Manual Level\"] = final_levels[taxonomy][i]\n",
    "\n",
    "    for taxonomy in passed_mappings.keys():\n",
    "        if failed_mappings[taxonomy] == 0:\n",
    "            failed_mappings[taxonomy] = 1\n",
    "        mapping_percentage = math.ceil((passed_mappings[taxonomy] / (passed_mappings[taxonomy] + failed_mappings[taxonomy])) * 100)\n",
    "        print(\"Percentage of \", taxonomy, \" mappings passed: \", mapping_percentage, \"%\")\n",
    "\n",
    "    total_mapping_percentage = math.ceil(\n",
    "        (total_passed / (total_passed + total_failed)) * 100\n",
    "    )\n",
    "    print(\"Total percentage of mappings passed: \", total_mapping_percentage, \"%\")\n",
    "\n",
    "    export_dataframe(mappings, \"./outputs/all_lo_mappings.xlsx\", \"all_mappings\", mappings.columns)\n",
    "    export_dataframe(failed_cases, \"./outputs/failed_lo_mappings.xlsx\", \"failed_mappings\", failed_cases.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Executing Learning Outcome Mapping Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def learning_outcome_mapping(\n",
    "    sentences, final_levels, SIM_THRESHOLD, SUGGESTED_SIM_THRESHOLD, calculate_accuracy_flag\n",
    "):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "        sentences: An array of Learning Outcomes (sentences) in string format.\n",
    "        final levels: An dictionary of arrays. The dictionary keys are the taxonomies and the arrays contain strings representing the final mapped level of the corresponding learning outcome. If LO is not mapped to that domain leave null value\n",
    "    \"\"\"\n",
    "\n",
    "    mappings = pd.DataFrame(\n",
    "        columns=[\n",
    "            \"Sentence\",\n",
    "            \"Chosen Taxonomy\",\n",
    "            \"Mapped Level\",\n",
    "            \"Verbs Identified\",\n",
    "            \"Suggested Verbs\"\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    for i in range(len(sentences)):  # Iterates over the LOs\n",
    "        sentence = britishise(sentences[i])\n",
    "\n",
    "        identified_verbs = identify_verbs(sentence)\n",
    "        similar_verbs = {}\n",
    "\n",
    "        score_list = {\n",
    "            \"Cognitive\": {\n",
    "                \"Remembering\": 0,\n",
    "                \"Understanding\": 0,\n",
    "                \"Applying\": 0,\n",
    "                \"Analysing\": 0,\n",
    "                \"Evaluating\": 0,\n",
    "                \"Creating\": 0,\n",
    "            },\n",
    "            \"Affective\": {\n",
    "                \"Receiving\": 0,\n",
    "                \"Responding\": 0,\n",
    "                \"Valuing\": 0,\n",
    "                \"Organisation\": 0,\n",
    "                \"Characterisation\": 0,\n",
    "            },\n",
    "            \"Psychomotor\": {\n",
    "                \"Perception\": 0,\n",
    "                \"Set\": 0,\n",
    "                \"Guided Response\": 0,\n",
    "                \"Mechanism\": 0,\n",
    "                \"Complex Overt Response\": 0,\n",
    "                \"Adaptation\": 0,\n",
    "                \"Origination\": 0,\n",
    "            },\n",
    "            \"SOLO\": {\n",
    "                \"Prestructural\": 0,\n",
    "                \"Unistructural\": 0,\n",
    "                \"Multistructural\": 0,\n",
    "                \"Relational\": 0,\n",
    "                \"Extended Abstract\": 0,\n",
    "            },\n",
    "        }\n",
    "\n",
    "        for taxonomy_key, taxonomy_item in mapped_verbs.items():\n",
    "            similar_verbs[taxonomy_key] = pd.DataFrame(columns=[\"Level\", \"Similarity\"])\n",
    "            for identified_verb in identified_verbs:\n",
    "                for k in range(taxonomy_item.shape[1]):  # Col (Level)\n",
    "                    for j in range(taxonomy_item.shape[0]):  # Row\n",
    "                        current_level =taxonomy_item.columns[k]\n",
    "                        verb = taxonomy_item.iloc[j, k]\n",
    "\n",
    "                        if verb is None or not verb or pd.isna(verb):\n",
    "                            continue\n",
    "\n",
    "                        similarity_score = 0\n",
    "                        try:  # Currently some of the 'verbs' identified are phrases rather than words and it was throwing errors so this is a temp solution\n",
    "                            sim_score = word_vectors.similarity(identified_verb, verb)\n",
    "                            if (\n",
    "                                sim_score >= SUGGESTED_SIM_THRESHOLD\n",
    "                                and identified_verb != verb\n",
    "                            ):\n",
    "                                similar_verbs[taxonomy_key].at[\n",
    "                                    verb, \"Level\"\n",
    "                                ] = current_level\n",
    "                                similar_verbs[taxonomy_key].at[\n",
    "                                    verb, \"Similarity\"\n",
    "                                ] = sim_score\n",
    "                            if sim_score >= SIM_THRESHOLD:\n",
    "                                similarity_score += sim_score\n",
    "                        except:\n",
    "                            pass\n",
    "                        score_list[taxonomy_key][\n",
    "                            current_level\n",
    "                        ] += similarity_score\n",
    "\n",
    "        # Determine Taxonomy and Identify level based on similarity\n",
    "        max_score = {\n",
    "            \"Cognitive\": {\"Level\": None, \"Score\": 0},\n",
    "            \"Affective\": {\"Level\": None, \"Score\": 0},\n",
    "            \"Psychomotor\": {\"Level\": None, \"Score\": 0},\n",
    "            \"SOLO\": {\"Level\": None, \"Score\": 0},\n",
    "        }\n",
    "        for t_key, t_item in score_list.items():\n",
    "            x = mappings.shape[0]\n",
    "\n",
    "            for l in t_item:\n",
    "                if max_score[t_key][\"Score\"] < score_list[t_key][l]:\n",
    "                    max_score[t_key] = {\"Level\": l, \"Score\": score_list[t_key][l]}\n",
    "\n",
    "            if max_score[t_key][\"Level\"] != None:\n",
    "                # Generate output for sentence\n",
    "                mappings.at[x, \"Index\"] = i\n",
    "                mappings.at[x, \"Sentence\"] = sentence\n",
    "                mappings.at[x, \"Chosen Taxonomy\"] = t_key\n",
    "                mappings.at[x, \"Mapped Level\"] = max_score[t_key][\"Level\"]\n",
    "\n",
    "                # Generate identified verb and level tuples for sentence data\n",
    "                s_d_identified_verbs = []\n",
    "                for verb in identified_verbs:\n",
    "                    level = \"Verb not mapped\"\n",
    "                    try:\n",
    "                        level = verb_list.at[(t_key, verb), \"Level\"]\n",
    "                    except:\n",
    "                        pass\n",
    "                    finally:\n",
    "                        s_d_identified_verbs.append((verb, level))\n",
    "\n",
    "                mappings.at[x, \"Verbs Identified\"] = s_d_identified_verbs\n",
    "\n",
    "                # Generate suggested verbs\n",
    "                sim_verbs = (\n",
    "                    similar_verbs[t_key]\n",
    "                    .sort_values(by=[\"Similarity\"], ascending=False)\n",
    "                    .head(5)\n",
    "                )\n",
    "                suggested_verbs = []\n",
    "                if sim_verbs.shape[0] > 0:\n",
    "                    suggested_verbs = [\n",
    "                        (verb, sim_verbs.at[verb, \"Level\"]) for verb in sim_verbs.index\n",
    "                    ]\n",
    "                mappings.at[x, \"Suggested Verbs\"] = suggested_verbs\n",
    "\n",
    "    if calculate_accuracy_flag:\n",
    "        calculate_accuracy(mappings, final_levels)\n",
    "\n",
    "    return mappings\n",
    "\n",
    "# TODO: Classify the verbs in each of the learning outcomes\n",
    "ulo_sentences = p1[1].to_list()\n",
    "plo_sentences = df[1].to_list()\n",
    "\n",
    "# Thresholds to filter the similarity of words to improve accuracy\n",
    "suggested_sim_threshold = 0.985\n",
    "sim_threshold = 0.997\n",
    "\n",
    "# Example usage\n",
    "csv_file = \"Learning outcomes manual mapping - Mappings.csv\"\n",
    "# csv_file = 'Learning outcomes manual mapping - Mappings - Testing.csv'\n",
    "columns_to_extract = [\"LO\", \"Cognitive\", \"Affective\", \"Psychomotor\", \"SOLO\"]\n",
    "\n",
    "extracted_data = extract_columns(csv_file, columns_to_extract)\n",
    "\n",
    "final_levels = {\n",
    "    \"Cognitive\": extracted_data[\"Cognitive\"],\n",
    "    \"Affective\": extracted_data[\"Affective\"],\n",
    "    \"Psychomotor\": extracted_data[\"Psychomotor\"],\n",
    "    \"SOLO\": extracted_data[\"SOLO\"],\n",
    "}\n",
    "\n",
    "ulo_classifications = learning_outcome_mapping(ulo_sentences, final_levels, sim_threshold, suggested_sim_threshold, True)\n",
    "plo_classifications = learning_outcome_mapping(plo_sentences, final_levels, sim_threshold, suggested_sim_threshold, False)\n",
    "\n",
    "# Iteration 1: Assume both CLOs and POs are classifiable into Cognitive, Affective, Psychomotor Levels.\n",
    "# If levels match, boost the CLO to PO coefficient by 0.1\n",
    "# If levels different, don't boost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plo_classifications"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regulatory Mapping using the LO Mapper\n",
    "Uses the LO Mapper to factor in Program Outcome alignment for the curriculum mapper. Note we were not able to achieve any increase in accuracy through using the LO Mapper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def regulatory_requirement_mapping(SIM_THRESHOLD, SUGGESTED_SIM_THRESHOLD):\n",
    "\n",
    "    document = Document(PROGRAM_OUTCOME_TABLE)\n",
    "    table_num = 1\n",
    "    po_table = read_docx_table(document,table_num)\n",
    "\n",
    "    final_levels = []\n",
    "\n",
    "    # PO1 to PO12\n",
    "    for i in range(1,13):\n",
    "        current_po_rr_sentences = []\n",
    "        for j in range(1,4):\n",
    "            split_sentences = po_table[j][i].split(\";\")\n",
    "            for k in range(len(split_sentences)):\n",
    "                split_sentences[k] = split_sentences[k].replace(\"\\n\",\"\")\n",
    "            while(\"\" in split_sentences):\n",
    "                split_sentences.remove(\"\")\n",
    "            while(\" \" in split_sentences):\n",
    "                split_sentences.remove(\" \")\n",
    "\n",
    "            for k in range(len(split_sentences)):\n",
    "                current_po_rr_sentences.append(split_sentences[k])\n",
    "\n",
    "        # print(current_po_rr_sentences)\n",
    "        rr_clasifications = learning_outcome_mapping(current_po_rr_sentences, final_levels, SIM_THRESHOLD, SUGGESTED_SIM_THRESHOLD, False)\n",
    "\n",
    "        mapped_levels = []\n",
    "        for j in range(len(rr_clasifications[\"Mapped Level\"])):\n",
    "            mapped_levels.append(rr_clasifications[\"Mapped Level\"][j])\n",
    "        final_level = max(mapped_levels,key=mapped_levels.count)\n",
    "        cnt = 0\n",
    "        for level in mapped_levels:\n",
    "            if level == final_level:\n",
    "                cnt += 1\n",
    "                \n",
    "        if plo_classifications['Mapped Level'][i-1] != final_level and cnt > 1:\n",
    "            final_levels.append(final_level)\n",
    "        else:\n",
    "            final_levels.append(plo_classifications['Mapped Level'][i-1])\n",
    "    return final_levels\n",
    "\n",
    "# final_levels = regulatory_requirement_mapping(sim_threshold, suggested_sim_threshold)\n",
    "# for i in range(len(final_levels)):\n",
    "#     plo_classifications[\"Mapped Level\"][i] = final_levels[i]\n",
    "# plo_classifications"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ADJUSTING THRESHOLDS USING THE LEVELS FOUND FOR LO Mapper & PO Mapper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy the original dd dataframe to compare w/ adjusted mappings.\n",
    "dd_original = dd.copy()\n",
    "print(dd_original)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for classification in ulo_classifications:\n",
    "#     print(classification)\n",
    "\n",
    "# BASE ALGORITHM ACCURACY (w/ no learning outcome mapper), ADJUSTMENT_THRESHOLD=0:\n",
    "# 0.8544061302681993\n",
    "# ALGORITHM ACCURACY (w/ new learning outcome mapper), ADJUSTMENT_THRESHOLD=0.1:\n",
    "# 0.8927203065134102\n",
    "# ALGORITHM ACCURACY (w/ new learning outcome mapper), ADJUSTMENT_THRESHOLD=0.15:\n",
    "# 0.8936781609195404\n",
    "# ALGORITHM ACCURACY (w/ new learning outcome mapper), ADJUSTMENT_THRESHOLD=0.2:\n",
    "# 0.8946360153256706\n",
    "# ALGORITHM ACCURACY (w/ new learning outcome mapper), ADJUSTMENT_THRESHOLD=0.25:\n",
    "# 0.8946360153256707\n",
    "# ALGORITHM ACCURACY (w/ new learning outcome mapper), ADJUSTMENT_THRESHOLD=0.3:\n",
    "# 0.8869731800766286\n",
    "# ALGORITHM ACCURACY (w/ new learning outcome mapper), ADJUSTMENT_THRESHOLD=0.4:\n",
    "# 0.8764367816091956\n",
    "# ALGORITHM ACCURACY (w/ new learning outcome mapper), ADJUSTMENT_THRESHOLD=0.5:\n",
    "# 0.8678160919540232\n",
    "# ALGORITHM ACCURACY (w/ new learning outcome mapper), ADJUSTMENT_THRESHOLD=1:\n",
    "# 0.8678160919540232\n",
    "\n",
    "data = {\n",
    "    'ulo': [],\n",
    "    'plo': [],\n",
    "    'ulo_level': [],\n",
    "    'plo_level': [],\n",
    "}\n",
    "po_mappings = pd.DataFrame(data)\n",
    "\n",
    "# Goes through each cell in the LO->PO Mapping table and adjusts the value using a static ADJUSTMENT_THRESHOLD Value.\n",
    "for x in range(len(plo_classifications['Mapped Level'])):\n",
    "    for i in range(len(ulo_classifications['Mapped Level'])):\n",
    "\n",
    "        new_row = pd.DataFrame({'ulo': [ulo_classifications['Sentence'][i]], 'plo': [plo_classifications['Sentence'][x]], 'ulo_level': [ulo_classifications['Mapped Level'][i]], 'plo_level': [plo_classifications['Mapped Level'][x]]})\n",
    "        if ulo_classifications['Mapped Level'][i] == plo_classifications['Mapped Level'][x]:\n",
    "            \n",
    "            po_mappings = pd.concat([new_row, po_mappings], ignore_index=True)\n",
    "            # po_mappings.loc[0] = [ulo_classifications['Sentence'][i], plo_classifications['Sentence'][x], ulo_classifications['Mapped Level'][i],plo_classifications['Mapped Level'][x]]  # adding a row\n",
    "        else:\n",
    "            po_mappings = po_mappings.append(new_row, ignore_index=True)\n",
    "            # po_mappings.loc[-1] = [ulo_classifications['Sentence'][i], plo_classifications['Sentence'][x], ulo_classifications['Mapped Level'][i],plo_classifications['Mapped Level'][x]]  # adding a row\n",
    "        # po_mappings.index = po_mappings.index + 1  # shifting index\n",
    "\n",
    "        if ulo_classifications['Mapped Level'][i] == plo_classifications['Mapped Level'][x]:\n",
    "            if dd[PO_LABEL+str(x+1)][i] + ADJUSTMENT_THRESHOLD <= 1:\n",
    "                dd[PO_LABEL+str(x+1)][i] += ADJUSTMENT_THRESHOLD # Add Offset\n",
    "            else:\n",
    "                dd[PO_LABEL+str(x+1)][i] = 1\n",
    "        else:\n",
    "            if dd[PO_LABEL+str(x+1)][i] - ADJUSTMENT_THRESHOLD >= 0:\n",
    "                dd[PO_LABEL+str(x+1)][i] -= ADJUSTMENT_THRESHOLD # Add Offset\n",
    "            else:\n",
    "                dd[PO_LABEL+str(x+1)][i] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(po_mappings[\"ulo_level\"]+\"-\"+po_mappings[\"plo_level\"])m\n",
    "print(po_mappings)\n",
    "export_dataframe(po_mappings, \"./outputs/po_mappings.xlsx\", \"po_mappings\", po_mappings.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store the matrix into csv file\n",
    "dd.to_csv('pseudocodematrix.csv', index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "JYhDRATaZU3Y"
   },
   "source": [
    "## Setting threshold value (taking min and max of each column and divided by 2)\n",
    "## threshold value = (min +max)/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OUqMq9LoZGMj",
    "outputId": "4c008335-10b4-41b8-a2b6-bdbc6c2ebe89"
   },
   "outputs": [],
   "source": [
    "\n",
    "# TODO: Alter the threshold based on matching hierarchy type & bloom verb instead of simply using (column_max+column_min)/2\n",
    "\n",
    "# Setting threshold value\n",
    "# Taking min max average of each column and set that as a threshold value\n",
    "\n",
    "# This will change the coefficients into 0 or 1 mappings in the dd dataframe\n",
    "for x in range(len(df)):\n",
    "    tes = dd[PO_LABEL+str(x+1)].values.min()\n",
    "    tes1 = dd[PO_LABEL+str(x+1)].values.max()\n",
    "    tt1 = (tes+tes1)/2\n",
    "\n",
    "    if tt1 == 0:\n",
    "      dd[PO_LABEL+str(x+1)] = dd[PO_LABEL+str(x+1)] \n",
    "    else:\n",
    "      dd[PO_LABEL+str(x+1)] = dd[PO_LABEL+str(x+1)].apply(lambda x: 1 if x >= tt1 else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# TODO: Alter the threshold based on matching hierarchy type & bloom verb instead of simply using (column_max+column_min)/2\n",
    "\n",
    "# Setting threshold value \n",
    "# Taking min max average of each column and set that as a threshold value\n",
    "\n",
    "# This will change the coefficients into 0 or 1 mappings in the dd dataframe\n",
    "for x in range(len(df)):\n",
    "    tes = dd_original[PO_LABEL+str(x+1)].values.min()\n",
    "    tes1 = dd_original[PO_LABEL+str(x+1)].values.max()\n",
    "    tt1 = (tes+tes1)/2\n",
    "    \n",
    "    if tt1 == 0:\n",
    "      dd_original[PO_LABEL+str(x+1)] = dd_original[PO_LABEL+str(x+1)] \n",
    "    else:\n",
    "      dd_original[PO_LABEL+str(x+1)] = dd_original[PO_LABEL+str(x+1)].apply(lambda x: 1 if x >= tt1 else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "t1V7uvAxZGPR"
   },
   "outputs": [],
   "source": [
    "\n",
    "dd.to_csv('PLO-CLOmapping.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "0MmjZwIBo05H",
    "outputId": "f1c5ee61-074f-4df0-aa58-19e4a6fc2566"
   },
   "outputs": [],
   "source": [
    "dd.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "nQWRhdsfo1pX",
    "outputId": "c75e374d-17a3-4005-d3d1-49438be48ddf"
   },
   "outputs": [],
   "source": [
    "# human generated output\n",
    "d= pd.read_csv(ORIGINAL_MAPPING)\n",
    "d.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_I05ih9go1s-"
   },
   "outputs": [],
   "source": [
    "# Duplicate over a dataframe for accuracy calculate & comparison.\n",
    "df3 = d.copy()\n",
    "df3_original = d.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "n6YEK04Ko1we"
   },
   "outputs": [],
   "source": [
    "# Check whether the levels of the original and automatic mapping match or not.\n",
    "for x in range(len(df)):\n",
    "  df3[PO_LABEL+str(x+1)] = np.where(dd[PO_LABEL+str(x+1)] == df3[PO_LABEL+str(x+1)], 'True', 'False')\n",
    "  df3_original[PO_LABEL+str(x+1)] = np.where(dd_original[PO_LABEL+str(x+1)] == df3_original[PO_LABEL+str(x+1)], 'True', 'False')\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "8FOKLSmFo1zO",
    "outputId": "8383f47b-bd17-4cdb-9bc2-a2cd491cd082"
   },
   "outputs": [],
   "source": [
    "df3.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3_original.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "J9fhnvqeo12P"
   },
   "outputs": [],
   "source": [
    "# Transform into something more readable than 1s or 0s\n",
    "for x in range(len(df)):\n",
    "  df3[PO_LABEL+str(x+1)] = df3[PO_LABEL+str(x+1)].replace('True', 1)\n",
    "  df3[PO_LABEL+str(x+1)] = df3[PO_LABEL+str(x+1)].replace('False', 0)\n",
    "  \n",
    "  df3_original[PO_LABEL+str(x+1)] = df3_original[PO_LABEL+str(x+1)].replace('True', 1)\n",
    "  df3_original[PO_LABEL+str(x+1)] = df3_original[PO_LABEL+str(x+1)].replace('False', 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "vML-4JNUo15u",
    "outputId": "c32db157-b88b-43f8-d0cc-d3632146294c"
   },
   "outputs": [],
   "source": [
    "df3.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3_original.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "tANDI1-6pBeH",
    "outputId": "e815045e-ceab-45b5-ef45-7f3db3a7ce34"
   },
   "outputs": [],
   "source": [
    "# calculating accuracy of the table\n",
    "df3['acc'] = df3.mean(axis=1)\n",
    "df3.head(n=100)\n",
    "\n",
    "# calculating accuracy of the table\n",
    "df3_original['acc'] = df3_original.mean(axis=1)\n",
    "df3_original.head(n=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "NSSFJ-0ppBhX",
    "outputId": "70654783-735c-474e-f592-dc5605a085a0"
   },
   "outputs": [],
   "source": [
    "df4 = pd.concat([df1[0],df1[1], df3], axis=1)\n",
    "df4.head(n=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df4_original = pd.concat([df1[0],df1[1], df3_original], axis=1)\n",
    "\n",
    "df4[\"acc_improved\"] = df4['acc'] >= df4_original['acc']\n",
    "df4[\"acc_improved_by\"] = df4['acc'] - df4_original['acc']\n",
    "\n",
    "df4_original.head(n=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 237
    },
    "id": "MXK949BmpBkX",
    "outputId": "05f31e23-74d6-4796-bba9-2fb7d04ec44c"
   },
   "outputs": [],
   "source": [
    "df4.set_index(0, inplace=True)\n",
    "df4.head(n=100)\n",
    "\n",
    "\n",
    "df4.to_csv('PO_Mapper_WasMappingSuccessful.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df4_original.set_index(0, inplace=True)\n",
    "df4_original.head(n=100)\n",
    "\n",
    "df4_original.to_csv('PO_Mapper_WasMappingSuccessful_original.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = df4.merge(df4_original,how='inner')\n",
    "# merged_df['PO1']\n",
    "# merged_df = merged_df[merged_df['Value1'] == merged_df['Value2']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base algorithm accuracy\n",
    "df4_original['acc'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gT1jyiUtpBnf",
    "outputId": "32484ddf-a355-4f14-c865-b22e05e9b9dd"
   },
   "outputs": [],
   "source": [
    "# Accuracy with LO mapper weightings factored in.\n",
    "df4['acc'].mean()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "CLOPLO_using_(minmax_2)_threshold_value_(pseudocode).ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3.9.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "11938c6bc6919ae2720b4d5011047913343b08a43b18698fd82dedb0d4417594"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
