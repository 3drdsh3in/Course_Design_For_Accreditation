{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: docx2txt in /home/arragon/.local/lib/python3.10/site-packages (0.8)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: gensim in /home/arragon/.local/lib/python3.10/site-packages (4.3.1)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in /home/arragon/.local/lib/python3.10/site-packages (from gensim) (6.3.0)\n",
      "Requirement already satisfied: numpy>=1.18.5 in /home/arragon/.local/lib/python3.10/site-packages (from gensim) (1.24.3)\n",
      "Requirement already satisfied: scipy>=1.7.0 in /home/arragon/.local/lib/python3.10/site-packages (from gensim) (1.11.1)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: keras in /home/arragon/.local/lib/python3.10/site-packages (2.13.1)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: nltk in /home/arragon/.local/lib/python3.10/site-packages (3.8.1)\n",
      "Requirement already satisfied: click in /home/arragon/.local/lib/python3.10/site-packages (from nltk) (8.1.6)\n",
      "Requirement already satisfied: tqdm in /home/arragon/.local/lib/python3.10/site-packages (from nltk) (4.65.1)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /home/arragon/.local/lib/python3.10/site-packages (from nltk) (2023.6.3)\n",
      "Requirement already satisfied: joblib in /home/arragon/.local/lib/python3.10/site-packages (from nltk) (1.3.1)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: scikit-learn in /home/arragon/.local/lib/python3.10/site-packages (1.3.0)\n",
      "Requirement already satisfied: numpy>=1.17.3 in /home/arragon/.local/lib/python3.10/site-packages (from scikit-learn) (1.24.3)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /home/arragon/.local/lib/python3.10/site-packages (from scikit-learn) (1.3.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/arragon/.local/lib/python3.10/site-packages (from scikit-learn) (3.2.0)\n",
      "Requirement already satisfied: scipy>=1.5.0 in /home/arragon/.local/lib/python3.10/site-packages (from scikit-learn) (1.11.1)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: python-docx in /home/arragon/.local/lib/python3.10/site-packages (0.8.11)\n",
      "Requirement already satisfied: lxml>=2.3.2 in /home/arragon/.local/lib/python3.10/site-packages (from python-docx) (4.9.3)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: tensorflow in /home/arragon/.local/lib/python3.10/site-packages (2.13.0)\n",
      "Requirement already satisfied: numpy<=1.24.3,>=1.22 in /home/arragon/.local/lib/python3.10/site-packages (from tensorflow) (1.24.3)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /home/arragon/.local/lib/python3.10/site-packages (from tensorflow) (1.4.0)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /home/arragon/.local/lib/python3.10/site-packages (from tensorflow) (3.3.0)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /home/arragon/.local/lib/python3.10/site-packages (from tensorflow) (0.33.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /home/arragon/.local/lib/python3.10/site-packages (from tensorflow) (1.56.2)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /home/arragon/.local/lib/python3.10/site-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: tensorflow-estimator<2.14,>=2.13.0 in /home/arragon/.local/lib/python3.10/site-packages (from tensorflow) (2.13.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /home/arragon/.local/lib/python3.10/site-packages (from tensorflow) (1.15.0)\n",
      "Requirement already satisfied: six>=1.12.0 in /home/arragon/.local/lib/python3.10/site-packages (from tensorflow) (1.12.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /home/arragon/.local/lib/python3.10/site-packages (from tensorflow) (2.3.0)\n",
      "Requirement already satisfied: packaging in /home/arragon/.local/lib/python3.10/site-packages (from tensorflow) (23.1)\n",
      "Requirement already satisfied: setuptools in /home/arragon/.local/lib/python3.10/site-packages (from tensorflow) (68.0.0)\n",
      "Requirement already satisfied: flatbuffers>=23.1.21 in /home/arragon/.local/lib/python3.10/site-packages (from tensorflow) (23.5.26)\n",
      "Requirement already satisfied: keras<2.14,>=2.13.1 in /home/arragon/.local/lib/python3.10/site-packages (from tensorflow) (2.13.1)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /home/arragon/.local/lib/python3.10/site-packages (from tensorflow) (4.23.4)\n",
      "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /home/arragon/.local/lib/python3.10/site-packages (from tensorflow) (0.4.0)\n",
      "Requirement already satisfied: typing-extensions<4.6.0,>=3.6.6 in /home/arragon/.local/lib/python3.10/site-packages (from tensorflow) (4.5.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /home/arragon/.local/lib/python3.10/site-packages (from tensorflow) (16.0.6)\n",
      "Requirement already satisfied: tensorboard<2.14,>=2.13 in /home/arragon/.local/lib/python3.10/site-packages (from tensorflow) (2.13.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /home/arragon/.local/lib/python3.10/site-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied: h5py>=2.9.0 in /home/arragon/.local/lib/python3.10/site-packages (from tensorflow) (3.9.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/lib/python3/dist-packages (from astunparse>=1.6.0->tensorflow) (0.37.1)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /home/arragon/.local/lib/python3.10/site-packages (from tensorboard<2.14,>=2.13->tensorflow) (2.3.6)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /home/arragon/.local/lib/python3.10/site-packages (from tensorboard<2.14,>=2.13->tensorflow) (0.7.1)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /home/arragon/.local/lib/python3.10/site-packages (from tensorboard<2.14,>=2.13->tensorflow) (2.22.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /home/arragon/.local/lib/python3.10/site-packages (from tensorboard<2.14,>=2.13->tensorflow) (3.4.4)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /home/arragon/.local/lib/python3.10/site-packages (from tensorboard<2.14,>=2.13->tensorflow) (2.22.0)\n",
      "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /home/arragon/.local/lib/python3.10/site-packages (from tensorboard<2.14,>=2.13->tensorflow) (1.0.0)\n",
      "Requirement already satisfied: urllib3<2.0 in /home/arragon/.local/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow) (1.25.3)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /home/arragon/.local/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow) (5.3.1)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /home/arragon/.local/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow) (0.3.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /home/arragon/.local/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow) (3.4.2)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /home/arragon/.local/lib/python3.10/site-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.14,>=2.13->tensorflow) (1.3.1)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /home/arragon/.local/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/arragon/.local/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow) (2019.6.16)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /home/arragon/.local/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow) (2.8)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /home/arragon/.local/lib/python3.10/site-packages (from werkzeug>=1.0.1->tensorboard<2.14,>=2.13->tensorflow) (2.1.3)\n",
      "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /home/arragon/.local/lib/python3.10/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow) (0.5.0)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /usr/lib/python3/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.14,>=2.13->tensorflow) (3.2.0)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: pandas in /home/arragon/.local/lib/python3.10/site-packages (2.0.3)\n",
      "Requirement already satisfied: numpy>=1.21.0 in /home/arragon/.local/lib/python3.10/site-packages (from pandas) (1.24.3)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/arragon/.local/lib/python3.10/site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /home/arragon/.local/lib/python3.10/site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/arragon/.local/lib/python3.10/site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in /home/arragon/.local/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas) (1.12.0)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: openpyxl in /home/arragon/.local/lib/python3.10/site-packages (3.1.2)\n",
      "Requirement already satisfied: et-xmlfile in /home/arragon/.local/lib/python3.10/site-packages (from openpyxl) (1.1.0)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: nltk in /home/arragon/.local/lib/python3.10/site-packages (3.8.1)\n",
      "Requirement already satisfied: joblib in /home/arragon/.local/lib/python3.10/site-packages (from nltk) (1.3.1)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /home/arragon/.local/lib/python3.10/site-packages (from nltk) (2023.6.3)\n",
      "Requirement already satisfied: tqdm in /home/arragon/.local/lib/python3.10/site-packages (from nltk) (4.65.1)\n",
      "Requirement already satisfied: click in /home/arragon/.local/lib/python3.10/site-packages (from nltk) (8.1.6)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: spacy in /home/arragon/.local/lib/python3.10/site-packages (3.6.0)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /home/arragon/.local/lib/python3.10/site-packages (from spacy) (2.0.9)\n",
      "Requirement already satisfied: setuptools in /home/arragon/.local/lib/python3.10/site-packages (from spacy) (68.0.0)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /home/arragon/.local/lib/python3.10/site-packages (from spacy) (6.3.0)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /home/arragon/.local/lib/python3.10/site-packages (from spacy) (3.0.12)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /home/arragon/.local/lib/python3.10/site-packages (from spacy) (1.0.9)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /home/arragon/.local/lib/python3.10/site-packages (from spacy) (3.3.0)\n",
      "Requirement already satisfied: jinja2 in /home/arragon/.local/lib/python3.10/site-packages (from spacy) (3.1.2)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /home/arragon/.local/lib/python3.10/site-packages (from spacy) (3.0.8)\n",
      "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /home/arragon/.local/lib/python3.10/site-packages (from spacy) (0.9.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/arragon/.local/lib/python3.10/site-packages (from spacy) (23.1)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /home/arragon/.local/lib/python3.10/site-packages (from spacy) (2.22.0)\n",
      "Requirement already satisfied: pathy>=0.10.0 in /home/arragon/.local/lib/python3.10/site-packages (from spacy) (0.10.2)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /home/arragon/.local/lib/python3.10/site-packages (from spacy) (1.24.3)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /home/arragon/.local/lib/python3.10/site-packages (from spacy) (1.1.2)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /home/arragon/.local/lib/python3.10/site-packages (from spacy) (2.4.7)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /home/arragon/.local/lib/python3.10/site-packages (from spacy) (2.0.7)\n",
      "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in /home/arragon/.local/lib/python3.10/site-packages (from spacy) (8.1.11)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /home/arragon/.local/lib/python3.10/site-packages (from spacy) (1.0.4)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /home/arragon/.local/lib/python3.10/site-packages (from spacy) (4.65.1)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /home/arragon/.local/lib/python3.10/site-packages (from spacy) (1.10.12)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in /home/arragon/.local/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy) (4.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/arragon/.local/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2019.6.16)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /home/arragon/.local/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy) (1.25.3)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /home/arragon/.local/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.8)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /home/arragon/.local/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.0.4)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /home/arragon/.local/lib/python3.10/site-packages (from thinc<8.2.0,>=8.1.8->spacy) (0.7.10)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /home/arragon/.local/lib/python3.10/site-packages (from thinc<8.2.0,>=8.1.8->spacy) (0.1.1)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /home/arragon/.local/lib/python3.10/site-packages (from typer<0.10.0,>=0.3.0->spacy) (8.1.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/arragon/.local/lib/python3.10/site-packages (from jinja2->spacy) (2.1.3)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not find a version that satisfies the requirement csv (from versions: none)\n",
      "ERROR: No matching distribution found for csv\n",
      "2023-08-31 15:36:09.378767: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-08-31 15:36:10.041826: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "2023-08-31 15:36:10.911510: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] could not open file to read NUMA node: /sys/bus/pci/devices/0000:29:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-08-31 15:36:10.911869: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1960] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting en-core-web-sm==3.6.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.6.0/en_core_web_sm-3.6.0-py3-none-any.whl (12.8 MB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 12.8/12.8 MB 11.7 MB/s eta 0:00:00\n",
      "Requirement already satisfied: spacy<3.7.0,>=3.6.0 in /home/arragon/.local/lib/python3.10/site-packages (from en-core-web-sm==3.6.0) (3.6.0)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /home/arragon/.local/lib/python3.10/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (1.0.4)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /home/arragon/.local/lib/python3.10/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.4.7)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /home/arragon/.local/lib/python3.10/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (1.1.2)\n",
      "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /home/arragon/.local/lib/python3.10/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (0.9.0)\n",
      "Requirement already satisfied: jinja2 in /home/arragon/.local/lib/python3.10/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (3.1.2)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /home/arragon/.local/lib/python3.10/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (3.0.12)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/arragon/.local/lib/python3.10/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (23.1)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /home/arragon/.local/lib/python3.10/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (1.24.3)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /home/arragon/.local/lib/python3.10/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (3.3.0)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /home/arragon/.local/lib/python3.10/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (3.0.8)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /home/arragon/.local/lib/python3.10/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.22.0)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /home/arragon/.local/lib/python3.10/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (1.0.9)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /home/arragon/.local/lib/python3.10/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.0.9)\n",
      "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in /home/arragon/.local/lib/python3.10/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (8.1.11)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /home/arragon/.local/lib/python3.10/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (1.10.12)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /home/arragon/.local/lib/python3.10/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.0.7)\n",
      "Requirement already satisfied: pathy>=0.10.0 in /home/arragon/.local/lib/python3.10/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (0.10.2)\n",
      "Requirement already satisfied: setuptools in /home/arragon/.local/lib/python3.10/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (68.0.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /home/arragon/.local/lib/python3.10/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (4.65.1)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /home/arragon/.local/lib/python3.10/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (6.3.0)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in /home/arragon/.local/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (4.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/arragon/.local/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2019.6.16)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /home/arragon/.local/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.8)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /home/arragon/.local/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (3.0.4)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /home/arragon/.local/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (1.25.3)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /home/arragon/.local/lib/python3.10/site-packages (from thinc<8.2.0,>=8.1.8->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (0.1.1)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /home/arragon/.local/lib/python3.10/site-packages (from thinc<8.2.0,>=8.1.8->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (0.7.10)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /home/arragon/.local/lib/python3.10/site-packages (from typer<0.10.0,>=0.3.0->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (8.1.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/arragon/.local/lib/python3.10/site-packages (from jinja2->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.1.3)\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-31 15:36:17.957124: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-08-31 15:36:18.623433: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "2023-08-31 15:36:19.446229: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] could not open file to read NUMA node: /sys/bus/pci/devices/0000:29:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-08-31 15:36:19.446549: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1960] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting en-core-web-trf==3.6.1\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_trf-3.6.1/en_core_web_trf-3.6.1-py3-none-any.whl (460.3 MB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 460.3/460.3 MB 2.7 MB/s eta 0:00:00\n",
      "Requirement already satisfied: spacy-transformers<1.3.0,>=1.2.2 in /home/arragon/.local/lib/python3.10/site-packages (from en-core-web-trf==3.6.1) (1.2.5)\n",
      "Requirement already satisfied: spacy<3.7.0,>=3.6.0 in /home/arragon/.local/lib/python3.10/site-packages (from en-core-web-trf==3.6.1) (3.6.0)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /home/arragon/.local/lib/python3.10/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-trf==3.6.1) (3.3.0)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /home/arragon/.local/lib/python3.10/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-trf==3.6.1) (1.24.3)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /home/arragon/.local/lib/python3.10/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-trf==3.6.1) (2.0.7)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /home/arragon/.local/lib/python3.10/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-trf==3.6.1) (1.10.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /home/arragon/.local/lib/python3.10/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-trf==3.6.1) (1.0.4)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /home/arragon/.local/lib/python3.10/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-trf==3.6.1) (2.0.9)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /home/arragon/.local/lib/python3.10/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-trf==3.6.1) (1.1.2)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /home/arragon/.local/lib/python3.10/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-trf==3.6.1) (1.0.9)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /home/arragon/.local/lib/python3.10/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-trf==3.6.1) (4.65.1)\n",
      "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /home/arragon/.local/lib/python3.10/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-trf==3.6.1) (0.9.0)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /home/arragon/.local/lib/python3.10/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-trf==3.6.1) (2.22.0)\n",
      "Requirement already satisfied: setuptools in /home/arragon/.local/lib/python3.10/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-trf==3.6.1) (68.0.0)\n",
      "Requirement already satisfied: jinja2 in /home/arragon/.local/lib/python3.10/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-trf==3.6.1) (3.1.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/arragon/.local/lib/python3.10/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-trf==3.6.1) (23.1)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /home/arragon/.local/lib/python3.10/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-trf==3.6.1) (3.0.12)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /home/arragon/.local/lib/python3.10/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-trf==3.6.1) (6.3.0)\n",
      "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in /home/arragon/.local/lib/python3.10/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-trf==3.6.1) (8.1.11)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /home/arragon/.local/lib/python3.10/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-trf==3.6.1) (2.4.7)\n",
      "Requirement already satisfied: pathy>=0.10.0 in /home/arragon/.local/lib/python3.10/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-trf==3.6.1) (0.10.2)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /home/arragon/.local/lib/python3.10/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-trf==3.6.1) (3.0.8)\n",
      "Requirement already satisfied: spacy-alignments<1.0.0,>=0.7.2 in /home/arragon/.local/lib/python3.10/site-packages (from spacy-transformers<1.3.0,>=1.2.2->en-core-web-trf==3.6.1) (0.9.0)\n",
      "Requirement already satisfied: transformers<4.31.0,>=3.4.0 in /home/arragon/.local/lib/python3.10/site-packages (from spacy-transformers<1.3.0,>=1.2.2->en-core-web-trf==3.6.1) (4.30.2)\n",
      "Requirement already satisfied: torch>=1.8.0 in /home/arragon/.local/lib/python3.10/site-packages (from spacy-transformers<1.3.0,>=1.2.2->en-core-web-trf==3.6.1) (2.0.1)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in /home/arragon/.local/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy<3.7.0,>=3.6.0->en-core-web-trf==3.6.1) (4.5.0)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /home/arragon/.local/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->en-core-web-trf==3.6.1) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/arragon/.local/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->en-core-web-trf==3.6.1) (2019.6.16)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /home/arragon/.local/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->en-core-web-trf==3.6.1) (1.25.3)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /home/arragon/.local/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->en-core-web-trf==3.6.1) (2.8)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /home/arragon/.local/lib/python3.10/site-packages (from thinc<8.2.0,>=8.1.8->spacy<3.7.0,>=3.6.0->en-core-web-trf==3.6.1) (0.1.1)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /home/arragon/.local/lib/python3.10/site-packages (from thinc<8.2.0,>=8.1.8->spacy<3.7.0,>=3.6.0->en-core-web-trf==3.6.1) (0.7.10)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in /home/arragon/.local/lib/python3.10/site-packages (from torch>=1.8.0->spacy-transformers<1.3.0,>=1.2.2->en-core-web-trf==3.6.1) (11.7.99)\n",
      "Requirement already satisfied: nvidia-cusolver-cu11==11.4.0.1 in /home/arragon/.local/lib/python3.10/site-packages (from torch>=1.8.0->spacy-transformers<1.3.0,>=1.2.2->en-core-web-trf==3.6.1) (11.4.0.1)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu11==11.7.101 in /home/arragon/.local/lib/python3.10/site-packages (from torch>=1.8.0->spacy-transformers<1.3.0,>=1.2.2->en-core-web-trf==3.6.1) (11.7.101)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in /home/arragon/.local/lib/python3.10/site-packages (from torch>=1.8.0->spacy-transformers<1.3.0,>=1.2.2->en-core-web-trf==3.6.1) (11.7.99)\n",
      "Requirement already satisfied: networkx in /home/arragon/.local/lib/python3.10/site-packages (from torch>=1.8.0->spacy-transformers<1.3.0,>=1.2.2->en-core-web-trf==3.6.1) (3.1)\n",
      "Requirement already satisfied: triton==2.0.0 in /home/arragon/.local/lib/python3.10/site-packages (from torch>=1.8.0->spacy-transformers<1.3.0,>=1.2.2->en-core-web-trf==3.6.1) (2.0.0)\n",
      "Requirement already satisfied: nvidia-nvtx-cu11==11.7.91 in /home/arragon/.local/lib/python3.10/site-packages (from torch>=1.8.0->spacy-transformers<1.3.0,>=1.2.2->en-core-web-trf==3.6.1) (11.7.91)\n",
      "Requirement already satisfied: nvidia-curand-cu11==10.2.10.91 in /home/arragon/.local/lib/python3.10/site-packages (from torch>=1.8.0->spacy-transformers<1.3.0,>=1.2.2->en-core-web-trf==3.6.1) (10.2.10.91)\n",
      "Requirement already satisfied: nvidia-cufft-cu11==10.9.0.58 in /home/arragon/.local/lib/python3.10/site-packages (from torch>=1.8.0->spacy-transformers<1.3.0,>=1.2.2->en-core-web-trf==3.6.1) (10.9.0.58)\n",
      "Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in /home/arragon/.local/lib/python3.10/site-packages (from torch>=1.8.0->spacy-transformers<1.3.0,>=1.2.2->en-core-web-trf==3.6.1) (8.5.0.96)\n",
      "Requirement already satisfied: filelock in /home/arragon/.local/lib/python3.10/site-packages (from torch>=1.8.0->spacy-transformers<1.3.0,>=1.2.2->en-core-web-trf==3.6.1) (3.12.2)\n",
      "Requirement already satisfied: nvidia-cusparse-cu11==11.7.4.91 in /home/arragon/.local/lib/python3.10/site-packages (from torch>=1.8.0->spacy-transformers<1.3.0,>=1.2.2->en-core-web-trf==3.6.1) (11.7.4.91)\n",
      "Requirement already satisfied: nvidia-nccl-cu11==2.14.3 in /home/arragon/.local/lib/python3.10/site-packages (from torch>=1.8.0->spacy-transformers<1.3.0,>=1.2.2->en-core-web-trf==3.6.1) (2.14.3)\n",
      "Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in /home/arragon/.local/lib/python3.10/site-packages (from torch>=1.8.0->spacy-transformers<1.3.0,>=1.2.2->en-core-web-trf==3.6.1) (11.10.3.66)\n",
      "Requirement already satisfied: sympy in /home/arragon/.local/lib/python3.10/site-packages (from torch>=1.8.0->spacy-transformers<1.3.0,>=1.2.2->en-core-web-trf==3.6.1) (1.12)\n",
      "Requirement already satisfied: wheel in /usr/lib/python3/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch>=1.8.0->spacy-transformers<1.3.0,>=1.2.2->en-core-web-trf==3.6.1) (0.37.1)\n",
      "Requirement already satisfied: lit in /home/arragon/.local/lib/python3.10/site-packages (from triton==2.0.0->torch>=1.8.0->spacy-transformers<1.3.0,>=1.2.2->en-core-web-trf==3.6.1) (16.0.6)\n",
      "Requirement already satisfied: cmake in /home/arragon/.local/lib/python3.10/site-packages (from triton==2.0.0->torch>=1.8.0->spacy-transformers<1.3.0,>=1.2.2->en-core-web-trf==3.6.1) (3.27.2)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /home/arragon/.local/lib/python3.10/site-packages (from transformers<4.31.0,>=3.4.0->spacy-transformers<1.3.0,>=1.2.2->en-core-web-trf==3.6.1) (0.3.3)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /home/arragon/.local/lib/python3.10/site-packages (from transformers<4.31.0,>=3.4.0->spacy-transformers<1.3.0,>=1.2.2->en-core-web-trf==3.6.1) (0.13.3)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /home/arragon/.local/lib/python3.10/site-packages (from transformers<4.31.0,>=3.4.0->spacy-transformers<1.3.0,>=1.2.2->en-core-web-trf==3.6.1) (0.16.4)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/arragon/.local/lib/python3.10/site-packages (from transformers<4.31.0,>=3.4.0->spacy-transformers<1.3.0,>=1.2.2->en-core-web-trf==3.6.1) (5.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/arragon/.local/lib/python3.10/site-packages (from transformers<4.31.0,>=3.4.0->spacy-transformers<1.3.0,>=1.2.2->en-core-web-trf==3.6.1) (2023.6.3)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /home/arragon/.local/lib/python3.10/site-packages (from typer<0.10.0,>=0.3.0->spacy<3.7.0,>=3.6.0->en-core-web-trf==3.6.1) (8.1.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/arragon/.local/lib/python3.10/site-packages (from jinja2->spacy<3.7.0,>=3.6.0->en-core-web-trf==3.6.1) (2.1.3)\n",
      "Requirement already satisfied: fsspec in /home/arragon/.local/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.14.1->transformers<4.31.0,>=3.4.0->spacy-transformers<1.3.0,>=1.2.2->en-core-web-trf==3.6.1) (2023.6.0)\n",
      "Requirement already satisfied: mpmath>=0.19 in /home/arragon/.local/lib/python3.10/site-packages (from sympy->torch>=1.8.0->spacy-transformers<1.3.0,>=1.2.2->en-core-web-trf==3.6.1) (1.3.0)\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_trf')\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "# Dependencies\n",
    "pip install docx2txt\n",
    "pip install gensim\n",
    "pip install keras\n",
    "pip install nltk\n",
    "pip install -U scikit-learn\n",
    "pip install python-docx\n",
    "pip install tensorflow\n",
    "pip install pandas\n",
    "pip install openpyxl\n",
    "pip install nltk\n",
    "pip install spacy\n",
    "pip install csv\n",
    "python3 -m spacy download en_core_web_sm\n",
    "python3 -m spacy download en_core_web_trf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.test.utils import common_texts\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "import pandas as pd\n",
    "import csv\n",
    "import math\n",
    "import pprint\n",
    "import re\n",
    "\n",
    "import spacy\n",
    "# Use if en_core_web_sm not installable via python3 in terminal:\n",
    "# spacy.cli.download(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def verb_classifier(verbs_file_path): # verbs_file_path - string value that contains the file path\n",
    "\n",
    "    xlsx = pd.ExcelFile(verbs_file_path, engine='openpyxl') \n",
    "\n",
    "    sheet_names = xlsx.sheet_names  # Get a list of sheet names\n",
    "\n",
    "    # Create an empty dictionary to store DataFrames for each sheet\n",
    "    dfs = {}\n",
    "\n",
    "    for sheet_name in sheet_names:\n",
    "        df = xlsx.parse(sheet_name)  # For XLSX files\n",
    "        \n",
    "        # Store the DataFrame in the dictionary\n",
    "        dfs[sheet_name] = df\n",
    "\n",
    "    domain_levels = pd.concat(dfs)\n",
    "    duplicate_checklist = []\n",
    "\n",
    "    for i in range(domain_levels.shape[0]):\n",
    "        for j in range(domain_levels.shape[1]):\n",
    "            cell_value = domain_levels.iloc[i, j]\n",
    "            if not pd.isna(cell_value):\n",
    "                cell_value_lower = cell_value.lower()\n",
    "                verb = lemmatizer.lemmatize(cell_value_lower, pos=\"v\")\n",
    "                if verb not in duplicate_checklist:\n",
    "                    domain_levels.iloc[i, j] = verb\n",
    "                    duplicate_checklist.append(verb)\n",
    "                else:\n",
    "                    domain_levels.iloc[i, j] = float('nan')\n",
    "\n",
    "    domain_levels = domain_levels.dropna(how='all')\n",
    "    return domain_levels\n",
    "\n",
    "\n",
    "# Paths\n",
    "solo_file_path = 'SOLO.xlsx'\n",
    "bloom_cognitive_file_path = 'Bloom_cognitive.xlsx'\n",
    "bloom_psychomotor_file_path = 'Bloom_psychomotor.xlsx'\n",
    "bloom_affective_file_path = 'Bloom_affective.xlsx'\n",
    "\n",
    "# Verbs\n",
    "mapped_verbs = {\n",
    "    \"Cognitive\": verb_classifier(bloom_cognitive_file_path),\n",
    "    \"Affective\": verb_classifier(bloom_affective_file_path),\n",
    "    \"Psychomotor\": verb_classifier(bloom_psychomotor_file_path),\n",
    "    \"SOLO\": verb_classifier(solo_file_path)\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No duplicates exist in Cognitive\n",
      "No duplicates exist in Affective\n",
      "No duplicates exist in Psychomotor\n",
      "No duplicates exist in SOLO\n"
     ]
    }
   ],
   "source": [
    "#### Check the dataframe for duplicates\n",
    "\n",
    "# verbs_df - The dataframe that consists of the classified verbs\n",
    "# domain_name - string name of the domain. (SOLO, Blooms Cognitive, Blooms Affective or Blooms Psychomotor)\n",
    "def check_duplicates(verbs_df, domain_name): \n",
    "\n",
    "    ## Checking duplicates for SOLO\n",
    "    list_of_lists = verbs_df.values.tolist()\n",
    "    merged_list = [item for sublist in list_of_lists for item in sublist]\n",
    "\n",
    "    # Remove nan\n",
    "    cleaned_list = list(filter(lambda x: not pd.isna(x), merged_list))\n",
    "\n",
    "    # Check for duplicate\n",
    "    if len(cleaned_list) != len(set(cleaned_list)):\n",
    "        print(\"Duplicates Found in \" + domain_name)\n",
    "        print(sorted(cleaned_list))\n",
    "    else:\n",
    "        print(\"No duplicates exist in \" + domain_name)\n",
    "    \n",
    "    list_of_lists.clear()\n",
    "    merged_list.clear()\n",
    "    cleaned_list.clear()\n",
    "\n",
    "\n",
    "# Check for duplicates\n",
    "for taxonomy_key, taxonomy_item in mapped_verbs.items():\n",
    "    check_duplicates(taxonomy_item, taxonomy_key)\n",
    "    taxonomy_item.to_csv(\"./outputs/mapped_\" + taxonomy_key + \".csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the English language model in spaCy\n",
    "# nlp = spacy.load('en_core_web_sm')\n",
    "nlp = spacy.load('en_core_web_trf', exclude=['ner'])\n",
    "\n",
    "## Function to identify verbs in a sentence\n",
    "def identify_verbs(sentence):\n",
    "    # Process the sentence using spaCy\n",
    "    doc = nlp(sentence)\n",
    "    \n",
    "    # Extract the verbs from the processed sentence\n",
    "    verbs = [token.lemma_ for token in doc if token.pos_ == 'VERB']\n",
    "    \n",
    "    return verbs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_columns(csv_file, columns):\n",
    "    extracted_data = {}\n",
    "    \n",
    "    with open(csv_file, 'r') as file:\n",
    "        reader = csv.reader(file)\n",
    "        headers = next(reader)  # Read the headers\n",
    "        \n",
    "        # Check if all specified columns exist in the CSV file\n",
    "        for column in columns:\n",
    "            if column not in headers:\n",
    "                raise ValueError(f\"Column '{column}' not found in the CSV file.\")\n",
    "        \n",
    "        # Initialize separate arrays for each column\n",
    "        for column in columns:\n",
    "            extracted_data[column] = []\n",
    "        \n",
    "        # Extract data from specified columns\n",
    "        for row in reader:\n",
    "            for column in columns:\n",
    "                column_index = headers.index(column)\n",
    "                extracted_data[column].append(row[column_index])\n",
    "    \n",
    "    return extracted_data\n",
    "\n",
    "# Example usage\n",
    "# csv_file = 'Learning outcomes manual mapping - Mappings.csv'\n",
    "csv_file = 'Learning outcomes manual mapping - Mappings.csv'\n",
    "# columns_to_extract = ['Learning outcomes', 'Final Bloom Level', 'Final SOLO Level']\n",
    "columns_to_extract = ['LO', 'Cognitive', 'Affective', 'Psychomotor', 'SOLO']\n",
    "extracted_data = extract_columns(csv_file, columns_to_extract)\n",
    "sentences = extracted_data['LO']\n",
    "final_levels = {\n",
    "    \"Cognitive\": extracted_data['Cognitive'],\n",
    "    \"Affective\": extracted_data['Affective'],\n",
    "    \"Psychomotor\": extracted_data['Psychomotor'],\n",
    "    \"SOLO\": extracted_data['SOLO']\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\musth\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\musth\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\musth\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "## Main piece of code that performs the mapping - Approach 2 - 12% accuracy\n",
    "\n",
    "# wiki word vectors no uppercase\n",
    "# TODO: Modify this section of the code to use the bloom level verbs from Arragon's spreadsheet, will also need to modify\n",
    "# Ideas for improving accuracy\n",
    "#### Reduce the number of verbs.\n",
    "#### Take more learning outcomes from the monash handbook website(need big dataset for this part) and identify verbs that are appearing multiple times \n",
    "#### The nummber of times that it appears could be set to a certain number ex: 5. \n",
    "#### If the verb doesnt appear atleast 5 times, we could remove the verb from our list of predefined verbs which will result in a shorter verb list\n",
    "\n",
    "output_bloom_levels = []\n",
    "\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"stopwords\")\n",
    "nltk.download(\"wordnet\")\n",
    "\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "# Array of all the PLOs and ULOs (We can couple them together as we're trying to identify Bloom/Solo level here)\n",
    "lo_sentence_array = []\n",
    "bloom_levels_str = [\n",
    "    \"Remembering\",\n",
    "    \"Understanding\",\n",
    "    \"Applying\",\n",
    "    \"Analysing\",\n",
    "    \"Evaluating\",\n",
    "    \"Creating\",\n",
    "]\n",
    "\n",
    "# TODO: train CLO classification with all data instead of just one course.\n",
    "for sentence in sentences:\n",
    "    sentence = re.sub(r'[^\\w\\s]', '', sentence)\n",
    "    # Process the sentence using spaCy\n",
    "    doc = nlp(sentence)\n",
    "    \n",
    "    # Extract the verbs from the processed sentence\n",
    "    cleaned_tokens = [token.lemma_ for token in doc if token.lower_ not in stop_words]\n",
    "    lo_sentence_array.append(cleaned_tokens)\n",
    "\n",
    "# build the vocabulary and train the model\n",
    "# IMPORTANT, N0TE THAT sg=1 flag specifies Word2Vec to use the Skip Gram Model as designated by the LSTM paper.\n",
    "model = Word2Vec(\n",
    "    sentences=lo_sentence_array, vector_size=100, window=5, min_count=1, workers=4, sg=1, epochs=30\n",
    ")\n",
    "\n",
    "word_vectors = model.wv\n",
    "# train the model with the course's ULOs and PLOs.\n",
    "# model.train([tokens], total_examples=len([tokens]), epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of  Cognitive  mappings passed:  31 %\n",
      "Percentage of  Affective  mappings passed:  40 %\n",
      "Percentage of  Psychomotor  mappings passed:  0 %\n",
      "Percentage of  SOLO  mappings passed:  65 %\n",
      "Total percentage of mappings passed:  45 %\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "def bloom_mapping(sentences, final_levels):\n",
    "    passed_mappings = {\n",
    "            \"Cognitive\": 0,\n",
    "            \"Affective\": 0,\n",
    "            \"Psychomotor\": 0,\n",
    "            \"SOLO\": 0\n",
    "        }\n",
    "    failed_mappings = {\n",
    "            \"Cognitive\": 0,\n",
    "            \"Affective\": 0,\n",
    "            \"Psychomotor\": 0,\n",
    "            \"SOLO\": 0\n",
    "        }\n",
    "    failed_cases = []\n",
    "    min_score = 1\n",
    "\n",
    "    for i in range(len(sentences)): # Iterates over the LOs\n",
    "        identified_verbs = identify_verbs(sentences[i])\n",
    "\n",
    "        score_list = {\n",
    "            \"Cognitive\": {\n",
    "                \"Remembering\": 0,\n",
    "                \"Understanding\": 0,\n",
    "                \"Applying\": 0,\n",
    "                \"Analysing\": 0,\n",
    "                \"Evaluating\": 0,\n",
    "                \"Creating\": 0\n",
    "            },\n",
    "            \"Affective\": {\n",
    "                \"Receiving\": 0,\n",
    "                \"Responding\": 0,\n",
    "                \"Valuing\": 0,\n",
    "                \"Organisation\": 0,\n",
    "                \"Characterisation\": 0\n",
    "            },\n",
    "            \"Psychomotor\": {\n",
    "                \"Perception\": 0,\n",
    "                \"Set\": 0,\n",
    "                \"Guided Response\": 0,\n",
    "                \"Mechanism\": 0,\n",
    "                \"Complex Overt Response\": 0,\n",
    "                \"Adaptation\": 0,\n",
    "                \"Origination\": 0    \n",
    "            },\n",
    "            \"SOLO\": {\n",
    "                \"Prestructural\": 0,\n",
    "                \"Unistructural\": 0,\n",
    "                \"Multistructural\": 0,\n",
    "                \"Relational\": 0,\n",
    "                \"Extended Abstract\": 0\n",
    "            }\n",
    "        }\n",
    "\n",
    "        for taxonomy_key, taxonomy_item in mapped_verbs.items():\n",
    "            # if final_levels[taxonomy_key][i] is None or not final_levels[taxonomy_key][i] or final_levels[taxonomy_key][i] == '-': continue\n",
    "            for j in range(taxonomy_item.shape[0]): # Level\n",
    "                for k in range(taxonomy_item.shape[1]): # Verb\n",
    "                    verb = taxonomy_item.iloc[j, k]\n",
    "                    \n",
    "                    if verb is None or not verb or pd.isna(verb): continue\n",
    "\n",
    "                    similarity_score = 0\n",
    "                    for l in range(len(identified_verbs)):\n",
    "                        try:    # Currently some of the 'verbs' identified are phrases rather than words and it was throwing errors so this is a temp solution \n",
    "                            sim_score = word_vectors.similarity(identified_verbs[l], verb)\n",
    "                            if sim_score > 0.9999: similarity_score += sim_score\n",
    "                        except:\n",
    "                            pass\n",
    "                    score_list[taxonomy_key][taxonomy_item.columns[k]] += similarity_score\n",
    "\n",
    "        # Identify level based on similarity\n",
    "        max_score = {\n",
    "            \"Cognitive\": { \"Level\": None, \"Score\": 0 },\n",
    "            \"Affective\": { \"Level\": None, \"Score\": 0 },\n",
    "            \"Psychomotor\": { \"Level\": None, \"Score\": 0 },\n",
    "            \"SOLO\": { \"Level\": None, \"Score\": 0 }\n",
    "        }\n",
    "        for t_key, t_item in score_list.items():\n",
    "            for l in t_item:\n",
    "                if max_score[t_key][\"Score\"] < score_list[t_key][l]:\n",
    "                    max_score[t_key] = { \"Level\": l, \"Score\": score_list[t_key][l] }\n",
    "\n",
    "        for t_key, t_item in final_levels.items():\n",
    "            if final_levels[t_key][i] is None or not final_levels[t_key][i] or final_levels[t_key][i] == '-': continue\n",
    "            \n",
    "            if max_score[t_key][\"Level\"] != None and final_levels[t_key][i].lower() == max_score[t_key][\"Level\"].lower():\n",
    "                passed_mappings[t_key] += 1\n",
    "            else:\n",
    "                failed_mappings[t_key] += 1\n",
    "                sentence_data = {\n",
    "                    \"Domain\": t_key,\n",
    "                    \"manually identified level\": final_levels[t_key][i],\n",
    "                    \"automatically identified level\": max_score[t_key][\"Level\"],\n",
    "                    \"verbs identified\": identified_verbs,\n",
    "                    \"sentence\": sentences[i],\n",
    "                }\n",
    "                failed_cases.append(sentence_data)\n",
    "\n",
    "    # for case in failed_cases:\n",
    "    #     print(case)\n",
    "    with open('./outputs/failed_lo_mappings.csv', 'w', newline='') as file: \n",
    "        writer = csv.DictWriter(file, fieldnames = sentence_data.keys())\n",
    "        writer.writeheader()\n",
    "        writer.writerows(failed_cases)\n",
    "\n",
    "\n",
    "\n",
    "    total_passed = 0\n",
    "    total_failed = 0\n",
    "    for taxonomy, passed in passed_mappings.items():\n",
    "        total_passed += passed\n",
    "        total_failed += failed_mappings[taxonomy]\n",
    "        tot = passed + failed_mappings[taxonomy] if passed + failed_mappings[taxonomy] > 0 else 1\n",
    "        mapping_percentage = math.ceil((passed/(tot))*100)\n",
    "        print(\"Percentage of \", taxonomy, \" mappings passed: \", mapping_percentage, \"%\")\n",
    "        \n",
    "    total_mapping_percentage = math.ceil((total_passed/(total_passed + total_failed))*100)\n",
    "    print(\"Total percentage of mappings passed: \", total_mapping_percentage, \"%\")\n",
    "    print(min_score)\n",
    "    pass\n",
    "\n",
    "\n",
    "### Todos\n",
    "# Find a way to use skipgrams\n",
    "# This method only works for blooms since this paper is only based on blooms mapping\n",
    "\n",
    "bloom_mapping(sentences, final_levels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_keys_by_value(dictionary, target_value):\n",
    "    key_value = \"\"\n",
    "    for key, value in dictionary.items():\n",
    "        if value == target_value:\n",
    "           key_value = key\n",
    "    \n",
    "    if key_value == \"\":\n",
    "        key_value = list(dictionary.keys())[-1]\n",
    "    return key_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "solo = mapped_verbs['SOLO']\n",
    "\n",
    "## Rankings are manually derived using the followinng sources\n",
    "## https://davenport.libguides.com/learningoutcomes/domains#:~:text=Bloom%20identified%20three%20domains%2C%20or,Psychomotor%20Skills%20or%20Physical%20Skills\n",
    "## https://www.vectorsolutions.com/resources/blogs/teaching-skills-the-psychomotor-domain-of-learning-and-learning-objectives/\n",
    "\n",
    "solo_levels_ranks = {\n",
    "    \"Prestructural\": 0,\n",
    "    \"Unistructural\": 1,\n",
    "    \"Multistructural\": 2,\n",
    "    \"Relational\": 3,\n",
    "    \"Extended Abstract\": 4\n",
    "}\n",
    "\n",
    "bloom_cognitive_levels_ranks = {\n",
    "    \"Remembering\": 0,\n",
    "    \"Understanding\": 0,\n",
    "    \"Applying\": 1,\n",
    "    \"Analysing\": 2,\n",
    "    \"Evaluating\": 3,\n",
    "    \"Creating\": 4\n",
    "}\n",
    "\n",
    "bloom_affective_levels_ranks = {\n",
    "    \"Receiving\": 0,\n",
    "    \"Responding\": 1,\n",
    "    \"Valuing\": 2,\n",
    "    \"Organisation\": 3,\n",
    "    \"Characterisation\": 4\n",
    "}\n",
    "\n",
    "bloom_psychomotor_levels_ranks = {\n",
    "    \"Perception\": 0,\n",
    "    \"Set\": 0,\n",
    "    \"Guided Response\": 1,\n",
    "    \"Mechanism\": 2,\n",
    "    \"Complex Overt Response\": 3,\n",
    "    \"Adaptation\": 3,\n",
    "    \"Origination\": 4\n",
    "}\n",
    "\n",
    "# bloom_cognitive_copy = bloom_cognitive.copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "report\n",
      "report\n",
      "debate\n",
      "transfer\n"
     ]
    }
   ],
   "source": [
    "def verb_ranking_identifier(current_taxonomy, comparison_taxonomy, verb, current_taxonomy_ranking_table, comparison_taxonomy_ranking_table):\n",
    "    if verb in comparison_taxonomy.values:\n",
    "        current_taxonomy_level = current_taxonomy[current_taxonomy == verb].stack().index[0][1]\n",
    "        comparison_taxonomy_level = comparison_taxonomy[comparison_taxonomy == verb].stack().index[0][1]\n",
    "\n",
    "        current_taxonomy_rank = current_taxonomy_ranking_table[current_taxonomy_level]\n",
    "        comparison_taxonomy_rank = comparison_taxonomy_ranking_table[comparison_taxonomy_level]\n",
    "        if verb=='relate':\n",
    "            print(\"fail\")\n",
    "\n",
    "        ranking_range = current_taxonomy_rank - comparison_taxonomy_rank\n",
    "\n",
    "\n",
    "        if abs(ranking_range) > 1:\n",
    "            # print(verb + '-' + current_taxonomy_level + '-' + str(current_taxonomy_rank) + '-' + str(comparison_taxonomy_rank))\n",
    "            new_current_taxonomy_rank = (current_taxonomy_rank + comparison_taxonomy_rank) // 2\n",
    "            new_current_taxonomy_level = get_keys_by_value(current_taxonomy_ranking_table, new_current_taxonomy_rank)\n",
    "            new_comparison_taxonomy_level = get_keys_by_value(comparison_taxonomy_ranking_table, new_current_taxonomy_rank)\n",
    "            current_taxonomy_dict = {\n",
    "                verb: new_current_taxonomy_level\n",
    "            }\n",
    "            comparison_taxonomy_dict = {\n",
    "                verb: new_comparison_taxonomy_level\n",
    "            }\n",
    "            current_taxonomy_modifications.append(current_taxonomy_dict)\n",
    "            comparison_taxonomy_modifications.append(comparison_taxonomy_dict)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def bloom_cog_remapping():\n",
    "    global current_taxonomy_modifications\n",
    "    current_taxonomy_modifications = []\n",
    "    global comparison_taxonomy_modifications\n",
    "    comparison_taxonomy_modifications = []\n",
    "\n",
    "    global solo_copy \n",
    "    solo_copy = solo.copy(deep=True)\n",
    "    solo_copy.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    global bloom_cognitive_copy\n",
    "    bloom_cognitive_copy = mapped_verbs['Cognitive']\n",
    "    bloom_cognitive_copy.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    global bloom_affective_copy\n",
    "    bloom_affective = mapped_verbs['Affective']\n",
    "    bloom_affective_copy = bloom_affective.copy(deep=True)\n",
    "    bloom_affective_copy.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    global bloom_psychomotor_copy\n",
    "    bloom_psychomotor = mapped_verbs['Psychomotor']\n",
    "    bloom_psychomotor_copy = bloom_psychomotor.copy(deep=True)\n",
    "    bloom_psychomotor_copy.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    # Check for solo verbs against other taxonomies. Identify new levels to put verbs into\n",
    "    for i in range(bloom_cognitive_copy.shape[0]): # Level\n",
    "        for j in range(bloom_cognitive_copy.shape[1]): # Verb\n",
    "            verb = bloom_cognitive_copy.iloc[i, j]\n",
    "            \n",
    "            if verb is None or not verb or pd.isna(verb): continue\n",
    "\n",
    "            verb_ranking_identifier(bloom_cognitive_copy, solo_copy, verb, bloom_cognitive_levels_ranks, solo_levels_ranks)\n",
    "            verb_ranking_identifier(bloom_cognitive_copy, bloom_affective_copy, verb, bloom_cognitive_levels_ranks, bloom_affective_levels_ranks)\n",
    "            verb_ranking_identifier(bloom_cognitive_copy, bloom_psychomotor_copy, verb, bloom_cognitive_levels_ranks, bloom_psychomotor_levels_ranks)\n",
    "\n",
    "\n",
    "    for i in range(len(current_taxonomy_modifications)):\n",
    "        current_item = current_taxonomy_modifications[i]\n",
    "        verb = list(current_item.keys())[0]\n",
    "        level = list(current_item.values())[0]\n",
    "        row, col = bloom_cognitive_copy[bloom_cognitive_copy == verb].stack().index[0]\n",
    "        bloom_cognitive_copy.at[row, col] = None\n",
    "        temp_df = pd.DataFrame({\n",
    "            level: verb\n",
    "        }, index=[0])\n",
    "        bloom_cognitive_copy = pd.concat([bloom_cognitive_copy, temp_df], ignore_index=True)\n",
    "\n",
    "\n",
    "    for i in range(len(comparison_taxonomy_modifications)):\n",
    "        current_item = comparison_taxonomy_modifications[i]\n",
    "        verb = list(current_item.keys())[0]\n",
    "        level = list(current_item.values())[0]\n",
    "        temp_df = pd.DataFrame({\n",
    "            level: verb\n",
    "        }, index=[0])\n",
    "        if level in solo_copy.columns:\n",
    "            row, col = solo_copy[solo_copy == verb].stack().index[0]\n",
    "            solo_copy.at[row, col] = None\n",
    "            solo_copy = pd.concat([solo_copy, temp_df], ignore_index=True)\n",
    "        elif level in bloom_affective_copy.columns:\n",
    "            row, col = bloom_affective_copy[bloom_affective_copy == verb].stack().index[0]\n",
    "            bloom_affective_copy.at[row, col] = None\n",
    "            bloom_affective_copy = pd.concat([bloom_affective_copy, temp_df], ignore_index=True)\n",
    "        elif level in bloom_psychomotor_copy.columns:\n",
    "            row, col = bloom_psychomotor_copy[bloom_psychomotor_copy == verb].stack().index[0]\n",
    "            bloom_psychomotor_copy.at[row, col] = None\n",
    "            bloom_psychomotor_copy = pd.concat([bloom_psychomotor_copy, temp_df], ignore_index=True)\n",
    "        else:\n",
    "            print(level)\n",
    "            print('level doesnt exist')\n",
    "\n",
    "bloom_cog_remapping()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def check_verb_rankings(current_taxonomy, comparison_taxonomy, verb, current_taxonomy_ranking_table, comparison_taxonomy_ranking_table):\n",
    "    if verb in comparison_taxonomy.values:\n",
    "        current_taxonomy_level = current_taxonomy[current_taxonomy == verb].stack().index[0][1]\n",
    "        comparison_taxonomy_level = comparison_taxonomy[comparison_taxonomy == verb].stack().index[0][1]\n",
    "\n",
    "        current_taxonomy_rank = current_taxonomy_ranking_table[current_taxonomy_level]\n",
    "        comparison_taxonomy_rank = comparison_taxonomy_ranking_table[comparison_taxonomy_level]\n",
    "\n",
    "        ranking_range = current_taxonomy_rank - comparison_taxonomy_rank\n",
    "\n",
    "\n",
    "        if abs(ranking_range) > 1:\n",
    "            print(verb + '-' + current_taxonomy_level + '-' + str(current_taxonomy_rank) + '-' + str(comparison_taxonomy_rank) + '-' + comparison_taxonomy_level)\n",
    "\n",
    "\n",
    "        \n",
    "for i in range(bloom_cognitive_copy.shape[0]): # Level\n",
    "    for j in range(bloom_cognitive_copy.shape[1]): # Verb\n",
    "        verb = bloom_cognitive_copy.iloc[i, j]\n",
    "        \n",
    "        if verb is None or not verb or pd.isna(verb): continue\n",
    "        \n",
    "        check_verb_rankings(bloom_cognitive_copy, solo_copy, verb, bloom_cognitive_levels_ranks, solo_levels_ranks)\n",
    "        check_verb_rankings(bloom_cognitive_copy, bloom_affective_copy, verb, bloom_cognitive_levels_ranks, bloom_affective_levels_ranks)\n",
    "        check_verb_rankings(bloom_cognitive_copy, bloom_psychomotor_copy, verb, bloom_cognitive_levels_ranks, bloom_psychomotor_levels_ranks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Prestructural</th>\n",
       "      <th>Unistructural</th>\n",
       "      <th>Multistructural</th>\n",
       "      <th>Relational</th>\n",
       "      <th>Extended Abstract</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>comply</td>\n",
       "      <td>None</td>\n",
       "      <td>address</td>\n",
       "      <td>appraise</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>identify</td>\n",
       "      <td>None</td>\n",
       "      <td>analyse</td>\n",
       "      <td>assess</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>locate</td>\n",
       "      <td>determine</td>\n",
       "      <td>apply</td>\n",
       "      <td>deliberate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>discern</td>\n",
       "      <td>appreciate</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>issue</td>\n",
       "      <td>calculate</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>paraphrase</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>summarise</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>translate</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>combine</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>NaN</td>\n",
       "      <td>relate</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>90 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Prestructural Unistructural Multistructural  Relational Extended Abstract\n",
       "0            NaN        comply            None     address          appraise\n",
       "1            NaN      identify            None     analyse            assess\n",
       "2            NaN        locate       determine       apply        deliberate\n",
       "3            NaN           NaN         discern  appreciate              None\n",
       "4            NaN           NaN           issue   calculate              None\n",
       "..           ...           ...             ...         ...               ...\n",
       "85           NaN           NaN      paraphrase         NaN               NaN\n",
       "86           NaN           NaN       summarise         NaN               NaN\n",
       "87           NaN           NaN       translate         NaN               NaN\n",
       "88           NaN           NaN             NaN         NaN           combine\n",
       "89           NaN        relate             NaN         NaN               NaN\n",
       "\n",
       "[90 rows x 5 columns]"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "solo_copy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "If using all scalar values, you must pass an index",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[87], line 16\u001b[0m\n\u001b[0;32m     12\u001b[0m     new_df \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mDataFrame(\u001b[39mdict\u001b[39m)\n\u001b[0;32m     13\u001b[0m     \u001b[39mreturn\u001b[39;00m new_df\n\u001b[1;32m---> 16\u001b[0m df \u001b[39m=\u001b[39m remove_nan_values(solo_copy)\n\u001b[0;32m     17\u001b[0m df\n",
      "Cell \u001b[1;32mIn[87], line 12\u001b[0m, in \u001b[0;36mremove_nan_values\u001b[1;34m(taxonomy_verb_df)\u001b[0m\n\u001b[0;32m      9\u001b[0m         verb_level \u001b[39m=\u001b[39m taxonomy_verb_df[taxonomy_verb_df \u001b[39m==\u001b[39m verb]\u001b[39m.\u001b[39mstack()\u001b[39m.\u001b[39mindex[\u001b[39m0\u001b[39m][\u001b[39m1\u001b[39m]\n\u001b[0;32m     10\u001b[0m         \u001b[39mdict\u001b[39m[verb_level] \u001b[39m=\u001b[39m verb\n\u001b[1;32m---> 12\u001b[0m new_df \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39;49mDataFrame(\u001b[39mdict\u001b[39;49m)\n\u001b[0;32m     13\u001b[0m \u001b[39mreturn\u001b[39;00m new_df\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\pandas\\core\\frame.py:663\u001b[0m, in \u001b[0;36mDataFrame.__init__\u001b[1;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[0;32m    657\u001b[0m     mgr \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_init_mgr(\n\u001b[0;32m    658\u001b[0m         data, axes\u001b[39m=\u001b[39m{\u001b[39m\"\u001b[39m\u001b[39mindex\u001b[39m\u001b[39m\"\u001b[39m: index, \u001b[39m\"\u001b[39m\u001b[39mcolumns\u001b[39m\u001b[39m\"\u001b[39m: columns}, dtype\u001b[39m=\u001b[39mdtype, copy\u001b[39m=\u001b[39mcopy\n\u001b[0;32m    659\u001b[0m     )\n\u001b[0;32m    661\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(data, \u001b[39mdict\u001b[39m):\n\u001b[0;32m    662\u001b[0m     \u001b[39m# GH#38939 de facto copy defaults to False only in non-dict cases\u001b[39;00m\n\u001b[1;32m--> 663\u001b[0m     mgr \u001b[39m=\u001b[39m dict_to_mgr(data, index, columns, dtype\u001b[39m=\u001b[39;49mdtype, copy\u001b[39m=\u001b[39;49mcopy, typ\u001b[39m=\u001b[39;49mmanager)\n\u001b[0;32m    664\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(data, ma\u001b[39m.\u001b[39mMaskedArray):\n\u001b[0;32m    665\u001b[0m     \u001b[39mimport\u001b[39;00m \u001b[39mnumpy\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mma\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmrecords\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mmrecords\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\pandas\\core\\internals\\construction.py:494\u001b[0m, in \u001b[0;36mdict_to_mgr\u001b[1;34m(data, index, columns, dtype, typ, copy)\u001b[0m\n\u001b[0;32m    490\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    491\u001b[0m         \u001b[39m# dtype check to exclude e.g. range objects, scalars\u001b[39;00m\n\u001b[0;32m    492\u001b[0m         arrays \u001b[39m=\u001b[39m [x\u001b[39m.\u001b[39mcopy() \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(x, \u001b[39m\"\u001b[39m\u001b[39mdtype\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39melse\u001b[39;00m x \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m arrays]\n\u001b[1;32m--> 494\u001b[0m \u001b[39mreturn\u001b[39;00m arrays_to_mgr(arrays, columns, index, dtype\u001b[39m=\u001b[39;49mdtype, typ\u001b[39m=\u001b[39;49mtyp, consolidate\u001b[39m=\u001b[39;49mcopy)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\pandas\\core\\internals\\construction.py:119\u001b[0m, in \u001b[0;36marrays_to_mgr\u001b[1;34m(arrays, columns, index, dtype, verify_integrity, typ, consolidate)\u001b[0m\n\u001b[0;32m    116\u001b[0m \u001b[39mif\u001b[39;00m verify_integrity:\n\u001b[0;32m    117\u001b[0m     \u001b[39m# figure out the index, if necessary\u001b[39;00m\n\u001b[0;32m    118\u001b[0m     \u001b[39mif\u001b[39;00m index \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 119\u001b[0m         index \u001b[39m=\u001b[39m _extract_index(arrays)\n\u001b[0;32m    120\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    121\u001b[0m         index \u001b[39m=\u001b[39m ensure_index(index)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\pandas\\core\\internals\\construction.py:657\u001b[0m, in \u001b[0;36m_extract_index\u001b[1;34m(data)\u001b[0m\n\u001b[0;32m    654\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mPer-column arrays must each be 1-dimensional\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    656\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m indexes \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m raw_lengths:\n\u001b[1;32m--> 657\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mIf using all scalar values, you must pass an index\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    659\u001b[0m \u001b[39melif\u001b[39;00m have_series:\n\u001b[0;32m    660\u001b[0m     index \u001b[39m=\u001b[39m union_indexes(indexes)\n",
      "\u001b[1;31mValueError\u001b[0m: If using all scalar values, you must pass an index"
     ]
    }
   ],
   "source": [
    "def remove_nan_values(taxonomy_verb_df):\n",
    "    dict = {}\n",
    "    for i in range(taxonomy_verb_df.shape[0]): # Level\n",
    "        for j in range(taxonomy_verb_df.shape[1]): # Verb\n",
    "            verb = taxonomy_verb_df.iloc[i, j]\n",
    "\n",
    "            if verb is None or not verb or pd.isna(verb): continue\n",
    "\n",
    "            verb_level = taxonomy_verb_df[taxonomy_verb_df == verb].stack().index[0][1]\n",
    "            dict[verb_level] = verb\n",
    "\n",
    "    \n",
    "    new_df = pd.DataFrame(dict)\n",
    "    return new_df\n",
    "\n",
    "\n",
    "df = remove_nan_values(solo_copy)\n",
    "df\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
