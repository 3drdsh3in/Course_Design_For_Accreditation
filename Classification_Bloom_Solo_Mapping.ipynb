{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: docx2txt in /home/apro0009/.local/lib/python3.10/site-packages (0.8)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: gensim in /home/apro0009/.local/lib/python3.10/site-packages (4.3.1)\n",
      "Requirement already satisfied: scipy>=1.7.0 in /home/apro0009/.local/lib/python3.10/site-packages (from gensim) (1.10.1)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in /home/apro0009/.local/lib/python3.10/site-packages (from gensim) (6.3.0)\n",
      "Requirement already satisfied: numpy>=1.18.5 in /home/apro0009/.local/lib/python3.10/site-packages (from gensim) (1.23.5)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: keras in /home/apro0009/.local/lib/python3.10/site-packages (2.12.0)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: scikit-learn in /home/apro0009/.local/lib/python3.10/site-packages (1.2.2)\n",
      "Requirement already satisfied: scipy>=1.3.2 in /home/apro0009/.local/lib/python3.10/site-packages (from scikit-learn) (1.10.1)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /home/apro0009/.local/lib/python3.10/site-packages (from scikit-learn) (1.2.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/apro0009/.local/lib/python3.10/site-packages (from scikit-learn) (3.1.0)\n",
      "Requirement already satisfied: numpy>=1.17.3 in /home/apro0009/.local/lib/python3.10/site-packages (from scikit-learn) (1.23.5)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: python-docx in /home/apro0009/.local/lib/python3.10/site-packages (0.8.11)\n",
      "Requirement already satisfied: lxml>=2.3.2 in /home/apro0009/.local/lib/python3.10/site-packages (from python-docx) (4.9.2)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: tensorflow in /home/apro0009/.local/lib/python3.10/site-packages (2.12.0)\n",
      "Requirement already satisfied: h5py>=2.9.0 in /home/apro0009/.local/lib/python3.10/site-packages (from tensorflow) (3.8.0)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /home/apro0009/.local/lib/python3.10/site-packages (from tensorflow) (3.3.0)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /home/apro0009/.local/lib/python3.10/site-packages (from tensorflow) (0.32.0)\n",
      "Requirement already satisfied: six>=1.12.0 in /home/apro0009/.local/lib/python3.10/site-packages (from tensorflow) (1.16.0)\n",
      "Requirement already satisfied: jax>=0.3.15 in /home/apro0009/.local/lib/python3.10/site-packages (from tensorflow) (0.4.10)\n",
      "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /home/apro0009/.local/lib/python3.10/site-packages (from tensorflow) (0.4.0)\n",
      "Requirement already satisfied: packaging in /home/apro0009/.local/lib/python3.10/site-packages (from tensorflow) (23.1)\n",
      "Requirement already satisfied: setuptools in /usr/lib/python3/dist-packages (from tensorflow) (59.6.0)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /home/apro0009/.local/lib/python3.10/site-packages (from tensorflow) (4.23.0)\n",
      "Requirement already satisfied: tensorboard<2.13,>=2.12 in /home/apro0009/.local/lib/python3.10/site-packages (from tensorflow) (2.12.3)\n",
      "Requirement already satisfied: tensorflow-estimator<2.13,>=2.12.0 in /home/apro0009/.local/lib/python3.10/site-packages (from tensorflow) (2.12.0)\n",
      "Requirement already satisfied: keras<2.13,>=2.12.0 in /home/apro0009/.local/lib/python3.10/site-packages (from tensorflow) (2.12.0)\n",
      "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /home/apro0009/.local/lib/python3.10/site-packages (from tensorflow) (1.14.1)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /home/apro0009/.local/lib/python3.10/site-packages (from tensorflow) (4.5.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /home/apro0009/.local/lib/python3.10/site-packages (from tensorflow) (1.4.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /home/apro0009/.local/lib/python3.10/site-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /home/apro0009/.local/lib/python3.10/site-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: numpy<1.24,>=1.22 in /home/apro0009/.local/lib/python3.10/site-packages (from tensorflow) (1.23.5)\n",
      "Requirement already satisfied: flatbuffers>=2.0 in /home/apro0009/.local/lib/python3.10/site-packages (from tensorflow) (23.5.9)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /home/apro0009/.local/lib/python3.10/site-packages (from tensorflow) (1.54.2)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /home/apro0009/.local/lib/python3.10/site-packages (from tensorflow) (2.3.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /home/apro0009/.local/lib/python3.10/site-packages (from tensorflow) (16.0.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/lib/python3/dist-packages (from astunparse>=1.6.0->tensorflow) (0.37.1)\n",
      "Requirement already satisfied: ml-dtypes>=0.1.0 in /home/apro0009/.local/lib/python3.10/site-packages (from jax>=0.3.15->tensorflow) (0.1.0)\n",
      "Requirement already satisfied: scipy>=1.7 in /home/apro0009/.local/lib/python3.10/site-packages (from jax>=0.3.15->tensorflow) (1.10.1)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /home/apro0009/.local/lib/python3.10/site-packages (from tensorboard<2.13,>=2.12->tensorflow) (0.7.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /home/apro0009/.local/lib/python3.10/site-packages (from tensorboard<2.13,>=2.12->tensorflow) (3.4.3)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /home/apro0009/.local/lib/python3.10/site-packages (from tensorboard<2.13,>=2.12->tensorflow) (2.3.4)\n",
      "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /home/apro0009/.local/lib/python3.10/site-packages (from tensorboard<2.13,>=2.12->tensorflow) (1.0.0)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /home/apro0009/.local/lib/python3.10/site-packages (from tensorboard<2.13,>=2.12->tensorflow) (2.30.0)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /home/apro0009/.local/lib/python3.10/site-packages (from tensorboard<2.13,>=2.12->tensorflow) (2.18.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /home/apro0009/.local/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow) (4.9)\n",
      "Requirement already satisfied: urllib3<2.0 in /home/apro0009/.local/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow) (1.26.15)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /home/apro0009/.local/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow) (0.3.0)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /home/apro0009/.local/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow) (5.3.0)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /home/apro0009/.local/lib/python3.10/site-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow) (1.3.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/apro0009/.local/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow) (2023.5.7)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/apro0009/.local/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow) (3.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/apro0009/.local/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow) (3.1.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /home/apro0009/.local/lib/python3.10/site-packages (from werkzeug>=1.0.1->tensorboard<2.13,>=2.12->tensorflow) (2.1.2)\n",
      "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /home/apro0009/.local/lib/python3.10/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow) (0.5.0)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /usr/lib/python3/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow) (3.2.0)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: nltk in /home/apro0009/.local/lib/python3.10/site-packages (3.8.1)\n",
      "Requirement already satisfied: joblib in /home/apro0009/.local/lib/python3.10/site-packages (from nltk) (1.2.0)\n",
      "Requirement already satisfied: click in /home/apro0009/.local/lib/python3.10/site-packages (from nltk) (8.1.3)\n",
      "Requirement already satisfied: tqdm in /home/apro0009/.local/lib/python3.10/site-packages (from nltk) (4.65.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /home/apro0009/.local/lib/python3.10/site-packages (from nltk) (2023.5.5)\n",
      "docx2csv exists.\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "# Dependencies\n",
    "pip install docx2txt\n",
    "pip install gensim\n",
    "pip install keras\n",
    "pip install nltk\n",
    "pip install -U scikit-learn\n",
    "pip install python-docx\n",
    "pip install tensorflow\n",
    "pip install nltk\n",
    "\n",
    "if ls docx2csv >/dev/null 2>&1; then\n",
    "    echo \"docx2csv exists.\"\n",
    "else\n",
    "    echo \"Folder does not exist. Cloning docx2csv.\"\n",
    "    git clone https://github.com/ivbeg/docx2csv.git\n",
    "fi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running install\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/setuptools/command/install.py:34: SetuptoolsDeprecationWarning: setup.py install is deprecated. Use build and pip and other standards-based tools.\n",
      "  warnings.warn(\n",
      "/usr/lib/python3/dist-packages/setuptools/command/easy_install.py:158: EasyInstallDeprecationWarning: easy_install command is deprecated. Use build and pip and other standards-based tools.\n",
      "  warnings.warn(\n",
      "/usr/lib/python3/dist-packages/pkg_resources/__init__.py:116: PkgResourcesDeprecationWarning: 1.1build1 is an invalid version and will not be supported in a future release\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running bdist_egg\n",
      "running egg_info\n",
      "writing docx2csv.egg-info/PKG-INFO\n",
      "writing dependency_links to docx2csv.egg-info/dependency_links.txt\n",
      "writing entry points to docx2csv.egg-info/entry_points.txt\n",
      "writing requirements to docx2csv.egg-info/requires.txt\n",
      "writing top-level names to docx2csv.egg-info/top_level.txt\n",
      "reading manifest file 'docx2csv.egg-info/SOURCES.txt'\n",
      "adding license file 'LICENSE'\n",
      "adding license file 'AUTHORS.rst'\n",
      "writing manifest file 'docx2csv.egg-info/SOURCES.txt'\n",
      "installing library code to build/bdist.linux-x86_64/egg\n",
      "running install_lib\n",
      "running build_py\n",
      "creating build/bdist.linux-x86_64/egg\n",
      "creating build/bdist.linux-x86_64/egg/docx2csv\n",
      "copying build/lib/docx2csv/converter.py -> build/bdist.linux-x86_64/egg/docx2csv\n",
      "copying build/lib/docx2csv/core.py -> build/bdist.linux-x86_64/egg/docx2csv\n",
      "copying build/lib/docx2csv/__init__.py -> build/bdist.linux-x86_64/egg/docx2csv\n",
      "copying build/lib/docx2csv/__main__.py -> build/bdist.linux-x86_64/egg/docx2csv\n",
      "byte-compiling build/bdist.linux-x86_64/egg/docx2csv/converter.py to converter.cpython-310.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/docx2csv/core.py to core.cpython-310.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/docx2csv/__init__.py to __init__.cpython-310.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/docx2csv/__main__.py to __main__.cpython-310.pyc\n",
      "creating build/bdist.linux-x86_64/egg/EGG-INFO\n",
      "copying docx2csv.egg-info/PKG-INFO -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
      "copying docx2csv.egg-info/SOURCES.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
      "copying docx2csv.egg-info/dependency_links.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
      "copying docx2csv.egg-info/entry_points.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
      "copying docx2csv.egg-info/not-zip-safe -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
      "copying docx2csv.egg-info/requires.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
      "copying docx2csv.egg-info/top_level.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
      "creating 'dist/docx2csv-0.1.2-py3.10.egg' and adding 'build/bdist.linux-x86_64/egg' to it\n",
      "removing 'build/bdist.linux-x86_64/egg' (and everything under it)\n",
      "Processing docx2csv-0.1.2-py3.10.egg\n",
      "removing '/usr/local/lib/python3.10/dist-packages/docx2csv-0.1.2-py3.10.egg' (and everything under it)\n",
      "creating /usr/local/lib/python3.10/dist-packages/docx2csv-0.1.2-py3.10.egg\n",
      "Extracting docx2csv-0.1.2-py3.10.egg to /usr/local/lib/python3.10/dist-packages\n",
      "docx2csv 0.1.2 is already the active version in easy-install.pth\n",
      "Installing docx2csv script to /usr/local/bin\n",
      "\n",
      "Installed /usr/local/lib/python3.10/dist-packages/docx2csv-0.1.2-py3.10.egg\n",
      "Processing dependencies for docx2csv==0.1.2\n",
      "Searching for xlwt==1.3.0\n",
      "Best match: xlwt 1.3.0\n",
      "Processing xlwt-1.3.0-py3.10.egg\n",
      "xlwt 1.3.0 is already the active version in easy-install.pth\n",
      "\n",
      "Using /usr/local/lib/python3.10/dist-packages/xlwt-1.3.0-py3.10.egg\n",
      "Searching for openpyxl==3.2.0b1\n",
      "Best match: openpyxl 3.2.0b1\n",
      "Processing openpyxl-3.2.0b1-py3.10.egg\n",
      "openpyxl 3.2.0b1 is already the active version in easy-install.pth\n",
      "\n",
      "Using /usr/local/lib/python3.10/dist-packages/openpyxl-3.2.0b1-py3.10.egg\n",
      "Searching for docx==0.2.4\n",
      "Best match: docx 0.2.4\n",
      "Processing docx-0.2.4-py3.10.egg\n",
      "docx 0.2.4 is already the active version in easy-install.pth\n",
      "\n",
      "Using /usr/local/lib/python3.10/dist-packages/docx-0.2.4-py3.10.egg\n",
      "Searching for click==8.1.3\n",
      "Best match: click 8.1.3\n",
      "Processing click-8.1.3-py3.10.egg\n",
      "click 8.1.3 is already the active version in easy-install.pth\n",
      "\n",
      "Using /usr/local/lib/python3.10/dist-packages/click-8.1.3-py3.10.egg\n",
      "Searching for et-xmlfile==1.1.0\n",
      "Best match: et-xmlfile 1.1.0\n",
      "Processing et_xmlfile-1.1.0-py3.10.egg\n",
      "et-xmlfile 1.1.0 is already the active version in easy-install.pth\n",
      "\n",
      "Using /usr/local/lib/python3.10/dist-packages/et_xmlfile-1.1.0-py3.10.egg\n",
      "Searching for lxml==4.9.2\n",
      "Best match: lxml 4.9.2\n",
      "Processing lxml-4.9.2-py3.10-linux-x86_64.egg\n",
      "lxml 4.9.2 is already the active version in easy-install.pth\n",
      "\n",
      "Using /usr/local/lib/python3.10/dist-packages/lxml-4.9.2-py3.10-linux-x86_64.egg\n",
      "Searching for Pillow==9.5.0\n",
      "Best match: Pillow 9.5.0\n",
      "Processing Pillow-9.5.0-py3.10-linux-x86_64.egg\n",
      "Pillow 9.5.0 is already the active version in easy-install.pth\n",
      "\n",
      "Using /usr/local/lib/python3.10/dist-packages/Pillow-9.5.0-py3.10-linux-x86_64.egg\n",
      "Finished processing dependencies for docx2csv==0.1.2\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "source .env\n",
    "cd docx2csv && echo \"$PASSWORD\" | sudo -S python3 setup.py install"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----- TEST DATA INPUT -----\n",
    "\n",
    "# Computer Science Test Data.\n",
    "# CURRENT_MAPPING=\"Lists_ComputerScience.docx\"\n",
    "# ORIGINAL_MAPPING=\"Original-Mapping-ComputerScience.csv\"\n",
    "\n",
    "# InformationSecurity Test Data.\n",
    "CURRENT_MAPPING=\"Lists_InformationSecurity.docx\"\n",
    "ORIGINAL_MAPPING=\"Original-Mapping-InfoSecurity.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/apro0009/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/apro0009/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/apro0009/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "from gensim.test.utils import common_texts\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract tables from word document\n",
    "from docx2csv import extract_tables, extract\n",
    "tables = extract_tables(CURRENT_MAPPING)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from docx import Document\n",
    "document = Document(CURRENT_MAPPING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Analyze',\n",
       "  'complex',\n",
       "  'computing',\n",
       "  'problem',\n",
       "  'apply',\n",
       "  'principle',\n",
       "  'computing',\n",
       "  'relevant',\n",
       "  'discipline',\n",
       "  'identify',\n",
       "  'solution',\n",
       "  '.'],\n",
       " ['Design',\n",
       "  ',',\n",
       "  'implement',\n",
       "  ',',\n",
       "  'evaluate',\n",
       "  'computing-based',\n",
       "  'solution',\n",
       "  'meet',\n",
       "  'given',\n",
       "  'set',\n",
       "  'computing',\n",
       "  'requirement',\n",
       "  'context',\n",
       "  'program',\n",
       "  '’',\n",
       "  'discipline',\n",
       "  '.'],\n",
       " ['Communicate', 'effectively', 'variety', 'professional', 'context', '.'],\n",
       " ['Recognize',\n",
       "  'professional',\n",
       "  'responsibility',\n",
       "  'make',\n",
       "  'informed',\n",
       "  'judgment',\n",
       "  'computing',\n",
       "  'practice',\n",
       "  'based',\n",
       "  'legal',\n",
       "  'ethical',\n",
       "  'principle',\n",
       "  '.'],\n",
       " ['Function',\n",
       "  'effectively',\n",
       "  'member',\n",
       "  'leader',\n",
       "  'team',\n",
       "  'engaged',\n",
       "  'activity',\n",
       "  'appropriate',\n",
       "  'program',\n",
       "  '’',\n",
       "  'discipline',\n",
       "  '.'],\n",
       " ['Apply',\n",
       "  'security',\n",
       "  'principle',\n",
       "  'practice',\n",
       "  'maintain',\n",
       "  'operation',\n",
       "  'presence',\n",
       "  'risk',\n",
       "  'threat',\n",
       "  '.'],\n",
       " ['Demonstrate',\n",
       "  'computational',\n",
       "  'thinking',\n",
       "  'skill',\n",
       "  'solve',\n",
       "  'computing',\n",
       "  'problem'],\n",
       " ['Describe',\n",
       "  'system',\n",
       "  'component',\n",
       "  'involved',\n",
       "  'building',\n",
       "  'usable',\n",
       "  'computing',\n",
       "  'platform'],\n",
       " ['Identify', 'different', 'tool', 'technique', 'used', 'system'],\n",
       " ['Discuss', 'related', 'ethical', 'issue'],\n",
       " ['Translate',\n",
       "  'problem',\n",
       "  'expressed',\n",
       "  'English',\n",
       "  ',',\n",
       "  'mathematics',\n",
       "  'diagram',\n",
       "  'computer',\n",
       "  'program'],\n",
       " ['Implement',\n",
       "  'algorithm',\n",
       "  'using',\n",
       "  'programming',\n",
       "  'construct',\n",
       "  '(',\n",
       "  'variable',\n",
       "  ',',\n",
       "  'control',\n",
       "  'structure',\n",
       "  ',',\n",
       "  'method',\n",
       "  ')'],\n",
       " ['Solve', 'problem', 'using', 'suitable', 'data', 'structure'],\n",
       " ['Implement', 'searching', ',', 'summing', 'selecting', 'algorithm'],\n",
       " ['Design', 'implement', 'algorithm', 'solve', 'simple', 'problem'],\n",
       " ['Choose', 'suitable', 'data', 'type', 'represent', 'information'],\n",
       " ['Apply',\n",
       "  'sequence',\n",
       "  ',',\n",
       "  'selection',\n",
       "  'repetition',\n",
       "  'structure',\n",
       "  'solve',\n",
       "  'problem'],\n",
       " ['Design', 'implement', 'program', 'containing', 'many', 'method'],\n",
       " ['Manipulate', 'One-Dimension', 'Two-Dimension', 'array'],\n",
       " ['Use',\n",
       "  'formal',\n",
       "  'method',\n",
       "  'symbolic',\n",
       "  'proposition',\n",
       "  'evaluate',\n",
       "  'elementary',\n",
       "  'mathematical',\n",
       "  'argument',\n",
       "  'identify',\n",
       "  'logical',\n",
       "  'reasoning'],\n",
       " ['Prove',\n",
       "  'assertion',\n",
       "  'using',\n",
       "  'basic',\n",
       "  'proof',\n",
       "  'method',\n",
       "  ',',\n",
       "  '(',\n",
       "  'eg',\n",
       "  'induction',\n",
       "  ',',\n",
       "  'contrapositive',\n",
       "  ',',\n",
       "  'contradiction',\n",
       "  ',',\n",
       "  '…etc',\n",
       "  ')'],\n",
       " ['Derive', 'closed-forms', 'summation', 'recursive', 'structure'],\n",
       " ['Apply',\n",
       "  'graph',\n",
       "  'theory',\n",
       "  'tree',\n",
       "  'model',\n",
       "  'solve',\n",
       "  'problem',\n",
       "  'connectivity'],\n",
       " ['Use',\n",
       "  'logical',\n",
       "  'notation',\n",
       "  'define',\n",
       "  'reason',\n",
       "  'fundamental',\n",
       "  'mathematical',\n",
       "  'concept',\n",
       "  'set',\n",
       "  ',',\n",
       "  'relation',\n",
       "  ',',\n",
       "  'function',\n",
       "  ',',\n",
       "  'matrix',\n",
       "  'integer'],\n",
       " ['Convert',\n",
       "  'different',\n",
       "  'number',\n",
       "  'system',\n",
       "  'represent',\n",
       "  'signed',\n",
       "  'number',\n",
       "  '1',\n",
       "  \"'s\",\n",
       "  '2',\n",
       "  \"'s\",\n",
       "  'complement',\n",
       "  'representation'],\n",
       " ['Analyze', 'combinational', 'sequential', 'circuit'],\n",
       " ['Design', 'implement', 'combinational', 'sequential', 'circuit'],\n",
       " ['Define',\n",
       "  'modern',\n",
       "  'computer',\n",
       "  'system',\n",
       "  \"'s\",\n",
       "  'major',\n",
       "  'component',\n",
       "  ',',\n",
       "  'function',\n",
       "  'inter-relationships'],\n",
       " ['Explain',\n",
       "  'memory',\n",
       "  'hierarchy',\n",
       "  'structure',\n",
       "  'importance',\n",
       "  'characteristic',\n",
       "  'level'],\n",
       " ['Describe',\n",
       "  'component',\n",
       "  'instruction',\n",
       "  'set',\n",
       "  'different',\n",
       "  'type',\n",
       "  'instruction',\n",
       "  'addressing',\n",
       "  'mode'],\n",
       " ['Explain', 'different', 'concept', 'function', 'physical', 'layer'],\n",
       " ['Apply',\n",
       "  'different',\n",
       "  'mechanism',\n",
       "  'error',\n",
       "  'control',\n",
       "  ',',\n",
       "  'flow',\n",
       "  'control',\n",
       "  'medium',\n",
       "  'access',\n",
       "  'control',\n",
       "  'data',\n",
       "  'link',\n",
       "  'layer'],\n",
       " ['Explain',\n",
       "  'main',\n",
       "  'function',\n",
       "  'network',\n",
       "  'layer',\n",
       "  'packet',\n",
       "  'switching',\n",
       "  ',',\n",
       "  'IP',\n",
       "  'addressing',\n",
       "  'fragmentation'],\n",
       " ['Discuss',\n",
       "  'operation',\n",
       "  'function',\n",
       "  'different',\n",
       "  'Transport',\n",
       "  'layer',\n",
       "  'protocol'],\n",
       " ['Implement', 'class', 'solve', 'given', 'problem'],\n",
       " ['Test', 'simple', 'class'],\n",
       " ['Design', 'class', 'using', 'existing', 'class', 'library'],\n",
       " ['Develop', 'class', 'hierarchy', 'using', 'inheritance'],\n",
       " ['Develop', 'class', 'simple', 'data', 'structure'],\n",
       " ['Design',\n",
       "  'implement',\n",
       "  'small',\n",
       "  'medium',\n",
       "  'size',\n",
       "  'software',\n",
       "  'problem',\n",
       "  'using',\n",
       "  'object'],\n",
       " ['Use', 'Arrays', 'Array-Lists', 'solving', 'problem'],\n",
       " ['Implement', 'user-defined', 'class', 'solve', 'given', 'problem'],\n",
       " ['Use',\n",
       "  'predefined',\n",
       "  'library',\n",
       "  'develop',\n",
       "  'program',\n",
       "  'graphical',\n",
       "  'user',\n",
       "  'interface'],\n",
       " ['Develop', 'class', 'hierarchy', 'using', 'inheritance'],\n",
       " ['Apply', 'project', 'lifecycle', 'process', 'project'],\n",
       " ['Apply', 'project', 'technique', 'tool', 'manage', 'project'],\n",
       " ['Identify', 'internal', 'external', 'constraint', 'given', 'project'],\n",
       " ['Produce', 'document', 'typically', 'used', 'development', 'project'],\n",
       " ['Demonstrate',\n",
       "  'verbal',\n",
       "  ',',\n",
       "  'written',\n",
       "  'communication',\n",
       "  'skill',\n",
       "  'part',\n",
       "  'team'],\n",
       " ['Explain', 'security', 'policy', ',', 'model', ',', 'mechanism'],\n",
       " ['Discuss', 'operating', 'system', 'security', 'model', 'mechanism'],\n",
       " ['Describe', 'cryptographic', 'technique', 'application'],\n",
       " ['Analyze', 'security', 'threat', 'vulnerability', 'computer', 'system'],\n",
       " ['Define', 'solution', 'defend', 'virus', 'malicious', 'program'],\n",
       " ['Explain', 'logical', 'progression', 'operating', 'system', 'development'],\n",
       " ['Explain', 'necessary', 'component', 'structure', 'operating', 'system'],\n",
       " ['Install', 'customize', 'operating', 'system'],\n",
       " ['Write', 'simple', 'shell', 'script', 'operating', 'system'],\n",
       " ['Evaluate',\n",
       "  'various',\n",
       "  'method',\n",
       "  'process',\n",
       "  'scheduling',\n",
       "  'inter-process',\n",
       "  'communication'],\n",
       " ['Explain', 'file-system', 'concept', 'operation'],\n",
       " ['Apply', 'recursion', 'solve', 'problem'],\n",
       " ['Use',\n",
       "  'APIs',\n",
       "  'implementing',\n",
       "  'moderate',\n",
       "  'size',\n",
       "  'program',\n",
       "  'data',\n",
       "  'structure'],\n",
       " ['Design', 'implement', 'linear', 'data', 'structure'],\n",
       " ['Design', 'implement', 'tree', 'data', 'structure'],\n",
       " ['Model', 'Solve', 'problem', 'using', 'graph'],\n",
       " ['Describe', 'main', 'concept', 'database', 'system'],\n",
       " ['Compare',\n",
       "  'database',\n",
       "  'system',\n",
       "  'approach',\n",
       "  'file-based',\n",
       "  'system',\n",
       "  'approach'],\n",
       " ['Design',\n",
       "  'database',\n",
       "  'using',\n",
       "  'entity-relationship',\n",
       "  'diagram',\n",
       "  '(',\n",
       "  'ERD',\n",
       "  ')'],\n",
       " ['Use',\n",
       "  'Relational',\n",
       "  'Algebra',\n",
       "  'perform',\n",
       "  'various',\n",
       "  'operation',\n",
       "  'relation'],\n",
       " ['Apply', 'normalization', 'database', 'table'],\n",
       " ['Function', 'effectively', 'team', 'create', 'query', 'database'],\n",
       " ['Analyze',\n",
       "  'issue',\n",
       "  'case',\n",
       "  'study',\n",
       "  'using',\n",
       "  'ethical',\n",
       "  'decision',\n",
       "  'making',\n",
       "  'based',\n",
       "  'code',\n",
       "  'ethic',\n",
       "  'formal',\n",
       "  'method'],\n",
       " ['Identify',\n",
       "  'privacy',\n",
       "  ',',\n",
       "  'freedom',\n",
       "  'speech',\n",
       "  'crime',\n",
       "  'issue',\n",
       "  'Cyberspace'],\n",
       " ['Discuss', 'intellectual', 'property', 'software', 'development', 'issue'],\n",
       " ['Discuss', 'implication', 'computing', 'workplace', 'worker', 'employer'],\n",
       " ['Discuss',\n",
       "  'socio-economic',\n",
       "  'implication',\n",
       "  'online',\n",
       "  'community',\n",
       "  'Digital',\n",
       "  'Divide'],\n",
       " ['Function',\n",
       "  'group',\n",
       "  'ass',\n",
       "  'current',\n",
       "  'ethical',\n",
       "  'issue',\n",
       "  'communicate',\n",
       "  'result',\n",
       "  'oral',\n",
       "  'written',\n",
       "  'form'],\n",
       " ['Describe',\n",
       "  'role',\n",
       "  'skill',\n",
       "  'entrepreneur',\n",
       "  'cultivate',\n",
       "  'entrepreneurial',\n",
       "  'mindset'],\n",
       " ['Apply',\n",
       "  'process',\n",
       "  'followed',\n",
       "  'entrepreneur',\n",
       "  'selecting',\n",
       "  'valuable',\n",
       "  'opportunity',\n",
       "  'related',\n",
       "  'business'],\n",
       " ['Explain', 'principle', 'success', 'factor', 'venture', 'planning'],\n",
       " ['Evaluate',\n",
       "  'viability',\n",
       "  'functional',\n",
       "  'planning',\n",
       "  'venture',\n",
       "  ',',\n",
       "  'considering',\n",
       "  'key',\n",
       "  'legal',\n",
       "  'intellectual',\n",
       "  'property',\n",
       "  'issue'],\n",
       " ['Create', 'business', 'model', 'financial', 'plan', 'venture'],\n",
       " ['Identify', 'structure', 'operation', 'workplace'],\n",
       " ['Recognize', 'employee', 'right', 'responsibility'],\n",
       " ['Apply', 'knowledge', 'workplace'],\n",
       " ['Function', 'effectively', ',', 'professionally', 'ethically', 'team'],\n",
       " ['Communicate', 'effectively', 'technically', 'orally', 'writing'],\n",
       " ['Recognize', 'need', 'continuing', 'professional', 'development'],\n",
       " ['Discuss', 'contemporary', 'cryptography', 'application'],\n",
       " ['Apply', 'hash', 'function', 'algorithm'],\n",
       " ['Investigate',\n",
       "  'mathematical',\n",
       "  'concept',\n",
       "  'required',\n",
       "  'cryptographic',\n",
       "  'algorithm'],\n",
       " ['Analyze', 'security', 'threat', 'associated', 'cryptographic', 'algorithm'],\n",
       " ['Evaluate', 'various', 'cryptographic', 'technique'],\n",
       " ['Analyze', 'classic', 'cryptographic', 'algorithm'],\n",
       " ['Implement', 'symmetric', 'cryptography', 'technique'],\n",
       " ['Implement', 'public-key', 'cryptosystems', 'technique'],\n",
       " ['Compare',\n",
       "  'security',\n",
       "  'property',\n",
       "  'well-known',\n",
       "  'cryptographic',\n",
       "  'protocol'],\n",
       " ['Implement', 'IPSec', 'VPN', 'solution', 'small', 'network'],\n",
       " ['Describe', 'TCP/IP', 'protocol', 'common', 'network', 'service'],\n",
       " ['Explain', 'network', 'traffic', 'service', 'filtering', 'concept'],\n",
       " ['Compare', 'stateless', 'stateful', 'firewall'],\n",
       " ['Assess', 'firewall', 'filtering', 'rule', 'consistency', 'efficiency'],\n",
       " ['Discuss', 'VPN', 'secure', 'network', 'architecture'],\n",
       " ['Demonstrate', 'teamwork', 'skill'],\n",
       " ['Describe', 'Internet', 'e-commerce', 'security', 'protocol'],\n",
       " ['Analyze',\n",
       "  'vulnerability',\n",
       "  'associated',\n",
       "  'insecure',\n",
       "  'Internet',\n",
       "  'e-commerce',\n",
       "  'protocol'],\n",
       " ['Demonstrate',\n",
       "  'security',\n",
       "  'protocol',\n",
       "  'used',\n",
       "  'achieve',\n",
       "  'internet',\n",
       "  'e-commerce',\n",
       "  'security'],\n",
       " ['Communicate',\n",
       "  'issue',\n",
       "  'relevant',\n",
       "  'public-key',\n",
       "  'infrastructure',\n",
       "  '(',\n",
       "  'PKI',\n",
       "  ')'],\n",
       " ['Explain',\n",
       "  'concept',\n",
       "  'trust',\n",
       "  '(',\n",
       "  'trust',\n",
       "  'model',\n",
       "  ')',\n",
       "  'internet',\n",
       "  'using',\n",
       "  'secure',\n",
       "  'internet',\n",
       "  'protocol'],\n",
       " ['Discuss', 'common', 'security', 'issue', 'software'],\n",
       " ['Analyze', 'software', 'vulnerability'],\n",
       " ['Explain', 'secure', 'software', 'design', '&', 'development', 'technique'],\n",
       " ['Compare',\n",
       "  'various',\n",
       "  'software',\n",
       "  'security',\n",
       "  'testing',\n",
       "  'tool',\n",
       "  'technique'],\n",
       " ['Analyze', 'web', 'application', 'security', 'threat', 'countermeasure'],\n",
       " ['Discuss', 'system', 'security', 'architecture', 'concept', 'attribute'],\n",
       " ['Design', 'secure', 'information', 'system', 'using', 'OM-AM', 'framework'],\n",
       " ['Apply', 'security', 'access', 'control', 'model'],\n",
       " ['Analyze',\n",
       "  'security',\n",
       "  'threat',\n",
       "  'associated',\n",
       "  'information',\n",
       "  'system',\n",
       "  'infrastructure'],\n",
       " ['Select', 'security', 'countermeasure', 'solution'],\n",
       " ['Evaluate', 'biometric', 'solution', 'authentication', 'protocol'],\n",
       " ['Configure', 'security', 'feature', 'network', 'device'],\n",
       " ['Analyze', 'common', 'network', 'threat', 'attack', 'vector'],\n",
       " ['Apply', 'attack', 'mitigation', 'technique'],\n",
       " ['Implement',\n",
       "  'packet',\n",
       "  'filter',\n",
       "  ',',\n",
       "  'stateful',\n",
       "  'firewall',\n",
       "  ',',\n",
       "  'application',\n",
       "  'layer',\n",
       "  'firewall'],\n",
       " ['Analyze',\n",
       "  'security',\n",
       "  'incident',\n",
       "  'using',\n",
       "  'intrusion',\n",
       "  'detection',\n",
       "  'prevention',\n",
       "  'system'],\n",
       " ['Evaluate', 'operating', 'system', 'security', 'aspect'],\n",
       " ['Configure', 'biometric', 'security', 'system'],\n",
       " ['Implement', 'lattice-based', 'access', 'control', 'model'],\n",
       " ['Design', 'active', 'response', 'security', 'architecture'],\n",
       " ['Use', 'digital', 'forensics', 'hardware', 'software', 'tool'],\n",
       " ['Analyze',\n",
       "  'designing',\n",
       "  'issue',\n",
       "  'pertaining',\n",
       "  'information',\n",
       "  'security',\n",
       "  'policy'],\n",
       " ['Investigate', 'sociological', 'legal', 'issue', 'policy', 'implementation'],\n",
       " ['Apply',\n",
       "  'principle',\n",
       "  'philosophy',\n",
       "  'underlie',\n",
       "  'successful',\n",
       "  'information',\n",
       "  'security',\n",
       "  'governance'],\n",
       " ['Discuss',\n",
       "  'interaction',\n",
       "  'information',\n",
       "  'security',\n",
       "  'concern',\n",
       "  'business',\n",
       "  'objective'],\n",
       " ['Evaluate',\n",
       "  'information',\n",
       "  'security',\n",
       "  'activity',\n",
       "  'within',\n",
       "  'implementation',\n",
       "  'project'],\n",
       " ['Analyze', 'common', 'security', 'threat', 'network', 'attack'],\n",
       " ['Evaluate', 'IPS', 'attack', 'signature', 'rule'],\n",
       " ['Compare', 'appropriate', 'countermeasure', 'common', 'network', 'attack'],\n",
       " ['Discuss', 'system', 'security', 'auditing', 'vulnerability', 'assessment'],\n",
       " ['Demonstrate', 'teamwork', 'skill'],\n",
       " ['Investigate', 'privacy', 'concern', 'different', 'context'],\n",
       " ['Apply', 'privacy', 'protection', 'solution'],\n",
       " ['Analyze',\n",
       "  'shortcoming',\n",
       "  'existing',\n",
       "  'privacy',\n",
       "  'technology',\n",
       "  'challenge',\n",
       "  'privacy',\n",
       "  'protection'],\n",
       " ['Discuss',\n",
       "  'privacy',\n",
       "  'legislation',\n",
       "  ',',\n",
       "  'policy',\n",
       "  'best',\n",
       "  'practice',\n",
       "  'different',\n",
       "  'context'],\n",
       " ['Design',\n",
       "  'information',\n",
       "  'security',\n",
       "  'risk',\n",
       "  'management',\n",
       "  'program',\n",
       "  'organization'],\n",
       " ['Implement', 'information', 'security', 'risk', 'management', 'roadmap'],\n",
       " ['Manage',\n",
       "  'information',\n",
       "  'security',\n",
       "  'risk',\n",
       "  'assessment',\n",
       "  'consulting',\n",
       "  'contract'],\n",
       " ['Compare', 'risk', 'assessment', 'technique'],\n",
       " ['Discuss', 'law', 'affecting', 'digital', 'forensics'],\n",
       " ['Analyze', 'concept', 'computer', 'digital', 'forensics'],\n",
       " ['Evaluate', 'current', 'computer', 'digital', 'forensics', 'tool'],\n",
       " ['Conduct',\n",
       "  'software',\n",
       "  'hardware',\n",
       "  'based',\n",
       "  'digital',\n",
       "  'forensic',\n",
       "  'analysis'],\n",
       " ['Evaluate', 'web', 'cloud', 'based', 'digital', 'forensic', 'tool'],\n",
       " ['Assess', 'digital', 'forensics', 'mobile', 'device', 'application'],\n",
       " ['Discuss', 'security', 'management', 'policy', 'best', 'practice'],\n",
       " ['Analyze',\n",
       "  'implement',\n",
       "  'Business',\n",
       "  'Continuity',\n",
       "  'Planning',\n",
       "  '(',\n",
       "  'BCP',\n",
       "  ')',\n",
       "  'Disaster',\n",
       "  'Recovery',\n",
       "  'Planning',\n",
       "  '(',\n",
       "  'DRP',\n",
       "  ')'],\n",
       " ['Criticize', 'legal', 'ethical', 'implication', 'security', 'management'],\n",
       " ['Apply', 'risk', 'evaluation', 'mitigation', 'strategy'],\n",
       " ['Apply', 'ISMS', 'standard'],\n",
       " ['Analyze', 'risk', 'management', 'auditing', 'technique'],\n",
       " ['Assess', 'vulnerability', 'hardware', 'device', 'system'],\n",
       " ['Analyze', 'attack', 'hardware', 'system'],\n",
       " ['Integrate', 'hardware', 'security', 'measure', 'design', 'metric'],\n",
       " ['Apply',\n",
       "  'detection',\n",
       "  ',',\n",
       "  'prevention',\n",
       "  'isolation',\n",
       "  'method',\n",
       "  'hardware',\n",
       "  'attack'],\n",
       " ['Evaluate', 'security', 'trust', 'hardware', 'system'],\n",
       " ['Describe', 'database', 'security', 'model', 'architecture'],\n",
       " ['Analyze', 'security', 'vulnerability', 'database'],\n",
       " ['Apply', 'database', 'security', 'model', 'policy', 'modern', 'database'],\n",
       " ['Discuss',\n",
       "  'data',\n",
       "  'application',\n",
       "  'auditing',\n",
       "  'procedure',\n",
       "  'database',\n",
       "  'security'],\n",
       " ['Compare', 'various', 'database', 'security', 'defense', 'mechanism'],\n",
       " ['Evaluate', 'option', 'needed', 'security', 'solution'],\n",
       " ['Compare', 'various', 'security', 'mechanism'],\n",
       " ['Apply',\n",
       "  'current',\n",
       "  'security',\n",
       "  'solution',\n",
       "  'address',\n",
       "  'certain',\n",
       "  'requirement'],\n",
       " ['Analyze', 'performance', 'applied', 'security', 'solution'],\n",
       " ['Explain', 'basic', 'principle', 'data', 'mining', 'process'],\n",
       " ['Prepare', 'data', 'mining', 'exploration'],\n",
       " ['Use',\n",
       "  'data',\n",
       "  'mining',\n",
       "  'technique',\n",
       "  'modern',\n",
       "  'tool',\n",
       "  'discover',\n",
       "  'trend',\n",
       "  'pattern',\n",
       "  'realistic',\n",
       "  'datasets'],\n",
       " ['Evaluate',\n",
       "  'different',\n",
       "  'data',\n",
       "  'mining',\n",
       "  'models/techniques',\n",
       "  'respect',\n",
       "  'performance',\n",
       "  'accuracy'],\n",
       " ['Function', 'team', 'communicate', 'effectively', 'written', 'oral', 'form']]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Array of all the PLOs and ULOs (We can couple them together as we're trying to identify Bloom/Solo level here)\n",
    "lo_sentence_array = []\n",
    "\n",
    "# TODO: train CLO classification with all data instead of just one course.\n",
    "for table in document.tables:\n",
    "    for row in table.rows:\n",
    "        for cell in row.cells[1:]:\n",
    "            tokens = nltk.word_tokenize(cell.text)\n",
    "            cleaned_tokens = [lemmatizer.lemmatize(token) for token in tokens if token.lower() not in stop_words]\n",
    "            lo_sentence_array.append(cleaned_tokens)\n",
    "\n",
    "# build the vocabulary and train the model\n",
    "# IMPORTANT, N0TE THAT sg=1 flag specifies Word2Vec to use the Skip Gram Model as designated by the LSTM paper.\n",
    "model = Word2Vec(sentences=lo_sentence_array,vector_size=100, window=5, min_count=1, workers=4, sg=1)\n",
    "\n",
    "# train the model with the course's ULOs and PLOs.\n",
    "model.train([tokens], total_examples=len([tokens]), epochs=10)\n",
    "\n",
    "lo_sentence_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/apro0009/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/apro0009/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/apro0009/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import docx2txt\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Embedding\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "\n",
    "# load in the Word document using docx2txt\n",
    "doc_text = docx2txt.process(CURRENT_MAPPING)\n",
    "\n",
    "# preprocess the text using NLTK\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# tokenize the text and remove stop words and non-alphabetic characters\n",
    "tokens = [word.lower() for word in word_tokenize(doc_text) if word.isalpha() and word.lower() not in stop_words]\n",
    "\n",
    "# lemmatization\n",
    "lemmatized_tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "\n",
    "\n",
    "# convert tokens back to single string format\n",
    "corpus = ' '.join(lemmatized_tokens)\n",
    "\n",
    "# create a tokenizer and fit on the corpus\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts([corpus])\n",
    "\n",
    "# convert the text to a sequence of integers\n",
    "# sequences = tokenizer.texts_to_sequences([corpus])\n",
    "\n",
    "# pad the sequences to have a fixed length\n",
    "max_length = 50\n",
    "# padded_sequences = pad_sequences(sequences, maxlen=max_length, padding='post')\n",
    "\n",
    "# LSTM model\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "embedding_dim = 50\n",
    "\n",
    "model = Sequential([\n",
    "    tf.keras.layers.Embedding(10000, 16),\n",
    "    tf.keras.layers.GlobalAveragePooling1D(),\n",
    "    tf.keras.layers.Dense(16, activation='relu'),\n",
    "    tf.keras.layers.Dropout(0.2), # Dropout rate set to 0.2 as specified from the paper\n",
    "    tf.keras.layers.Dense(6, activation='softmax')\n",
    "])\n",
    "\n",
    "model.add(Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=max_length))\n",
    "model.add(LSTM(units=64, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(units=3, activation='softmax'))\n",
    "\n",
    "# RMS Optimizer as specified by the paper.\n",
    "optimizer = RMSprop(learning_rate=0.001)\n",
    "\n",
    "# compile the model\n",
    "model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# train the model\n",
    "# X_train and y_train are assumed to be already defined\n",
    "# \n",
    "# model.fit(X_train, y_train, epochs=10, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import re\n",
    "import string\n",
    "import tqdm\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "## training data LO\n",
    "## sentence = \"use big data streaming technologies.\"\n",
    "## word = \"apply\"\n",
    "## categories = [\"Remembering\", \"Understanding\", \"Applying\", \"Analysing\", \"Evaluating\", \"Creating\"]\n",
    "\n",
    "\n",
    "\n",
    "# Load the TensorBoard notebook extension\n",
    "%load_ext tensorboard\n",
    "\n",
    "SEED = 42\n",
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "\n",
    "sentence = \"The wide road shimmered in the hot sun\"\n",
    "tokens = list(sentence.lower().split())\n",
    "print(len(tokens))\n",
    "\n",
    "vocab, index = {}, 1  # start indexing from 1\n",
    "vocab['<pad>'] = 0  # add a padding token\n",
    "for token in tokens:\n",
    "  if token not in vocab:\n",
    "    vocab[token] = index\n",
    "    index += 1\n",
    "vocab_size = len(vocab)\n",
    "print(vocab)\n",
    "\n",
    "inverse_vocab = {index: token for token, index in vocab.items()}\n",
    "print(inverse_vocab)\n",
    "\n",
    "example_sequence = [vocab[word] for word in tokens]\n",
    "print(example_sequence)\n",
    "\n",
    "window_size = 2\n",
    "positive_skip_grams, _ = tf.keras.preprocessing.sequence.skipgrams(\n",
    "      example_sequence,\n",
    "      vocabulary_size=vocab_size,\n",
    "      window_size=window_size,\n",
    "      negative_samples=0)\n",
    "print(len(positive_skip_grams))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## packages to install\n",
    "%%bash\n",
    "\n",
    "python3 -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test1\n",
      "test2\n"
     ]
    }
   ],
   "source": [
    "## This code loads the vector file into the word_vectors variable\n",
    "## Download the vector file from https://fasttext.cc/docs/en/english-vectors.html (first file on the website), unzip the file and store in your local development folder\n",
    "## Note: This piece of code may take upto an hour or two to run depending on your pc specs.\n",
    "## My i5 8th gen with 8gig ram took 58mins to run.\n",
    "\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "# Path to the downloaded .vec file\n",
    "path_to_vectors = 'wiki-news-300d-1M.vec'\n",
    "# Load the word vectors\n",
    "word_vectors = KeyedVectors.load_word2vec_format(path_to_vectors)\n",
    "\n",
    "# Find similar words\n",
    "similar_words = word_vectors.most_similar('cat')\n",
    "\n",
    "# Calculate word similarity\n",
    "similarity = word_vectors.similarity('cat', 'dog')\n",
    "\n",
    "# Perform vector arithmetic\n",
    "result = word_vectors['king'] - word_vectors['man'] + word_vectors['woman']\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tensorboard extension is already loaded. To reload it, use:\n",
      "  %reload_ext tensorboard\n"
     ]
    }
   ],
   "source": [
    "import io\n",
    "import re\n",
    "import string\n",
    "import tqdm\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "## training data LO\n",
    "## sentence = \"use big data streaming technologies.\"\n",
    "## word = \"apply\"\n",
    "## categories = [\"Remembering\", \"Understanding\", \"Applying\", \"Analysing\", \"Evaluating\", \"Creating\"]\n",
    "\n",
    "\n",
    "\n",
    "# Load the TensorBoard notebook extension\n",
    "%load_ext tensorboard\n",
    "\n",
    "SEED = 42\n",
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "\n",
    "\n",
    "def generate_skipgrams(sentence):\n",
    "    tokens = list(sentence.lower().split())\n",
    "    print(len(tokens))\n",
    "\n",
    "    vocab, index = {}, 1  # start indexing from 1\n",
    "    vocab['<pad>'] = 0  # add a padding token\n",
    "    for token in tokens:\n",
    "      if token not in vocab:\n",
    "        vocab[token] = index\n",
    "        index += 1\n",
    "    vocab_size = len(vocab)\n",
    "    print(vocab)\n",
    "\n",
    "    inverse_vocab = {index: token for token, index in vocab.items()}\n",
    "    print(inverse_vocab)\n",
    "\n",
    "    example_sequence = [vocab[word] for word in tokens]\n",
    "    print(example_sequence)\n",
    "\n",
    "    window_size = 2\n",
    "    positive_skip_grams, _ = tf.keras.preprocessing.sequence.skipgrams(\n",
    "          example_sequence,\n",
    "          vocabulary_size=vocab_size,\n",
    "          window_size=window_size,\n",
    "          negative_samples=0)\n",
    "    \n",
    "    return positive_skip_grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "## Function to identify verbs in a sentence\n",
    "def identify_verbs(sentence):\n",
    "    # Load the English language model in spaCy\n",
    "    nlp = spacy.load('en_core_web_sm')\n",
    "    \n",
    "    # Process the sentence using spaCy\n",
    "    doc = nlp(sentence)\n",
    "    \n",
    "    # Extract the verbs from the processed sentence\n",
    "    verbs = [token.lemma_ for token in doc if token.pos_ == 'VERB']\n",
    "    \n",
    "    return verbs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['apply', 'learn']\n",
      "Sentence:  apply common data analytics and machine learning algorithms in a big data environment.  Identified blooms level:  Applying\n",
      "['use']\n",
      "Sentence:  use big data streaming technologies.  Identified blooms level:  Applying\n"
     ]
    }
   ],
   "source": [
    "## Main piece of code that performs the mapping \n",
    "\n",
    "sentences = [\n",
    "    \"apply common data analytics and machine learning algorithms in a big data environment.\",\n",
    "    \"use big data streaming technologies.\"\n",
    "]\n",
    "bloom_levels = [\"Remembering\", \"Understanding\", \"Applying\", \"Analysing\", \"Evaluating\", \"Creating\"]\n",
    "\n",
    "# identified_levels = []\n",
    "final_level = None\n",
    "\n",
    "for i in range(len(sentences)):\n",
    "    verbs = identify_verbs(sentences[i])\n",
    "    print(verbs)\n",
    "    score = 0\n",
    "    for j in range(len(verbs)):\n",
    "        for k in range(len(bloom_levels)):\n",
    "            similarity_score = word_vectors.similarity(verbs[j], bloom_levels[k])\n",
    "            if similarity_score >= score:\n",
    "                score=similarity_score\n",
    "                final_level = bloom_levels[k]\n",
    "    print(\"Sentence: \", sentences[i], \" Identified blooms level: \", final_level)\n",
    "\n",
    "\n",
    "### Todos\n",
    "# Find a way to use skipgrams\n",
    "# This method only works for blooms since this paper is only based on blooms mapping\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
