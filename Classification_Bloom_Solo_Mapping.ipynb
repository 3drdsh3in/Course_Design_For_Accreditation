{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "# Dependencies\n",
    "pip install docx2txt\n",
    "pip install gensim\n",
    "pip install keras\n",
    "pip install nltk\n",
    "pip install -U scikit-learn\n",
    "pip install python-docx\n",
    "pip install tensorflow\n",
    "\n",
    "if ls docx2csv >/dev/null 2>&1; then\n",
    "    echo \"docx2csv exists.\"\n",
    "else\n",
    "    echo \"Folder does not exist. Cloning docx2csv.\"\n",
    "    git clone https://github.com/ivbeg/docx2csv.git\n",
    "fi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----- TEST DATA INPUT -----\n",
    "\n",
    "# Computer Science Test Data.\n",
    "# CURRENT_MAPPING=\"Lists_ComputerScience.docx\"\n",
    "# ORIGINAL_MAPPING=\"Original-Mapping-ComputerScience.csv\"\n",
    "\n",
    "# InformationSecurity Test Data.\n",
    "CURRENT_MAPPING=\"Lists_InformationSecurity.docx\"\n",
    "ORIGINAL_MAPPING=\"Original-Mapping-InfoSecurity.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "from gensim.test.utils import common_texts\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract tables from word document\n",
    "from docx2csv import extract_tables, extract\n",
    "tables = extract_tables(CURRENT_MAPPING)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from docx import Document\n",
    "document = Document(CURRENT_MAPPING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Array of all the PLOs and ULOs (We can couple them together as we're trying to identify Bloom/Solo level here)\n",
    "lo_sentence_array = []\n",
    "\n",
    "# TODO: train CLO classification with all data instead of just one course.\n",
    "for table in document.tables:\n",
    "    for row in table.rows:\n",
    "        for cell in row.cells[1:]:\n",
    "            tokens = nltk.word_tokenize(cell.text)\n",
    "            cleaned_tokens = [lemmatizer.lemmatize(token) for token in tokens if token.lower() not in stop_words]\n",
    "            lo_sentence_array.append(cleaned_tokens)\n",
    "\n",
    "# build the vocabulary and train the model\n",
    "# IMPORTANT, N0TE THAT sg=1 flag specifies Word2Vec to use the Skip Gram Model as designated by the LSTM paper.\n",
    "model = Word2Vec(sentences=lo_sentence_array,vector_size=100, window=5, min_count=1, workers=4, sg=1)\n",
    "\n",
    "# train the model with the course's ULOs and PLOs.\n",
    "model.train([tokens], total_examples=len([tokens]), epochs=10)\n",
    "\n",
    "lo_sentence_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import docx2txt\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Embedding\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "\n",
    "# load in the Word document using docx2txt\n",
    "doc_text = docx2txt.process(CURRENT_MAPPING)\n",
    "\n",
    "# preprocess the text using NLTK\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# tokenize the text and remove stop words and non-alphabetic characters\n",
    "tokens = [word.lower() for word in word_tokenize(doc_text) if word.isalpha() and word.lower() not in stop_words]\n",
    "\n",
    "# lemmatization\n",
    "lemmatized_tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "\n",
    "\n",
    "# convert tokens back to single string format\n",
    "corpus = ' '.join(lemmatized_tokens)\n",
    "\n",
    "# create a tokenizer and fit on the corpus\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts([corpus])\n",
    "\n",
    "# convert the text to a sequence of integers\n",
    "# sequences = tokenizer.texts_to_sequences([corpus])\n",
    "\n",
    "# pad the sequences to have a fixed length\n",
    "max_length = 50\n",
    "# padded_sequences = pad_sequences(sequences, maxlen=max_length, padding='post')\n",
    "\n",
    "# LSTM model\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "embedding_dim = 50\n",
    "\n",
    "model = Sequential([\n",
    "    tf.keras.layers.Embedding(10000, 16),\n",
    "    tf.keras.layers.GlobalAveragePooling1D(),\n",
    "    tf.keras.layers.Dense(16, activation='relu'),\n",
    "    tf.keras.layers.Dropout(0.2), # Dropout rate set to 0.2 as specified from the paper\n",
    "    tf.keras.layers.Dense(6, activation='softmax')\n",
    "])\n",
    "\n",
    "model.add(Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=max_length))\n",
    "model.add(LSTM(units=64, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(units=3, activation='softmax'))\n",
    "\n",
    "# RMS Optimizer as specified by the paper.\n",
    "optimizer = RMSprop(learning_rate=0.001)\n",
    "\n",
    "# compile the model\n",
    "model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# train the model\n",
    "# X_train and y_train are assumed to be already defined\n",
    "# \n",
    "# model.fit(X_train, y_train, epochs=10, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import re\n",
    "import string\n",
    "import tqdm\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "## training data LO\n",
    "## sentence = \"use big data streaming technologies.\"\n",
    "## word = \"apply\"\n",
    "## categories = [\"Remembering\", \"Understanding\", \"Applying\", \"Analysing\", \"Evaluating\", \"Creating\"]\n",
    "\n",
    "\n",
    "\n",
    "# Load the TensorBoard notebook extension\n",
    "%load_ext tensorboard\n",
    "\n",
    "SEED = 42\n",
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "\n",
    "sentence = \"The wide road shimmered in the hot sun\"\n",
    "tokens = list(sentence.lower().split())\n",
    "print(len(tokens))\n",
    "\n",
    "vocab, index = {}, 1  # start indexing from 1\n",
    "vocab['<pad>'] = 0  # add a padding token\n",
    "for token in tokens:\n",
    "  if token not in vocab:\n",
    "    vocab[token] = index\n",
    "    index += 1\n",
    "vocab_size = len(vocab)\n",
    "print(vocab)\n",
    "\n",
    "inverse_vocab = {index: token for token, index in vocab.items()}\n",
    "print(inverse_vocab)\n",
    "\n",
    "example_sequence = [vocab[word] for word in tokens]\n",
    "print(example_sequence)\n",
    "\n",
    "window_size = 2\n",
    "positive_skip_grams, _ = tf.keras.preprocessing.sequence.skipgrams(\n",
    "      example_sequence,\n",
    "      vocabulary_size=vocab_size,\n",
    "      window_size=window_size,\n",
    "      negative_samples=0)\n",
    "print(len(positive_skip_grams))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## packages to install\n",
    "%%bash\n",
    "\n",
    "python3 -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test1\n",
      "test2\n"
     ]
    }
   ],
   "source": [
    "## This code loads the vector file into the word_vectors variable\n",
    "## Note: This piece of code may take upto an hour or two to run depending on your pc specs.\n",
    "## My i5 8th gen with 8gig ram took 58mins to run.\n",
    "\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "# Path to the downloaded .vec file\n",
    "path_to_vectors = 'wiki-news-300d-1M.vec'\n",
    "# Load the word vectors\n",
    "word_vectors = KeyedVectors.load_word2vec_format(path_to_vectors)\n",
    "\n",
    "# Find similar words\n",
    "similar_words = word_vectors.most_similar('cat')\n",
    "\n",
    "# Calculate word similarity\n",
    "similarity = word_vectors.similarity('cat', 'dog')\n",
    "\n",
    "# Perform vector arithmetic\n",
    "result = word_vectors['king'] - word_vectors['man'] + word_vectors['woman']\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tensorboard extension is already loaded. To reload it, use:\n",
      "  %reload_ext tensorboard\n"
     ]
    }
   ],
   "source": [
    "import io\n",
    "import re\n",
    "import string\n",
    "import tqdm\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "## training data LO\n",
    "## sentence = \"use big data streaming technologies.\"\n",
    "## word = \"apply\"\n",
    "## categories = [\"Remembering\", \"Understanding\", \"Applying\", \"Analysing\", \"Evaluating\", \"Creating\"]\n",
    "\n",
    "\n",
    "\n",
    "# Load the TensorBoard notebook extension\n",
    "%load_ext tensorboard\n",
    "\n",
    "SEED = 42\n",
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "\n",
    "\n",
    "def generate_skipgrams(sentence):\n",
    "    tokens = list(sentence.lower().split())\n",
    "    print(len(tokens))\n",
    "\n",
    "    vocab, index = {}, 1  # start indexing from 1\n",
    "    vocab['<pad>'] = 0  # add a padding token\n",
    "    for token in tokens:\n",
    "      if token not in vocab:\n",
    "        vocab[token] = index\n",
    "        index += 1\n",
    "    vocab_size = len(vocab)\n",
    "    print(vocab)\n",
    "\n",
    "    inverse_vocab = {index: token for token, index in vocab.items()}\n",
    "    print(inverse_vocab)\n",
    "\n",
    "    example_sequence = [vocab[word] for word in tokens]\n",
    "    print(example_sequence)\n",
    "\n",
    "    window_size = 2\n",
    "    positive_skip_grams, _ = tf.keras.preprocessing.sequence.skipgrams(\n",
    "          example_sequence,\n",
    "          vocabulary_size=vocab_size,\n",
    "          window_size=window_size,\n",
    "          negative_samples=0)\n",
    "    \n",
    "    return positive_skip_grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "## Function to identify verbs in a sentence\n",
    "def identify_verbs(sentence):\n",
    "    # Load the English language model in spaCy\n",
    "    nlp = spacy.load('en_core_web_sm')\n",
    "    \n",
    "    # Process the sentence using spaCy\n",
    "    doc = nlp(sentence)\n",
    "    \n",
    "    # Extract the verbs from the processed sentence\n",
    "    verbs = [token.lemma_ for token in doc if token.pos_ == 'VERB']\n",
    "    \n",
    "    return verbs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['apply', 'learn']\n",
      "Sentence:  apply common data analytics and machine learning algorithms in a big data environment.  Identified blooms level:  Applying\n",
      "['use']\n",
      "Sentence:  use big data streaming technologies.  Identified blooms level:  Applying\n"
     ]
    }
   ],
   "source": [
    "## Main piece of code that performs the mapping \n",
    "\n",
    "sentences = [\n",
    "    \"apply common data analytics and machine learning algorithms in a big data environment.\",\n",
    "    \"use big data streaming technologies.\"\n",
    "]\n",
    "bloom_levels = [\"Remembering\", \"Understanding\", \"Applying\", \"Analysing\", \"Evaluating\", \"Creating\"]\n",
    "\n",
    "# identified_levels = []\n",
    "final_level = None\n",
    "\n",
    "for i in range(len(sentences)):\n",
    "    verbs = identify_verbs(sentences[i])\n",
    "    print(verbs)\n",
    "    score = 0\n",
    "    for j in range(len(verbs)):\n",
    "        for k in range(len(bloom_levels)):\n",
    "            similarity_score = word_vectors.similarity(verbs[j], bloom_levels[k])\n",
    "            if similarity_score >= score:\n",
    "                score=similarity_score\n",
    "                final_level = bloom_levels[k]\n",
    "    print(\"Sentence: \", sentences[i], \" Identified blooms level: \", final_level)\n",
    "\n",
    "\n",
    "### Todos\n",
    "# Find a way to use skipgrams\n",
    "# This method only works for blooms since this paper is only based on blooms mapping\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
