{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dependencies\n",
    "\n",
    "The following code installs dependencies and imports modules that are used.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: docx2txt in /home/arragon/.local/lib/python3.10/site-packages (0.8)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: gensim in /home/arragon/.local/lib/python3.10/site-packages (4.3.1)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in /home/arragon/.local/lib/python3.10/site-packages (from gensim) (6.3.0)\n",
      "Requirement already satisfied: scipy>=1.7.0 in /home/arragon/.local/lib/python3.10/site-packages (from gensim) (1.11.1)\n",
      "Requirement already satisfied: numpy>=1.18.5 in /home/arragon/.local/lib/python3.10/site-packages (from gensim) (1.24.3)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: keras in /home/arragon/.local/lib/python3.10/site-packages (2.13.1)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: nltk in /home/arragon/.local/lib/python3.10/site-packages (3.8.1)\n",
      "Requirement already satisfied: tqdm in /home/arragon/.local/lib/python3.10/site-packages (from nltk) (4.65.1)\n",
      "Requirement already satisfied: click in /home/arragon/.local/lib/python3.10/site-packages (from nltk) (8.1.6)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /home/arragon/.local/lib/python3.10/site-packages (from nltk) (2023.6.3)\n",
      "Requirement already satisfied: joblib in /home/arragon/.local/lib/python3.10/site-packages (from nltk) (1.3.1)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: scikit-learn in /home/arragon/.local/lib/python3.10/site-packages (1.3.0)\n",
      "Requirement already satisfied: numpy>=1.17.3 in /home/arragon/.local/lib/python3.10/site-packages (from scikit-learn) (1.24.3)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /home/arragon/.local/lib/python3.10/site-packages (from scikit-learn) (1.3.1)\n",
      "Requirement already satisfied: scipy>=1.5.0 in /home/arragon/.local/lib/python3.10/site-packages (from scikit-learn) (1.11.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/arragon/.local/lib/python3.10/site-packages (from scikit-learn) (3.2.0)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: python-docx in /home/arragon/.local/lib/python3.10/site-packages (0.8.11)\n",
      "Requirement already satisfied: lxml>=2.3.2 in /home/arragon/.local/lib/python3.10/site-packages (from python-docx) (4.9.3)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: tensorflow in /home/arragon/.local/lib/python3.10/site-packages (2.13.0)\n",
      "Requirement already satisfied: h5py>=2.9.0 in /home/arragon/.local/lib/python3.10/site-packages (from tensorflow) (3.9.0)\n",
      "Requirement already satisfied: six>=1.12.0 in /home/arragon/.local/lib/python3.10/site-packages (from tensorflow) (1.12.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /home/arragon/.local/lib/python3.10/site-packages (from tensorflow) (16.0.6)\n",
      "Requirement already satisfied: tensorboard<2.14,>=2.13 in /home/arragon/.local/lib/python3.10/site-packages (from tensorflow) (2.13.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /home/arragon/.local/lib/python3.10/site-packages (from tensorflow) (1.56.2)\n",
      "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /home/arragon/.local/lib/python3.10/site-packages (from tensorflow) (0.4.0)\n",
      "Requirement already satisfied: flatbuffers>=23.1.21 in /home/arragon/.local/lib/python3.10/site-packages (from tensorflow) (23.5.26)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /home/arragon/.local/lib/python3.10/site-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /home/arragon/.local/lib/python3.10/site-packages (from tensorflow) (4.23.4)\n",
      "Requirement already satisfied: setuptools in /home/arragon/.local/lib/python3.10/site-packages (from tensorflow) (68.0.0)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /home/arragon/.local/lib/python3.10/site-packages (from tensorflow) (0.33.0)\n",
      "Requirement already satisfied: tensorflow-estimator<2.14,>=2.13.0 in /home/arragon/.local/lib/python3.10/site-packages (from tensorflow) (2.13.0)\n",
      "Requirement already satisfied: typing-extensions<4.6.0,>=3.6.6 in /home/arragon/.local/lib/python3.10/site-packages (from tensorflow) (4.5.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /home/arragon/.local/lib/python3.10/site-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: packaging in /home/arragon/.local/lib/python3.10/site-packages (from tensorflow) (23.1)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /home/arragon/.local/lib/python3.10/site-packages (from tensorflow) (3.3.0)\n",
      "Requirement already satisfied: keras<2.14,>=2.13.1 in /home/arragon/.local/lib/python3.10/site-packages (from tensorflow) (2.13.1)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /home/arragon/.local/lib/python3.10/site-packages (from tensorflow) (2.3.0)\n",
      "Requirement already satisfied: numpy<=1.24.3,>=1.22 in /home/arragon/.local/lib/python3.10/site-packages (from tensorflow) (1.24.3)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /home/arragon/.local/lib/python3.10/site-packages (from tensorflow) (1.15.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /home/arragon/.local/lib/python3.10/site-packages (from tensorflow) (1.4.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/lib/python3/dist-packages (from astunparse>=1.6.0->tensorflow) (0.37.1)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /home/arragon/.local/lib/python3.10/site-packages (from tensorboard<2.14,>=2.13->tensorflow) (2.22.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /home/arragon/.local/lib/python3.10/site-packages (from tensorboard<2.14,>=2.13->tensorflow) (3.4.4)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /home/arragon/.local/lib/python3.10/site-packages (from tensorboard<2.14,>=2.13->tensorflow) (0.7.1)\n",
      "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /home/arragon/.local/lib/python3.10/site-packages (from tensorboard<2.14,>=2.13->tensorflow) (1.0.0)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /home/arragon/.local/lib/python3.10/site-packages (from tensorboard<2.14,>=2.13->tensorflow) (2.22.0)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /home/arragon/.local/lib/python3.10/site-packages (from tensorboard<2.14,>=2.13->tensorflow) (2.3.6)\n",
      "Requirement already satisfied: urllib3<2.0 in /home/arragon/.local/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow) (1.25.3)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /home/arragon/.local/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow) (0.3.0)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /home/arragon/.local/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow) (5.3.1)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /home/arragon/.local/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow) (3.4.2)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /home/arragon/.local/lib/python3.10/site-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.14,>=2.13->tensorflow) (1.3.1)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /home/arragon/.local/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow) (2.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/arragon/.local/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow) (2019.6.16)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /home/arragon/.local/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow) (3.0.4)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /home/arragon/.local/lib/python3.10/site-packages (from werkzeug>=1.0.1->tensorboard<2.14,>=2.13->tensorflow) (2.1.3)\n",
      "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /home/arragon/.local/lib/python3.10/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow) (0.5.0)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /usr/lib/python3/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.14,>=2.13->tensorflow) (3.2.0)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: pandas in /home/arragon/.local/lib/python3.10/site-packages (2.0.3)\n",
      "Requirement already satisfied: numpy>=1.21.0 in /home/arragon/.local/lib/python3.10/site-packages (from pandas) (1.24.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/arragon/.local/lib/python3.10/site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /home/arragon/.local/lib/python3.10/site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/arragon/.local/lib/python3.10/site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: six>=1.5 in /home/arragon/.local/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas) (1.12.0)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: openpyxl in /home/arragon/.local/lib/python3.10/site-packages (3.1.2)\n",
      "Requirement already satisfied: et-xmlfile in /home/arragon/.local/lib/python3.10/site-packages (from openpyxl) (1.1.0)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: nltk in /home/arragon/.local/lib/python3.10/site-packages (3.8.1)\n",
      "Requirement already satisfied: tqdm in /home/arragon/.local/lib/python3.10/site-packages (from nltk) (4.65.1)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /home/arragon/.local/lib/python3.10/site-packages (from nltk) (2023.6.3)\n",
      "Requirement already satisfied: click in /home/arragon/.local/lib/python3.10/site-packages (from nltk) (8.1.6)\n",
      "Requirement already satisfied: joblib in /home/arragon/.local/lib/python3.10/site-packages (from nltk) (1.3.1)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: spacy in /home/arragon/.local/lib/python3.10/site-packages (3.6.0)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /home/arragon/.local/lib/python3.10/site-packages (from spacy) (6.3.0)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /home/arragon/.local/lib/python3.10/site-packages (from spacy) (2.4.7)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /home/arragon/.local/lib/python3.10/site-packages (from spacy) (2.0.7)\n",
      "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in /home/arragon/.local/lib/python3.10/site-packages (from spacy) (8.1.11)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /home/arragon/.local/lib/python3.10/site-packages (from spacy) (2.22.0)\n",
      "Requirement already satisfied: pathy>=0.10.0 in /home/arragon/.local/lib/python3.10/site-packages (from spacy) (0.10.2)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /home/arragon/.local/lib/python3.10/site-packages (from spacy) (2.0.9)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/arragon/.local/lib/python3.10/site-packages (from spacy) (23.1)\n",
      "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /home/arragon/.local/lib/python3.10/site-packages (from spacy) (0.9.0)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /home/arragon/.local/lib/python3.10/site-packages (from spacy) (1.24.3)\n",
      "Requirement already satisfied: setuptools in /home/arragon/.local/lib/python3.10/site-packages (from spacy) (68.0.0)\n",
      "Requirement already satisfied: jinja2 in /home/arragon/.local/lib/python3.10/site-packages (from spacy) (3.1.2)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /home/arragon/.local/lib/python3.10/site-packages (from spacy) (3.0.8)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /home/arragon/.local/lib/python3.10/site-packages (from spacy) (1.0.4)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /home/arragon/.local/lib/python3.10/site-packages (from spacy) (1.10.12)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /home/arragon/.local/lib/python3.10/site-packages (from spacy) (1.1.2)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /home/arragon/.local/lib/python3.10/site-packages (from spacy) (3.3.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /home/arragon/.local/lib/python3.10/site-packages (from spacy) (4.65.1)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /home/arragon/.local/lib/python3.10/site-packages (from spacy) (3.0.12)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /home/arragon/.local/lib/python3.10/site-packages (from spacy) (1.0.9)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in /home/arragon/.local/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy) (4.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/arragon/.local/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2019.6.16)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /home/arragon/.local/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.8)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /home/arragon/.local/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy) (1.25.3)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /home/arragon/.local/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.0.4)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /home/arragon/.local/lib/python3.10/site-packages (from thinc<8.2.0,>=8.1.8->spacy) (0.7.10)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /home/arragon/.local/lib/python3.10/site-packages (from thinc<8.2.0,>=8.1.8->spacy) (0.1.1)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /home/arragon/.local/lib/python3.10/site-packages (from typer<0.10.0,>=0.3.0->spacy) (8.1.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/arragon/.local/lib/python3.10/site-packages (from jinja2->spacy) (2.1.3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-21 09:26:43.783650: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-09-21 09:26:45.061626: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "2023-09-21 09:26:47.209672: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] could not open file to read NUMA node: /sys/bus/pci/devices/0000:29:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-09-21 09:26:47.209962: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1960] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting en-core-web-trf==3.6.1\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_trf-3.6.1/en_core_web_trf-3.6.1-py3-none-any.whl (460.3 MB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 460.3/460.3 MB 2.4 MB/s eta 0:00:00\n",
      "Requirement already satisfied: spacy-transformers<1.3.0,>=1.2.2 in /home/arragon/.local/lib/python3.10/site-packages (from en-core-web-trf==3.6.1) (1.2.5)\n",
      "Requirement already satisfied: spacy<3.7.0,>=3.6.0 in /home/arragon/.local/lib/python3.10/site-packages (from en-core-web-trf==3.6.1) (3.6.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/arragon/.local/lib/python3.10/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-trf==3.6.1) (23.1)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /home/arragon/.local/lib/python3.10/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-trf==3.6.1) (4.65.1)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /home/arragon/.local/lib/python3.10/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-trf==3.6.1) (3.0.8)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /home/arragon/.local/lib/python3.10/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-trf==3.6.1) (2.22.0)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /home/arragon/.local/lib/python3.10/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-trf==3.6.1) (2.0.9)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /home/arragon/.local/lib/python3.10/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-trf==3.6.1) (1.0.4)\n",
      "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /home/arragon/.local/lib/python3.10/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-trf==3.6.1) (0.9.0)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /home/arragon/.local/lib/python3.10/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-trf==3.6.1) (1.24.3)\n",
      "Requirement already satisfied: jinja2 in /home/arragon/.local/lib/python3.10/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-trf==3.6.1) (3.1.2)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /home/arragon/.local/lib/python3.10/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-trf==3.6.1) (2.4.7)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /home/arragon/.local/lib/python3.10/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-trf==3.6.1) (1.1.2)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /home/arragon/.local/lib/python3.10/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-trf==3.6.1) (1.0.9)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /home/arragon/.local/lib/python3.10/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-trf==3.6.1) (2.0.7)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /home/arragon/.local/lib/python3.10/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-trf==3.6.1) (6.3.0)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /home/arragon/.local/lib/python3.10/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-trf==3.6.1) (1.10.12)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /home/arragon/.local/lib/python3.10/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-trf==3.6.1) (3.3.0)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /home/arragon/.local/lib/python3.10/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-trf==3.6.1) (3.0.12)\n",
      "Requirement already satisfied: setuptools in /home/arragon/.local/lib/python3.10/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-trf==3.6.1) (68.0.0)\n",
      "Requirement already satisfied: pathy>=0.10.0 in /home/arragon/.local/lib/python3.10/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-trf==3.6.1) (0.10.2)\n",
      "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in /home/arragon/.local/lib/python3.10/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-trf==3.6.1) (8.1.11)\n",
      "Requirement already satisfied: spacy-alignments<1.0.0,>=0.7.2 in /home/arragon/.local/lib/python3.10/site-packages (from spacy-transformers<1.3.0,>=1.2.2->en-core-web-trf==3.6.1) (0.9.0)\n",
      "Requirement already satisfied: transformers<4.31.0,>=3.4.0 in /home/arragon/.local/lib/python3.10/site-packages (from spacy-transformers<1.3.0,>=1.2.2->en-core-web-trf==3.6.1) (4.30.2)\n",
      "Requirement already satisfied: torch>=1.8.0 in /home/arragon/.local/lib/python3.10/site-packages (from spacy-transformers<1.3.0,>=1.2.2->en-core-web-trf==3.6.1) (2.0.1)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in /home/arragon/.local/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy<3.7.0,>=3.6.0->en-core-web-trf==3.6.1) (4.5.0)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /home/arragon/.local/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->en-core-web-trf==3.6.1) (1.25.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/arragon/.local/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->en-core-web-trf==3.6.1) (2019.6.16)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /home/arragon/.local/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->en-core-web-trf==3.6.1) (2.8)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /home/arragon/.local/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->en-core-web-trf==3.6.1) (3.0.4)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /home/arragon/.local/lib/python3.10/site-packages (from thinc<8.2.0,>=8.1.8->spacy<3.7.0,>=3.6.0->en-core-web-trf==3.6.1) (0.7.10)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /home/arragon/.local/lib/python3.10/site-packages (from thinc<8.2.0,>=8.1.8->spacy<3.7.0,>=3.6.0->en-core-web-trf==3.6.1) (0.1.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu11==2.14.3 in /home/arragon/.local/lib/python3.10/site-packages (from torch>=1.8.0->spacy-transformers<1.3.0,>=1.2.2->en-core-web-trf==3.6.1) (2.14.3)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in /home/arragon/.local/lib/python3.10/site-packages (from torch>=1.8.0->spacy-transformers<1.3.0,>=1.2.2->en-core-web-trf==3.6.1) (11.7.99)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in /home/arragon/.local/lib/python3.10/site-packages (from torch>=1.8.0->spacy-transformers<1.3.0,>=1.2.2->en-core-web-trf==3.6.1) (11.7.99)\n",
      "Requirement already satisfied: nvidia-cufft-cu11==10.9.0.58 in /home/arragon/.local/lib/python3.10/site-packages (from torch>=1.8.0->spacy-transformers<1.3.0,>=1.2.2->en-core-web-trf==3.6.1) (10.9.0.58)\n",
      "Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in /home/arragon/.local/lib/python3.10/site-packages (from torch>=1.8.0->spacy-transformers<1.3.0,>=1.2.2->en-core-web-trf==3.6.1) (11.10.3.66)\n",
      "Requirement already satisfied: nvidia-cusolver-cu11==11.4.0.1 in /home/arragon/.local/lib/python3.10/site-packages (from torch>=1.8.0->spacy-transformers<1.3.0,>=1.2.2->en-core-web-trf==3.6.1) (11.4.0.1)\n",
      "Requirement already satisfied: sympy in /home/arragon/.local/lib/python3.10/site-packages (from torch>=1.8.0->spacy-transformers<1.3.0,>=1.2.2->en-core-web-trf==3.6.1) (1.12)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu11==11.7.101 in /home/arragon/.local/lib/python3.10/site-packages (from torch>=1.8.0->spacy-transformers<1.3.0,>=1.2.2->en-core-web-trf==3.6.1) (11.7.101)\n",
      "Requirement already satisfied: nvidia-curand-cu11==10.2.10.91 in /home/arragon/.local/lib/python3.10/site-packages (from torch>=1.8.0->spacy-transformers<1.3.0,>=1.2.2->en-core-web-trf==3.6.1) (10.2.10.91)\n",
      "Requirement already satisfied: filelock in /home/arragon/.local/lib/python3.10/site-packages (from torch>=1.8.0->spacy-transformers<1.3.0,>=1.2.2->en-core-web-trf==3.6.1) (3.12.2)\n",
      "Requirement already satisfied: nvidia-nvtx-cu11==11.7.91 in /home/arragon/.local/lib/python3.10/site-packages (from torch>=1.8.0->spacy-transformers<1.3.0,>=1.2.2->en-core-web-trf==3.6.1) (11.7.91)\n",
      "Requirement already satisfied: triton==2.0.0 in /home/arragon/.local/lib/python3.10/site-packages (from torch>=1.8.0->spacy-transformers<1.3.0,>=1.2.2->en-core-web-trf==3.6.1) (2.0.0)\n",
      "Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in /home/arragon/.local/lib/python3.10/site-packages (from torch>=1.8.0->spacy-transformers<1.3.0,>=1.2.2->en-core-web-trf==3.6.1) (8.5.0.96)\n",
      "Requirement already satisfied: nvidia-cusparse-cu11==11.7.4.91 in /home/arragon/.local/lib/python3.10/site-packages (from torch>=1.8.0->spacy-transformers<1.3.0,>=1.2.2->en-core-web-trf==3.6.1) (11.7.4.91)\n",
      "Requirement already satisfied: networkx in /home/arragon/.local/lib/python3.10/site-packages (from torch>=1.8.0->spacy-transformers<1.3.0,>=1.2.2->en-core-web-trf==3.6.1) (3.1)\n",
      "Requirement already satisfied: wheel in /usr/lib/python3/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch>=1.8.0->spacy-transformers<1.3.0,>=1.2.2->en-core-web-trf==3.6.1) (0.37.1)\n",
      "Requirement already satisfied: lit in /home/arragon/.local/lib/python3.10/site-packages (from triton==2.0.0->torch>=1.8.0->spacy-transformers<1.3.0,>=1.2.2->en-core-web-trf==3.6.1) (16.0.6)\n",
      "Requirement already satisfied: cmake in /home/arragon/.local/lib/python3.10/site-packages (from triton==2.0.0->torch>=1.8.0->spacy-transformers<1.3.0,>=1.2.2->en-core-web-trf==3.6.1) (3.27.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/arragon/.local/lib/python3.10/site-packages (from transformers<4.31.0,>=3.4.0->spacy-transformers<1.3.0,>=1.2.2->en-core-web-trf==3.6.1) (2023.6.3)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/arragon/.local/lib/python3.10/site-packages (from transformers<4.31.0,>=3.4.0->spacy-transformers<1.3.0,>=1.2.2->en-core-web-trf==3.6.1) (5.1)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /home/arragon/.local/lib/python3.10/site-packages (from transformers<4.31.0,>=3.4.0->spacy-transformers<1.3.0,>=1.2.2->en-core-web-trf==3.6.1) (0.3.3)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /home/arragon/.local/lib/python3.10/site-packages (from transformers<4.31.0,>=3.4.0->spacy-transformers<1.3.0,>=1.2.2->en-core-web-trf==3.6.1) (0.16.4)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /home/arragon/.local/lib/python3.10/site-packages (from transformers<4.31.0,>=3.4.0->spacy-transformers<1.3.0,>=1.2.2->en-core-web-trf==3.6.1) (0.13.3)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /home/arragon/.local/lib/python3.10/site-packages (from typer<0.10.0,>=0.3.0->spacy<3.7.0,>=3.6.0->en-core-web-trf==3.6.1) (8.1.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/arragon/.local/lib/python3.10/site-packages (from jinja2->spacy<3.7.0,>=3.6.0->en-core-web-trf==3.6.1) (2.1.3)\n",
      "Requirement already satisfied: fsspec in /home/arragon/.local/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.14.1->transformers<4.31.0,>=3.4.0->spacy-transformers<1.3.0,>=1.2.2->en-core-web-trf==3.6.1) (2023.6.0)\n",
      "Requirement already satisfied: mpmath>=0.19 in /home/arragon/.local/lib/python3.10/site-packages (from sympy->torch>=1.8.0->spacy-transformers<1.3.0,>=1.2.2->en-core-web-trf==3.6.1) (1.3.0)\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_trf')\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "# Dependencies\n",
    "pip install docx2txt\n",
    "pip install gensim\n",
    "pip install keras\n",
    "pip install nltk\n",
    "pip install -U scikit-learn\n",
    "pip install python-docx\n",
    "pip install tensorflow\n",
    "pip install pandas\n",
    "pip install openpyxl\n",
    "pip install nltk\n",
    "pip install spacy\n",
    "\n",
    "# python3 -m spacy download en_core_web_sm\n",
    "python3 -m spacy download en_core_web_trf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.test.utils import common_texts\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "import pandas as pd\n",
    "import csv\n",
    "import math\n",
    "\n",
    "# import pprint\n",
    "import re\n",
    "import json\n",
    "\n",
    "import spacy\n",
    "\n",
    "# Use if en_core_web_sm or en_core_web_trf not installable via python3 in terminal:\n",
    "# spacy.cli.download(\"en_core_web_trf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Britishise\n",
    "\n",
    "A utility function which converts words to UK spelling\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialises dictionary containing American to UK spelling translations\n",
    "american_to_british_dict = {}\n",
    "american_to_british_path = \"American-British-English-Translator.json\"\n",
    "with open(american_to_british_path, \"r\") as file:\n",
    "    data = file.read()\n",
    "american_to_british_dict = json.loads(data)\n",
    "\n",
    "\n",
    "def britishise(sentence):\n",
    "    \"\"\"\n",
    "    Convert words in a sentence to UK spelling to ensure consistency\n",
    "\n",
    "    Input:\n",
    "        sentence: An array of strings\n",
    "\n",
    "    Output:\n",
    "        sentence: An array of strings, which have been converted to UK spelling\n",
    "    \"\"\"\n",
    "\n",
    "    for j in range(len(sentence)):\n",
    "        try:\n",
    "            sentence[j] = american_to_british_dict[sentence[j].lower()]\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Verb Classifier\n",
    "\n",
    "Start by generating a spreadsheet containing verbs to be classified. Use a separate file for each taxonomy.\n",
    "\n",
    "The spreadsheet should be in the following format: - The first row in each column should contain the header for that column. - Headers must be consistent across sheets within the same file - Subsequent cells in the column should contain a single verb - A separate sheet should be used for each individual source - Sheets should be arranged in order of priority from left to right.\n",
    "\n",
    "The use the verb_classifier() function to process the spreadsheet and load the verbs into memory.\n",
    "The output from verb_classifier() should then be stored in a dictionary where the key is the name of the taxonomy.\n",
    "E.g.\n",
    "mapped_verbs = {\n",
    "\"Cognitive\": verb_classifier(bloom_cognitive_file_path),\n",
    "\"Affective\": verb_classifier(bloom_affective_file_path),\n",
    "\"Psychomotor\": verb_classifier(bloom_psychomotor_file_path),\n",
    "\"SOLO\": verb_classifier(solo_file_path)\n",
    "}\n",
    "\n",
    "Next the check_duplicates() function should be used to check the Dataframes for duplicates.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "\n",
    "def verb_classifier(verbs_file_path):  #\n",
    "    \"\"\"\n",
    "    Takes an excel spreadsheet containing verbs, classifies the verbs and stores it into a Dataframe.\n",
    "\n",
    "\n",
    "    Inputs:\n",
    "        verbs_file_path: A string that contains the path to the excel spreadsheet to be read\n",
    "\n",
    "    Outputs:\n",
    "        domain_levels: A DataFrame which contains all the verbs from the spreadsheet classified into their respective levels\n",
    "    \"\"\"\n",
    "\n",
    "    xlsx = pd.ExcelFile(verbs_file_path, engine=\"openpyxl\")\n",
    "\n",
    "    sheet_names = xlsx.sheet_names  # Get a list of sheet names\n",
    "\n",
    "    # Create an empty dictionary to store DataFrames for each sheet\n",
    "    dfs = {}\n",
    "\n",
    "    for sheet_name in sheet_names:\n",
    "        df = xlsx.parse(sheet_name)  # For XLSX files\n",
    "\n",
    "        # Store the DataFrame in the dictionary\n",
    "        dfs[sheet_name] = df\n",
    "\n",
    "    domain_levels = pd.concat(dfs)\n",
    "    duplicate_checklist = []\n",
    "\n",
    "    # Iterate over all values in the spreadsheet\n",
    "    for i in range(domain_levels.shape[0]):\n",
    "        for j in range(domain_levels.shape[1]):\n",
    "            cell_value = domain_levels.iloc[i, j]\n",
    "            if not pd.isna(\n",
    "                cell_value\n",
    "            ):  # Format verbs (lower case, UK spelling, lemmatised format)\n",
    "                cell_value_lower = cell_value.lower()\n",
    "                verb_brit = britishise([cell_value_lower])[0]\n",
    "                verb = lemmatizer.lemmatize(verb_brit, pos=\"v\")\n",
    "\n",
    "                if (\n",
    "                    verb not in duplicate_checklist\n",
    "                ):  # Check if the verb is already mapped\n",
    "                    domain_levels.iloc[i, j] = verb\n",
    "                    duplicate_checklist.append(verb)\n",
    "                else:\n",
    "                    domain_levels.iloc[i, j] = float(\"nan\")\n",
    "\n",
    "    domain_levels = domain_levels.dropna(how=\"all\")\n",
    "    return domain_levels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths\n",
    "solo_file_path = \"SOLO.xlsx\"\n",
    "bloom_cognitive_file_path = \"Bloom_cognitive.xlsx\"\n",
    "bloom_psychomotor_file_path = \"Bloom_psychomotor.xlsx\"\n",
    "bloom_affective_file_path = \"Bloom_affective.xlsx\"\n",
    "\n",
    "# Verbs\n",
    "mapped_verbs = {\n",
    "    \"Cognitive\": verb_classifier(bloom_cognitive_file_path),\n",
    "    \"Affective\": verb_classifier(bloom_affective_file_path),\n",
    "    \"Psychomotor\": verb_classifier(bloom_psychomotor_file_path),\n",
    "    \"SOLO\": verb_classifier(solo_file_path),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No duplicates exist in Cognitive\n",
      "No duplicates exist in Affective\n",
      "No duplicates exist in Psychomotor\n",
      "No duplicates exist in SOLO\n"
     ]
    }
   ],
   "source": [
    "def check_duplicates(verbs_df, domain_name):\n",
    "    \"\"\"\n",
    "    Check the dataframe for duplicate verbs\n",
    "\n",
    "    verbs_df - The dataframe that consists of the classified verbs\n",
    "    domain_name - string name of the domain. (SOLO, Blooms Cognitive, Blooms Affective or Blooms Psychomotor)\n",
    "    \"\"\"\n",
    "\n",
    "    # Convert DF to list\n",
    "    list_of_lists = verbs_df.values.tolist()\n",
    "    merged_list = [item for sublist in list_of_lists for item in sublist]\n",
    "\n",
    "    # Remove nan\n",
    "    cleaned_list = list(filter(lambda x: not pd.isna(x), merged_list))\n",
    "\n",
    "    # Check for duplicate\n",
    "    if len(cleaned_list) != len(set(cleaned_list)):\n",
    "        print(\"Duplicates Found in \" + domain_name)\n",
    "        print(sorted(cleaned_list))\n",
    "    else:\n",
    "        print(\"No duplicates exist in \" + domain_name)\n",
    "\n",
    "    list_of_lists.clear()\n",
    "    merged_list.clear()\n",
    "    cleaned_list.clear()\n",
    "\n",
    "\n",
    "# Check for duplicates\n",
    "for taxonomy_key, taxonomy_item in mapped_verbs.items():\n",
    "    check_duplicates(taxonomy_item, taxonomy_key)\n",
    "    taxonomy_item.to_csv(\"./outputs/mapped_verbs_\" + taxonomy_key + \".csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                 Level\n",
      "Cognitive define           Remembering\n",
      "          classify       Understanding\n",
      "          choose              Applying\n",
      "          breakdown          Analysing\n",
      "          appraise          Evaluating\n",
      "...                                ...\n",
      "SOLO      validate   Extended Abstract\n",
      "          solve        Multistructural\n",
      "          perform           Relational\n",
      "          visualise  Extended Abstract\n",
      "          symbolise    Multistructural\n",
      "\n",
      "[730 rows x 1 columns]\n"
     ]
    }
   ],
   "source": [
    "# Convert mapped_verbs DataFrames which is used to find verbs at a certain level, to a 'dictionary' to lookup the level of a verb\n",
    "def generate_verb_list(mapped_verbs):\n",
    "    tp_arr = []\n",
    "    levels = []\n",
    "    for t_key, t_item in mapped_verbs.items():\n",
    "        columns = t_item.columns.values\n",
    "        for x in range(t_item.shape[0]):\n",
    "            for y in range(t_item.shape[1]):\n",
    "                verb = t_item.iloc[x, y]\n",
    "                if not pd.isna(verb):\n",
    "                    tp_arr.append((t_key, verb))\n",
    "                    levels.append(columns[y])\n",
    "\n",
    "    index = pd.MultiIndex.from_tuples(tp_arr)\n",
    "    verb_list = pd.DataFrame(levels, index=index, columns=[\"Level\"])\n",
    "    return verb_list\n",
    "\n",
    "\n",
    "verb_list = generate_verb_list(mapped_verbs)\n",
    "print(verb_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the English language model in spaCy\n",
    "# nlp = spacy.load('en_core_web_sm')\n",
    "nlp = spacy.load(\"en_core_web_trf\", exclude=[\"ner\"])\n",
    "\n",
    "\n",
    "## Function to identify verbs in a sentence\n",
    "def identify_verbs(sentence):\n",
    "    \"\"\"\n",
    "    Identify verbs within a sentence and lemmatise them (convert them into their base word)\n",
    "\n",
    "    Inputs:\n",
    "        sentence: A string\n",
    "\n",
    "    Outputs:\n",
    "        verbs: An array of strings representing identified verbs in their lemmatised form\n",
    "    \"\"\"\n",
    "\n",
    "    # Process the sentence using spaCy\n",
    "    doc = nlp(sentence)\n",
    "\n",
    "    # Extract the verbs from the processed sentence\n",
    "    verbs = [token.lemma_ for token in doc if token.pos_ == \"VERB\"]\n",
    "\n",
    "    return verbs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract Columns\n",
    "\n",
    "A utility function to extract LOs (sentences) and the corresponding levels for each taxonomy the LO is mapped to\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_columns(file_path, columns):\n",
    "    extracted_data = {}\n",
    "\n",
    "    with open(file_path, \"r\") as file:\n",
    "        reader = csv.reader(file)\n",
    "        headers = next(reader)  # Read the headers\n",
    "\n",
    "        # Check if all specified columns exist in the CSV file\n",
    "        for column in columns:\n",
    "            if column not in headers:\n",
    "                raise ValueError(f\"Column '{column}' not found in the CSV file.\")\n",
    "\n",
    "        # Initialize separate arrays for each column\n",
    "        for column in columns:\n",
    "            extracted_data[column] = []\n",
    "\n",
    "        # Extract data from specified columns\n",
    "        for row in reader:\n",
    "            for column in columns:\n",
    "                column_index = headers.index(column)\n",
    "                extracted_data[column].append(row[column_index])\n",
    "\n",
    "    return extracted_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "csv_file = \"Learning outcomes manual mapping - Mappings.csv\"\n",
    "# csv_file = 'Learning outcomes manual mapping - Mappings - Testing.csv'\n",
    "columns_to_extract = [\"LO\", \"Cognitive\", \"Affective\", \"Psychomotor\", \"SOLO\"]\n",
    "\n",
    "extracted_data = extract_columns(csv_file, columns_to_extract)\n",
    "\n",
    "sentences = extracted_data[\"LO\"]\n",
    "final_levels = {\n",
    "    \"Cognitive\": extracted_data[\"Cognitive\"],\n",
    "    \"Affective\": extracted_data[\"Affective\"],\n",
    "    \"Psychomotor\": extracted_data[\"Psychomotor\"],\n",
    "    \"SOLO\": extracted_data[\"SOLO\"],\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model\n",
    "\n",
    "The following two code cells contain methods for generating word vectors. The first method involves processing sentences and training a Word2Vec model. The second uses pre-trained word vectors. We found that because the pre-trained word vectors aren't embedded with the context of the learning outcomes it wasn't as accurate.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method 1: Word2Vec Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\musth\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\musth\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\musth\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "## Main piece of code that performs the mapping - Approach 2 - 12% accuracy\n",
    "\n",
    "# wiki word vectors no uppercase\n",
    "# TODO: Modify this section of the code to use the bloom level verbs from Arragon's spreadsheet, will also need to modify\n",
    "# Ideas for improving accuracy\n",
    "#### Reduce the number of verbs.\n",
    "#### Take more learning outcomes from the monash handbook website(need big dataset for this part) and identify verbs that are appearing multiple times\n",
    "#### The nummber of times that it appears could be set to a certain number ex: 5.\n",
    "#### If the verb doesnt appear atleast 5 times, we could remove the verb from our list of predefined verbs which will result in a shorter verb list\n",
    "\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"stopwords\")\n",
    "nltk.download(\"wordnet\")\n",
    "\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "# Array of all the PLOs and ULOs (We can couple them together as we're trying to identify Bloom/Solo level here)\n",
    "lo_sentence_array = []\n",
    "\n",
    "# TODO: train CLO classification with all data instead of just one course.\n",
    "for sentence in sentences:\n",
    "    sentence = re.sub(r\"[^\\w\\s]\", \"\", sentence)\n",
    "    # Process the sentence using spaCy\n",
    "    doc = nlp(sentence)\n",
    "\n",
    "    # Extract the verbs from the processed sentence\n",
    "    cleaned_tokens = [token.lemma_ for token in doc if token.lower_ not in stop_words]\n",
    "    cleaned_tokens = britishise(cleaned_tokens)\n",
    "    lo_sentence_array.append(cleaned_tokens)\n",
    "\n",
    "# build the vocabulary and train the model\n",
    "# IMPORTANT, N0TE THAT sg=1 flag specifies Word2Vec to use the Skip Gram Model as designated by the LSTM paper.\n",
    "model = Word2Vec(\n",
    "    sentences=lo_sentence_array,\n",
    "    vector_size=100,\n",
    "    window=5,\n",
    "    min_count=1,\n",
    "    workers=4,\n",
    "    sg=1,\n",
    "    epochs=30,\n",
    ")\n",
    "\n",
    "model.build_vocab(corpus_iterable=verb_list, update=True)\n",
    "model.update_weights()\n",
    "\n",
    "model_1_wv = model.wv\n",
    "# train the model with the course's ULOs and PLOs.\n",
    "# model.train([tokens], total_examples=len([tokens]), epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method 2: Pre-trained Word Vectors\n",
    "\n",
    "As this method was only used for accuracy comparison, it has been commented out to save computation time.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Download the vector file from https://fasttext.cc/docs/en/english-vectors.html (first file on the website), unzip the file and store in your local development folder\n",
    "\n",
    "# # Path to the downloaded .vec file\n",
    "# path_to_vectors = './wiki-news-300d-1M.vec'\n",
    "# path_to_vectors = 'wiki.en.vec'\n",
    "\n",
    "# # Load the word vectors\n",
    "# model_2_wv = KeyedVectors.load_word2vec_format(path_to_vectors)\n",
    "\n",
    "# # Accuracy when using the KeyedVectors\n",
    "# # Cognitive:  48 %            Affective:  32 %\n",
    "# # Psychomotor:  62 %          SOLO:  64 %\n",
    "# # Total Accuracy:  51 %\n",
    "\n",
    "# # Find similar words\n",
    "# similar_words = model_2_wv.most_similar('cat')\n",
    "\n",
    "# # Calculate word similarity\n",
    "# similarity = model_2_wv.similarity('cat', 'dog')\n",
    "\n",
    "# # Perform vector arithmetic\n",
    "# result = model_2_wv['king'] - model_2_wv['man'] + model_2_wv['woman']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LO Mapping\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def learning_outcome_mapping(\n",
    "    sentences, final_levels, SIM_THRESHOLD, SUGGESTED_SIM_THRESHOLD\n",
    "):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "        sentences: An array of Learning Outcomes (sentences) in string format.\n",
    "        final levels: An dictionary of arrays. The dictionary keys are the taxonomies and the arrays contain strings representing the final mapped level of the corresponding learning outcome. If LO is not mapped to that domain leave null value\n",
    "    \"\"\"\n",
    "    passed_mappings = {\"Cognitive\": 0, \"Affective\": 0, \"Psychomotor\": 0, \"SOLO\": 0}\n",
    "    failed_mappings = {\"Cognitive\": 0, \"Affective\": 0, \"Psychomotor\": 0, \"SOLO\": 0}\n",
    "\n",
    "    failed_cases = pd.DataFrame(\n",
    "        columns=[\n",
    "            \"Sentence\",\n",
    "            \"Domain\",\n",
    "            \"Manual Level\",\n",
    "            \"Auto Level\",\n",
    "            \"Verbs Identified\",\n",
    "            \"Suggested Verbs\",\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    for i in range(len(sentences)):  # Iterates over the LOs\n",
    "        sentences[i] = britishise(sentences[i])\n",
    "\n",
    "        # if final_levels[t_key][i] is None or not final_levels[t_key][i] or final_levels[t_key][i] == '-': continue\n",
    "\n",
    "        identified_verbs = identify_verbs(sentences[i])\n",
    "        similar_verbs = {}\n",
    "\n",
    "        score_list = {\n",
    "            \"Cognitive\": {\n",
    "                \"Remembering\": 0,\n",
    "                \"Understanding\": 0,\n",
    "                \"Applying\": 0,\n",
    "                \"Analysing\": 0,\n",
    "                \"Evaluating\": 0,\n",
    "                \"Creating\": 0,\n",
    "            },\n",
    "            \"Affective\": {\n",
    "                \"Receiving\": 0,\n",
    "                \"Responding\": 0,\n",
    "                \"Valuing\": 0,\n",
    "                \"Organisation\": 0,\n",
    "                \"Characterisation\": 0,\n",
    "            },\n",
    "            \"Psychomotor\": {\n",
    "                \"Perception\": 0,\n",
    "                \"Set\": 0,\n",
    "                \"Guided Response\": 0,\n",
    "                \"Mechanism\": 0,\n",
    "                \"Complex Overt Response\": 0,\n",
    "                \"Adaptation\": 0,\n",
    "                \"Origination\": 0,\n",
    "            },\n",
    "            \"SOLO\": {\n",
    "                \"Prestructural\": 0,\n",
    "                \"Unistructural\": 0,\n",
    "                \"Multistructural\": 0,\n",
    "                \"Relational\": 0,\n",
    "                \"Extended Abstract\": 0,\n",
    "            },\n",
    "        }\n",
    "\n",
    "        for taxonomy_key, taxonomy_item in mapped_verbs.items():\n",
    "            similar_verbs[taxonomy_key] = pd.DataFrame(columns=[\"Level\", \"Similarity\"])\n",
    "\n",
    "            for identified_verb in identified_verbs:\n",
    "                for k in range(taxonomy_item.shape[1]):  # Col (Level)\n",
    "                    for j in range(taxonomy_item.shape[0]):  # Row\n",
    "                        verb = taxonomy_item.iloc[j, k]\n",
    "\n",
    "                        if verb is None or not verb or pd.isna(verb):\n",
    "                            continue\n",
    "\n",
    "                        similarity_score = 0\n",
    "                        try:  # Currently some of the 'verbs' identified are phrases rather than words and it was throwing errors so this is a temp solution\n",
    "                            sim_score = word_vectors.similarity(identified_verb, verb)\n",
    "                            if (\n",
    "                                sim_score >= SUGGESTED_SIM_THRESHOLD\n",
    "                                and identified_verb != verb\n",
    "                            ):\n",
    "                                similar_verbs[taxonomy_key].at[\n",
    "                                    verb, \"Level\"\n",
    "                                ] = taxonomy_item.columns[k]\n",
    "                                similar_verbs[taxonomy_key].at[\n",
    "                                    verb, \"Similarity\"\n",
    "                                ] = sim_score\n",
    "                            if sim_score >= SIM_THRESHOLD:\n",
    "                                similarity_score += sim_score\n",
    "                        except:\n",
    "                            pass\n",
    "                        score_list[taxonomy_key][\n",
    "                            taxonomy_item.columns[k]\n",
    "                        ] += similarity_score\n",
    "\n",
    "        # Identify level based on similarity\n",
    "        max_score = {\n",
    "            \"Cognitive\": {\"Level\": None, \"Score\": 0},\n",
    "            \"Affective\": {\"Level\": None, \"Score\": 0},\n",
    "            \"Psychomotor\": {\"Level\": None, \"Score\": 0},\n",
    "            \"SOLO\": {\"Level\": None, \"Score\": 0},\n",
    "        }\n",
    "        for t_key, t_item in score_list.items():\n",
    "            for l in t_item:\n",
    "                if max_score[t_key][\"Score\"] < score_list[t_key][l]:\n",
    "                    max_score[t_key] = {\"Level\": l, \"Score\": score_list[t_key][l]}\n",
    "\n",
    "        # Determine if case passed or failed\n",
    "        for t_key, t_item in final_levels.items():\n",
    "            if (\n",
    "                final_levels[t_key][i] is None\n",
    "                or not final_levels[t_key][i]\n",
    "                or final_levels[t_key][i] == \"-\"\n",
    "            ):\n",
    "                continue\n",
    "\n",
    "            if (\n",
    "                max_score[t_key][\"Level\"] != None\n",
    "                and final_levels[t_key][i].lower() == max_score[t_key][\"Level\"].lower()\n",
    "            ):\n",
    "                passed_mappings[t_key] += 1\n",
    "            else:\n",
    "                failed_mappings[t_key] += 1\n",
    "\n",
    "                x = failed_cases.shape[0]\n",
    "\n",
    "                failed_cases.at[x, \"Sentence\"] = sentences[i]\n",
    "                failed_cases.at[x, \"Domain\"] = t_key\n",
    "                failed_cases.at[x, \"Manual Level\"] = final_levels[t_key][i]\n",
    "                failed_cases.at[x, \"Auto Level\"] = max_score[t_key][\"Level\"]\n",
    "\n",
    "                # Generate verb and level tuples for sentence data\n",
    "                s_d_identified_verbs = []\n",
    "                for verb in identified_verbs:\n",
    "                    level = \"Verb not mapped\"\n",
    "                    try:\n",
    "                        level = verb_list.at[(t_key, verb), \"Level\"]\n",
    "                    except:\n",
    "                        pass\n",
    "                    finally:\n",
    "                        s_d_identified_verbs.append((verb, level))\n",
    "\n",
    "                failed_cases.at[x, \"Verbs Identified\"] = s_d_identified_verbs\n",
    "\n",
    "                # Generate suggested verbs\n",
    "                sim_verbs = (\n",
    "                    similar_verbs[t_key]\n",
    "                    .sort_values(by=[\"Similarity\"], ascending=False)\n",
    "                    .head(5)\n",
    "                )\n",
    "                suggested_verbs = []\n",
    "                if sim_verbs.shape[0] > 0:\n",
    "                    suggested_verbs = [\n",
    "                        (verb, sim_verbs.at[verb, \"Level\"]) for verb in sim_verbs.index\n",
    "                    ]\n",
    "                failed_cases.at[x, \"Suggested Verbs\"] = suggested_verbs\n",
    "\n",
    "    # Export Failed Cases\n",
    "    # print(failed_cases)\n",
    "    with pd.ExcelWriter(\"./outputs/failed_lo_mappings.xlsx\") as writer:\n",
    "        failed_cases.to_excel(\n",
    "            writer,\n",
    "            sheet_name=\"Mappings\",\n",
    "            columns=[\n",
    "                \"Sentence\",\n",
    "                \"Domain\",\n",
    "                \"Manual Level\",\n",
    "                \"Auto Level\",\n",
    "                \"Verbs Identified\",\n",
    "                \"Suggested Verbs\",\n",
    "            ],\n",
    "        )\n",
    "\n",
    "    # Calculate Accuracy\n",
    "    total_passed = 0\n",
    "    total_failed = 0\n",
    "    for taxonomy, passed in passed_mappings.items():\n",
    "        total_passed += passed\n",
    "        total_failed += failed_mappings[taxonomy]\n",
    "        tot = (\n",
    "            passed + failed_mappings[taxonomy]\n",
    "            if passed + failed_mappings[taxonomy] > 0\n",
    "            else 1\n",
    "        )\n",
    "        mapping_percentage = math.ceil((passed / (tot)) * 100)\n",
    "        print(\"Percentage of \", taxonomy, \" mappings passed: \", mapping_percentage, \"%\")\n",
    "\n",
    "    total_mapping_percentage = math.ceil(\n",
    "        (total_passed / (total_passed + total_failed)) * 100\n",
    "    )\n",
    "    print(\"Total percentage of mappings passed: \", total_mapping_percentage, \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing different similarity thresholds to determine accuracy\n",
    "\n",
    "# As the main mapping code compares verbs identified in an LO to a complete list of verbs\n",
    "# and uses their similarity to determine the overall level of the LO, it was theorised\n",
    "# that verbs which have low similarity could impact the overall accuracy.\n",
    "\n",
    "# Threshold should be between 0 and 1, i.e. 0% and 100% similarity\n",
    "\n",
    "# sim_thresholds_arr = [0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1]\n",
    "# # Similarity thresholds from 0 to 0.9 all have the same accuracy, thus for following\n",
    "# # iterations they have been excluded to reduce run time.\n",
    "# # Cognitive:  27 %            Affective:  23 %\n",
    "# # Psychomotor:  43 %          SOLO:  52 %\n",
    "# # Total Accuracy:  36 %\n",
    "# # Similarity threshold 1 improves the accuracy to:\n",
    "# # Cognitive:  38 %            Affective:  29 %\n",
    "# # Psychomotor:  59 %          SOLO:  54 %\n",
    "# # Total Accuracy:  44 %\n",
    "\n",
    "\n",
    "# sim_thresholds_arr = [0.9, 0.92, 0.94, 0.96, 0.98, 1]\n",
    "# # Similarity threshold 0.98 sees a slight decrease in accuracy:\n",
    "# # Cognitive:  27 %            Affective:  22 %\n",
    "# # Psychomotor:  43 %          SOLO:  52 %\n",
    "# # Total Accuracy:  36 %\n",
    "\n",
    "\n",
    "# sim_thresholds_arr = [0.98, 0.985, 0.99, 0.995, 1]\n",
    "# # Similarity threshold 0.99 sees a slight increase in accuracy:\n",
    "# # Cognitive:  29 %            Affective:  22 %\n",
    "# # Psychomotor:  44 %          SOLO:  52 %\n",
    "# # Total Accuracy:  36 %\n",
    "# # Similarity threshold 0.995 sees a major increase in accuracy for most domains,\n",
    "# # but a decrease for Psychomotor:\n",
    "# # Cognitive:  43 %            Affective:  38 %\n",
    "# # Psychomotor:  37 %          SOLO:  52 %\n",
    "# # Total Accuracy:  43 %\n",
    "\n",
    "# sim_thresholds_arr = [0.995, 0.996, 0.997, 0.998, 0.999, 1]\n",
    "# # Similarity threshold 0.996 sees a major increase for Pychomotor in accuracy:\n",
    "# # Cognitive:  42 %            Affective:  38 %\n",
    "# # Psychomotor:  61 %          SOLO:  58 %\n",
    "# # Total Accuracy:  49 %\n",
    "\n",
    "# # Similarity thresholds 0.96, 0.997, 0.998, 0.999 only vary in accuracy by 1%\n",
    "# # in different domains. As such 0.997 was chosen as the similarity threshold.\n",
    "\n",
    "# suggested_sim_threshold = 0.985\n",
    "# # The suggested similarity threshold is used for generating suggested verbs.\n",
    "# # This threshold is set lower than the similarity threshold by ~ 0.01 to\n",
    "# # ensure that enough similar verbs can be found.\n",
    "\n",
    "# for sim_thresh in sim_thresholds_arr:\n",
    "#     print(\"Similarity Threshold: \", sim_thresh)\n",
    "#     learning_outcome_mapping(sentences, final_levels, sim_thresh, suggested_sim_threshold)\n",
    "#     print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Method 1\n",
      "Percentage of  Cognitive  mappings passed:  48 %\n",
      "Percentage of  Affective  mappings passed:  36 %\n",
      "Percentage of  Psychomotor  mappings passed:  62 %\n",
      "Percentage of  SOLO  mappings passed:  64 %\n",
      "Total percentage of mappings passed:  52 %\n"
     ]
    }
   ],
   "source": [
    "# Thresholds to filter the similarity of words to improve accuracy\n",
    "suggested_sim_threshold = 0.985\n",
    "sim_threshold = 0.997\n",
    "\n",
    "# Verbs\n",
    "mapped_verbs = {\n",
    "    \"Cognitive\": verb_classifier(bloom_cognitive_file_path),\n",
    "    \"Affective\": verb_classifier(bloom_affective_file_path),\n",
    "    \"Psychomotor\": verb_classifier(bloom_psychomotor_file_path),\n",
    "    \"SOLO\": verb_classifier(solo_file_path),\n",
    "}\n",
    "\n",
    "verb_list = generate_verb_list(mapped_verbs)\n",
    "\n",
    "\n",
    "print(\"Method 1\")\n",
    "word_vectors = model_1_wv\n",
    "learning_outcome_mapping(\n",
    "    sentences, final_levels, sim_threshold, suggested_sim_threshold\n",
    ")\n",
    "\n",
    "# print(\"\\nMethod 2\")\n",
    "# word_vectors = model_2_wv\n",
    "# learning_outcome_mapping(sentences, final_levels, sim_threshold, suggested_sim_threshold)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_keys_by_value(dictionary, target_value):\n",
    "    key_value = \"\"\n",
    "    for key, value in dictionary.items():\n",
    "        if value == target_value:\n",
    "            key_value = key\n",
    "\n",
    "    if key_value == \"\":\n",
    "        key_value = list(dictionary.keys())[-1]\n",
    "    return key_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "solo = mapped_verbs[\"SOLO\"]\n",
    "\n",
    "## Rankings are manually derived using the followinng sources\n",
    "## https://davenport.libguides.com/learningoutcomes/domains#:~:text=Bloom%20identified%20three%20domains%2C%20or,Psychomotor%20Skills%20or%20Physical%20Skills\n",
    "## https://www.vectorsolutions.com/resources/blogs/teaching-skills-the-psychomotor-domain-of-learning-and-learning-objectives/\n",
    "\n",
    "# Predefined domain rankings\n",
    "solo_levels_ranks = {\n",
    "    \"Prestructural\": 0,\n",
    "    \"Unistructural\": 1,\n",
    "    \"Multistructural\": 2,\n",
    "    \"Relational\": 3,\n",
    "    \"Extended Abstract\": 4,\n",
    "}\n",
    "\n",
    "bloom_cognitive_levels_ranks = {\n",
    "    \"Remembering\": 0,\n",
    "    \"Understanding\": 0,\n",
    "    \"Applying\": 1,\n",
    "    \"Analysing\": 2,\n",
    "    \"Evaluating\": 3,\n",
    "    \"Creating\": 4,\n",
    "}\n",
    "\n",
    "bloom_affective_levels_ranks = {\n",
    "    \"Receiving\": 0,\n",
    "    \"Responding\": 1,\n",
    "    \"Valuing\": 2,\n",
    "    \"Organisation\": 3,\n",
    "    \"Characterisation\": 4,\n",
    "}\n",
    "\n",
    "bloom_psychomotor_levels_ranks = {\n",
    "    \"Perception\": 0,\n",
    "    \"Set\": 0,\n",
    "    \"Guided Response\": 1,\n",
    "    \"Mechanism\": 2,\n",
    "    \"Complex Overt Response\": 3,\n",
    "    \"Adaptation\": 3,\n",
    "    \"Origination\": 4,\n",
    "}\n",
    "\n",
    "global solo_copy\n",
    "solo_copy = solo.copy(deep=True)\n",
    "solo_copy.reset_index(drop=True, inplace=True)\n",
    "\n",
    "global bloom_cognitive_copy\n",
    "bloom_cognitive = mapped_verbs[\"Cognitive\"]\n",
    "bloom_cognitive_copy = bloom_cognitive.copy(deep=True)\n",
    "bloom_cognitive_copy.reset_index(drop=True, inplace=True)\n",
    "\n",
    "global bloom_affective_copy\n",
    "bloom_affective = mapped_verbs[\"Affective\"]\n",
    "bloom_affective_copy = bloom_affective.copy(deep=True)\n",
    "bloom_affective_copy.reset_index(drop=True, inplace=True)\n",
    "\n",
    "global bloom_psychomotor_copy\n",
    "bloom_psychomotor = mapped_verbs[\"Psychomotor\"]\n",
    "bloom_psychomotor_copy = bloom_psychomotor.copy(deep=True)\n",
    "bloom_psychomotor_copy.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# bloom_cognitive_copy = bloom_cognitive.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fail\n",
      "fail\n",
      "fail\n",
      "fail\n"
     ]
    }
   ],
   "source": [
    "# Identify new verb rankings\n",
    "def verb_ranking_identifier(\n",
    "    current_taxonomy,  # Current taxonomy being compared\n",
    "    comparison_taxonomy,  # Taxonomy being compared with\n",
    "    verb,  # Curret Verb\n",
    "    current_taxonomy_ranking_table,  # Ranking table for current taxonomy\n",
    "    comparison_taxonomy_ranking_table,  # Ranking table for comparison taxonomy\n",
    "):\n",
    "    if verb in comparison_taxonomy.values:\n",
    "        current_taxonomy_level = (\n",
    "            current_taxonomy[current_taxonomy == verb].stack().index[0][1]\n",
    "        )\n",
    "        comparison_taxonomy_level = (\n",
    "            comparison_taxonomy[comparison_taxonomy == verb].stack().index[0][1]\n",
    "        )\n",
    "\n",
    "        current_taxonomy_rank = current_taxonomy_ranking_table[current_taxonomy_level]\n",
    "        comparison_taxonomy_rank = comparison_taxonomy_ranking_table[\n",
    "            comparison_taxonomy_level\n",
    "        ]\n",
    "\n",
    "        ranking_range = current_taxonomy_rank - comparison_taxonomy_rank\n",
    "\n",
    "        # Check ranking range of the same verb in the two taxonomies\n",
    "        if abs(ranking_range) > 1:\n",
    "            new_current_taxonomy_rank = (\n",
    "                current_taxonomy_rank + comparison_taxonomy_rank\n",
    "            ) // 2\n",
    "            new_current_taxonomy_level = get_keys_by_value(\n",
    "                current_taxonomy_ranking_table, new_current_taxonomy_rank\n",
    "            )\n",
    "            new_comparison_taxonomy_level = get_keys_by_value(\n",
    "                comparison_taxonomy_ranking_table, new_current_taxonomy_rank\n",
    "            )\n",
    "            # Place the verb levels of the current taxonnomy and comparison taxonomy details into the dictionary\n",
    "            current_taxonomy_dict = {verb: new_current_taxonomy_level}\n",
    "            comparison_taxonomy_dict = {verb: new_comparison_taxonomy_level}\n",
    "            current_taxonomy_modifications.append(current_taxonomy_dict)\n",
    "            comparison_taxonomy_modifications.append(comparison_taxonomy_dict)\n",
    "\n",
    "\n",
    "def taxonomy_remapping(\n",
    "    current_taxonomy,\n",
    "    comparison_taxonomy1,\n",
    "    comparison_taxonomy2,\n",
    "    comparison_taxonomy3,\n",
    "    current_taxonomy_rank,\n",
    "    comparison_taxonomy1_rank,\n",
    "    comparison_taxonomy2_rank,\n",
    "    comparison_taxonomy3_rank,\n",
    "):\n",
    "    global current_taxonomy_modifications\n",
    "    current_taxonomy_modifications = []\n",
    "    global comparison_taxonomy_modifications\n",
    "    comparison_taxonomy_modifications = []\n",
    "\n",
    "    # Check for solo verbs against other taxonomies. Identify new levels to put verbs into\n",
    "    for i in range(current_taxonomy.shape[0]):  # Level\n",
    "        for j in range(current_taxonomy.shape[1]):  # Verb\n",
    "            verb = current_taxonomy.iloc[i, j]\n",
    "\n",
    "            if verb is None or not verb or pd.isna(verb):\n",
    "                continue\n",
    "\n",
    "            verb_ranking_identifier(\n",
    "                current_taxonomy,\n",
    "                comparison_taxonomy1,\n",
    "                verb,\n",
    "                current_taxonomy_rank,\n",
    "                comparison_taxonomy1_rank,\n",
    "            )\n",
    "            verb_ranking_identifier(\n",
    "                current_taxonomy,\n",
    "                comparison_taxonomy2,\n",
    "                verb,\n",
    "                current_taxonomy_rank,\n",
    "                comparison_taxonomy2_rank,\n",
    "            )\n",
    "\n",
    "            verb_ranking_identifier(\n",
    "                current_taxonomy,\n",
    "                comparison_taxonomy3,\n",
    "                verb,\n",
    "                current_taxonomy_rank,\n",
    "                comparison_taxonomy3_rank,\n",
    "            )\n",
    "\n",
    "    # Perform the modifications based on the ranking ranges idetified in the verb ranking identifier\n",
    "    for i in range(len(current_taxonomy_modifications)):\n",
    "        current_item = current_taxonomy_modifications[i]\n",
    "        verb = list(current_item.keys())[0]\n",
    "        level = list(current_item.values())[0]\n",
    "        row, col = current_taxonomy[current_taxonomy == verb].stack().index[0]\n",
    "\n",
    "        if verb == \"explain\":\n",
    "            print(\"fail\")\n",
    "        current_taxonomy.at[row, col] = None\n",
    "        temp_df = pd.DataFrame({level: verb}, index=[0])\n",
    "        current_taxonomy = pd.concat([current_taxonomy, temp_df], ignore_index=True)\n",
    "\n",
    "    for i in range(len(comparison_taxonomy_modifications)):\n",
    "        current_item = comparison_taxonomy_modifications[i]\n",
    "        verb = list(current_item.keys())[0]\n",
    "        level = list(current_item.values())[0]\n",
    "        temp_df = pd.DataFrame({level: verb}, index=[0])\n",
    "        if verb == \"explain\":\n",
    "            print(\"fail\")\n",
    "\n",
    "        if level in comparison_taxonomy1.columns:\n",
    "            row, col = (\n",
    "                comparison_taxonomy1[comparison_taxonomy1 == verb].stack().index[0]\n",
    "            )\n",
    "            comparison_taxonomy1.at[row, col] = None\n",
    "            comparison_taxonomy1 = pd.concat(\n",
    "                [comparison_taxonomy1, temp_df], ignore_index=True\n",
    "            )\n",
    "        elif level in comparison_taxonomy2.columns:\n",
    "            row, col = (\n",
    "                comparison_taxonomy2[comparison_taxonomy2 == verb].stack().index[0]\n",
    "            )\n",
    "            comparison_taxonomy2.at[row, col] = None\n",
    "            comparison_taxonomy2 = pd.concat(\n",
    "                [comparison_taxonomy2, temp_df], ignore_index=True\n",
    "            )\n",
    "        elif level in comparison_taxonomy3.columns:\n",
    "            row, col = (\n",
    "                comparison_taxonomy3[comparison_taxonomy3 == verb].stack().index[0]\n",
    "            )\n",
    "            comparison_taxonomy3.at[row, col] = None\n",
    "            comparison_taxonomy3 = pd.concat(\n",
    "                [comparison_taxonomy3, temp_df], ignore_index=True\n",
    "            )\n",
    "        else:\n",
    "            print(level)\n",
    "            print(\"level doesnt exist\")\n",
    "\n",
    "    return (\n",
    "        current_taxonomy,\n",
    "        comparison_taxonomy1,\n",
    "        comparison_taxonomy2,\n",
    "        comparison_taxonomy3,\n",
    "    )\n",
    "\n",
    "\n",
    "# Bloom remapping\n",
    "taxonomies = taxonomy_remapping(\n",
    "    bloom_cognitive_copy,\n",
    "    bloom_affective_copy,\n",
    "    bloom_psychomotor_copy,\n",
    "    solo_copy,\n",
    "    bloom_cognitive_levels_ranks,\n",
    "    bloom_affective_levels_ranks,\n",
    "    bloom_psychomotor_levels_ranks,\n",
    "    solo_levels_ranks,\n",
    ")\n",
    "bloom_cognitive_copy = taxonomies[0]\n",
    "bloom_affective_copy = taxonomies[1]\n",
    "bloom_psychomotor_copy = taxonomies[2]\n",
    "solo_copy = taxonomies[3]\n",
    "\n",
    "# Affective domain mapping\n",
    "taxonomies = taxonomy_remapping(\n",
    "    bloom_affective_copy,\n",
    "    bloom_cognitive_copy,\n",
    "    bloom_psychomotor_copy,\n",
    "    solo_copy,\n",
    "    bloom_affective_levels_ranks,\n",
    "    bloom_cognitive_levels_ranks,\n",
    "    bloom_psychomotor_levels_ranks,\n",
    "    solo_levels_ranks,\n",
    ")\n",
    "bloom_affective_copy = taxonomies[0]\n",
    "bloom_cognitive_copy = taxonomies[1]\n",
    "bloom_psychomotor_copy = taxonomies[2]\n",
    "solo_copy = taxonomies[3]\n",
    "\n",
    "# Psychomotor domain remapping\n",
    "taxonomies = taxonomy_remapping(\n",
    "    bloom_psychomotor_copy,\n",
    "    bloom_affective_copy,\n",
    "    bloom_cognitive_copy,\n",
    "    solo_copy,\n",
    "    bloom_psychomotor_levels_ranks,\n",
    "    bloom_affective_levels_ranks,\n",
    "    bloom_cognitive_levels_ranks,\n",
    "    solo_levels_ranks,\n",
    ")\n",
    "bloom_psychomotor_copy = taxonomies[0]\n",
    "bloom_affective_copy = taxonomies[1]\n",
    "bloom_cognitive_copy = taxonomies[2]\n",
    "solo_copy = taxonomies[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A tester funnction for checking verb ranking differences after performing the mapping\n",
    "def check_verb_rankings(\n",
    "    current_taxonomy,\n",
    "    comparison_taxonomy,\n",
    "    verb,\n",
    "    current_taxonomy_ranking_table,\n",
    "    comparison_taxonomy_ranking_table,\n",
    "):\n",
    "    if verb in comparison_taxonomy.values:\n",
    "        current_taxonomy_level = (\n",
    "            current_taxonomy[current_taxonomy == verb].stack().index[0][1]\n",
    "        )\n",
    "        comparison_taxonomy_level = (\n",
    "            comparison_taxonomy[comparison_taxonomy == verb].stack().index[0][1]\n",
    "        )\n",
    "\n",
    "        current_taxonomy_rank = current_taxonomy_ranking_table[current_taxonomy_level]\n",
    "        comparison_taxonomy_rank = comparison_taxonomy_ranking_table[\n",
    "            comparison_taxonomy_level\n",
    "        ]\n",
    "\n",
    "        ranking_range = current_taxonomy_rank - comparison_taxonomy_rank\n",
    "\n",
    "        if abs(ranking_range) > 1:\n",
    "            print(\n",
    "                verb\n",
    "                + \"-\"\n",
    "                + current_taxonomy_level\n",
    "                + \"-\"\n",
    "                + str(current_taxonomy_rank)\n",
    "                + \"-\"\n",
    "                + str(comparison_taxonomy_rank)\n",
    "                + \"-\"\n",
    "                + comparison_taxonomy_level\n",
    "            )\n",
    "\n",
    "\n",
    "def taxonomy_rank_tester(\n",
    "    current_taxonomy,\n",
    "    comparison_taxonomy1,\n",
    "    comparison_taxonomy2,\n",
    "    comparison_taxonomy3,\n",
    "    current_taxonomy_rank,\n",
    "    comparison_taxonomy1_rank,\n",
    "    comparison_taxonomy2_rank,\n",
    "    comparison_taxonomy3_rank,\n",
    "):\n",
    "    for i in range(current_taxonomy.shape[0]):  # Level\n",
    "        for j in range(current_taxonomy.shape[1]):  # Verb\n",
    "            verb = current_taxonomy.iloc[i, j]\n",
    "\n",
    "            if verb is None or not verb or pd.isna(verb):\n",
    "                continue\n",
    "\n",
    "            check_verb_rankings(\n",
    "                current_taxonomy,\n",
    "                comparison_taxonomy1,\n",
    "                verb,\n",
    "                current_taxonomy_rank,\n",
    "                comparison_taxonomy1_rank,\n",
    "            )\n",
    "            check_verb_rankings(\n",
    "                current_taxonomy,\n",
    "                comparison_taxonomy2,\n",
    "                verb,\n",
    "                current_taxonomy_rank,\n",
    "                comparison_taxonomy2_rank,\n",
    "            )\n",
    "\n",
    "            check_verb_rankings(\n",
    "                current_taxonomy,\n",
    "                comparison_taxonomy3,\n",
    "                verb,\n",
    "                current_taxonomy_rank,\n",
    "                comparison_taxonomy3_rank,\n",
    "            )\n",
    "\n",
    "\n",
    "# Test Bloom cognitive against other domains\n",
    "taxonomy_rank_tester(\n",
    "    bloom_cognitive_copy,\n",
    "    bloom_affective_copy,\n",
    "    bloom_psychomotor_copy,\n",
    "    solo_copy,\n",
    "    bloom_cognitive_levels_ranks,\n",
    "    bloom_affective_levels_ranks,\n",
    "    bloom_psychomotor_levels_ranks,\n",
    "    solo_levels_ranks,\n",
    ")\n",
    "\n",
    "# Test Bloom affective agains other domains\n",
    "taxonomy_rank_tester(\n",
    "    bloom_affective_copy,\n",
    "    bloom_cognitive_copy,\n",
    "    bloom_psychomotor_copy,\n",
    "    solo_copy,\n",
    "    bloom_affective_levels_ranks,\n",
    "    bloom_cognitive_levels_ranks,\n",
    "    bloom_psychomotor_levels_ranks,\n",
    "    solo_levels_ranks,\n",
    ")\n",
    "\n",
    "# Test Bloom psychomotor agianst other domains\n",
    "taxonomy_rank_tester(\n",
    "    bloom_psychomotor_copy,\n",
    "    bloom_affective_copy,\n",
    "    bloom_cognitive_copy,\n",
    "    solo_copy,\n",
    "    bloom_psychomotor_levels_ranks,\n",
    "    bloom_affective_levels_ranks,\n",
    "    bloom_cognitive_levels_ranks,\n",
    "    solo_levels_ranks,\n",
    ")\n",
    "\n",
    "# bloom_cognitive_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Method 1: Before adding solo verbs to other domains\n",
      "Percentage of  Cognitive  mappings passed:  34 %\n",
      "Percentage of  Affective  mappings passed:  28 %\n",
      "Percentage of  Psychomotor  mappings passed:  57 %\n",
      "Percentage of  SOLO  mappings passed:  32 %\n",
      "Total percentage of mappings passed:  36 %\n"
     ]
    }
   ],
   "source": [
    "suggested_sim_threshold = 0.985\n",
    "sim_threshold = 0.997\n",
    "\n",
    "# Before changing rankings\n",
    "# Percentage of  Cognitive  mappings passed:  31 %\n",
    "# Percentage of  Affective  mappings passed:  33 %\n",
    "# Percentage of  Psychomotor  mappings passed:  29 %\n",
    "# Percentage of  SOLO  mappings passed:  27 %\n",
    "# Total percentage of mappings passed:  30 %\n",
    "\n",
    "# After changing rankings\n",
    "# Percentage of  Cognitive  mappings passed:  29 %\n",
    "# Percentage of  Affective  mappings passed:  27 %\n",
    "# Percentage of  Psychomotor  mappings passed:  29 %\n",
    "# Percentage of  SOLO  mappings passed:  36 %\n",
    "# Total percentage of mappings passed:  30 %\n",
    "\n",
    "mapped_verbs = {\n",
    "    \"Cognitive\": bloom_cognitive_copy,\n",
    "    \"Affective\": bloom_affective_copy,\n",
    "    \"Psychomotor\": bloom_psychomotor_copy,\n",
    "    \"SOLO\": solo_copy,\n",
    "}\n",
    "verb_list = generate_verb_list(mapped_verbs)\n",
    "\n",
    "print(\"Method 1: Before adding solo verbs to other domains\")\n",
    "\n",
    "word_vectors = model_1_wv\n",
    "learning_outcome_mapping(\n",
    "    sentences, final_levels, sim_threshold, suggested_sim_threshold\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mapping solo verbs into other domains\n",
    "\n",
    "for i in range(solo_copy.shape[0]):  # Level\n",
    "    for j in range(solo_copy.shape[1]):  # Verb\n",
    "        verb = solo_copy.iloc[i, j]\n",
    "\n",
    "        if verb is None or not verb or pd.isna(verb):\n",
    "            continue\n",
    "\n",
    "        # Get level and ranking of current verb\n",
    "        solo_level = solo_copy[solo_copy == verb].stack().index[0][1]\n",
    "        solo_level_rank = solo_levels_ranks[solo_level]\n",
    "\n",
    "        # Check if solo verb exists in each domain\n",
    "        # If verb does not exist in the domain, map the current solo verb into the domain dataframe at the level ranking same as solo level rank\n",
    "        if verb not in bloom_cognitive_copy.values:\n",
    "            bloom_cognitive_level = get_keys_by_value(\n",
    "                bloom_cognitive_levels_ranks, solo_level_rank\n",
    "            )\n",
    "            temp_df = pd.DataFrame({bloom_cognitive_level: verb}, index=[0])\n",
    "            bloom_cognitive_copy = pd.concat(\n",
    "                [bloom_cognitive_copy, temp_df], ignore_index=True\n",
    "            )\n",
    "\n",
    "        if verb not in bloom_affective_copy.values:\n",
    "            bloom_affective_level = get_keys_by_value(\n",
    "                bloom_affective_levels_ranks, solo_level_rank\n",
    "            )\n",
    "            temp_df = pd.DataFrame({bloom_affective_level: verb}, index=[0])\n",
    "            bloom_affective_copy = pd.concat(\n",
    "                [bloom_affective_copy, temp_df], ignore_index=True\n",
    "            )\n",
    "\n",
    "        if verb not in bloom_psychomotor_copy.values:\n",
    "            bloom_psychomotor_level = get_keys_by_value(\n",
    "                bloom_psychomotor_levels_ranks, solo_level_rank\n",
    "            )\n",
    "            temp_df = pd.DataFrame({bloom_psychomotor_level: verb}, index=[0])\n",
    "            bloom_psychomotor_copy = pd.concat(\n",
    "                [bloom_psychomotor_copy, temp_df], ignore_index=True\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Method 1: After adding solo verbs to other domains\n",
      "Percentage of  Cognitive  mappings passed:  32 %\n",
      "Percentage of  Affective  mappings passed:  25 %\n",
      "Percentage of  Psychomotor  mappings passed:  48 %\n",
      "Percentage of  SOLO  mappings passed:  32 %\n",
      "Total percentage of mappings passed:  34 %\n"
     ]
    }
   ],
   "source": [
    "suggested_sim_threshold = 0.985\n",
    "sim_threshold = 0.997\n",
    "\n",
    "# Before changing rankings\n",
    "# Percentage of  Cognitive  mappings passed:  31 %\n",
    "# Percentage of  Affective  mappings passed:  33 %\n",
    "# Percentage of  Psychomotor  mappings passed:  29 %\n",
    "# Percentage of  SOLO  mappings passed:  27 %\n",
    "# Total percentage of mappings passed:  30 %\n",
    "\n",
    "# After changing rankings\n",
    "# Percentage of  Cognitive  mappings passed:  29 %\n",
    "# Percentage of  Affective  mappings passed:  27 %\n",
    "# Percentage of  Psychomotor  mappings passed:  29 %\n",
    "# Percentage of  SOLO  mappings passed:  36 %\n",
    "# Total percentage of mappings passed:  30 %\n",
    "\n",
    "mapped_verbs = {\n",
    "    \"Cognitive\": bloom_cognitive_copy,\n",
    "    \"Affective\": bloom_affective_copy,\n",
    "    \"Psychomotor\": bloom_psychomotor_copy,\n",
    "    \"SOLO\": solo_copy,\n",
    "}\n",
    "verb_list = generate_verb_list(mapped_verbs)\n",
    "\n",
    "print(\"Method 1: After adding solo verbs to other domains\")\n",
    "\n",
    "word_vectors = model_1_wv\n",
    "learning_outcome_mapping(\n",
    "    sentences, final_levels, sim_threshold, suggested_sim_threshold\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
