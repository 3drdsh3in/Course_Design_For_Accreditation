{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: docx2txt in /home/edward/.local/lib/python3.8/site-packages (0.8)\n",
      "Requirement already satisfied: gensim in /home/edward/.local/lib/python3.8/site-packages (4.3.1)\n",
      "Requirement already satisfied: scipy>=1.7.0 in /home/edward/.local/lib/python3.8/site-packages (from gensim) (1.9.2)\n",
      "Requirement already satisfied: numpy>=1.18.5 in /home/edward/.local/lib/python3.8/site-packages (from gensim) (1.22.3)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in /home/edward/.local/lib/python3.8/site-packages (from gensim) (5.2.1)\n",
      "Requirement already satisfied: keras in /home/edward/.local/lib/python3.8/site-packages (2.10.0)\n",
      "Requirement already satisfied: nltk in /home/edward/.local/lib/python3.8/site-packages (3.7)\n",
      "Requirement already satisfied: tqdm in /home/edward/.local/lib/python3.8/site-packages (from nltk) (4.63.1)\n",
      "Requirement already satisfied: joblib in /home/edward/.local/lib/python3.8/site-packages (from nltk) (1.2.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /home/edward/.local/lib/python3.8/site-packages (from nltk) (2022.3.15)\n",
      "Requirement already satisfied: click in /usr/lib/python3/dist-packages (from nltk) (7.0)\n",
      "Requirement already up-to-date: scikit-learn in /home/edward/.local/lib/python3.8/site-packages (1.2.2)\n",
      "Requirement already satisfied, skipping upgrade: scipy>=1.3.2 in /home/edward/.local/lib/python3.8/site-packages (from scikit-learn) (1.9.2)\n",
      "Requirement already satisfied, skipping upgrade: joblib>=1.1.1 in /home/edward/.local/lib/python3.8/site-packages (from scikit-learn) (1.2.0)\n",
      "Requirement already satisfied, skipping upgrade: numpy>=1.17.3 in /home/edward/.local/lib/python3.8/site-packages (from scikit-learn) (1.22.3)\n",
      "Requirement already satisfied, skipping upgrade: threadpoolctl>=2.0.0 in /home/edward/.local/lib/python3.8/site-packages (from scikit-learn) (3.1.0)\n",
      "Requirement already satisfied: python-docx in /home/edward/.local/lib/python3.8/site-packages (0.8.11)\n",
      "Requirement already satisfied: lxml>=2.3.2 in /home/edward/.local/lib/python3.8/site-packages (from python-docx) (4.6.3)\n",
      "Requirement already satisfied: tensorflow in /home/edward/.local/lib/python3.8/site-packages (2.10.0)\n",
      "Requirement already satisfied: tensorflow-estimator<2.11,>=2.10.0 in /home/edward/.local/lib/python3.8/site-packages (from tensorflow) (2.10.0)\n",
      "Requirement already satisfied: protobuf<3.20,>=3.9.2 in /home/edward/.local/lib/python3.8/site-packages (from tensorflow) (3.19.6)\n",
      "Requirement already satisfied: keras<2.11,>=2.10.0 in /home/edward/.local/lib/python3.8/site-packages (from tensorflow) (2.10.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /home/edward/.local/lib/python3.8/site-packages (from tensorflow) (4.1.1)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /home/edward/.local/lib/python3.8/site-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /home/edward/.local/lib/python3.8/site-packages (from tensorflow) (3.3.0)\n",
      "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /home/edward/.local/lib/python3.8/site-packages (from tensorflow) (0.4.0)\n",
      "Requirement already satisfied: h5py>=2.9.0 in /home/edward/.local/lib/python3.8/site-packages (from tensorflow) (3.7.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /home/edward/.local/lib/python3.8/site-packages (from tensorflow) (1.14.1)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /home/edward/.local/lib/python3.8/site-packages (from tensorflow) (14.0.6)\n",
      "Requirement already satisfied: tensorboard<2.11,>=2.10 in /home/edward/.local/lib/python3.8/site-packages (from tensorflow) (2.10.1)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /home/edward/.local/lib/python3.8/site-packages (from tensorflow) (1.2.0)\n",
      "Requirement already satisfied: keras-preprocessing>=1.1.1 in /home/edward/.local/lib/python3.8/site-packages (from tensorflow) (1.1.2)\n",
      "Requirement already satisfied: six>=1.12.0 in /home/edward/.local/lib/python3.8/site-packages (from tensorflow) (1.16.0)\n",
      "Requirement already satisfied: flatbuffers>=2.0 in /home/edward/.local/lib/python3.8/site-packages (from tensorflow) (22.9.24)\n",
      "Requirement already satisfied: numpy>=1.20 in /home/edward/.local/lib/python3.8/site-packages (from tensorflow) (1.22.3)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /home/edward/.local/lib/python3.8/site-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /home/edward/.local/lib/python3.8/site-packages (from tensorflow) (0.27.0)\n",
      "Requirement already satisfied: packaging in /home/edward/.local/lib/python3.8/site-packages (from tensorflow) (21.3)\n",
      "Requirement already satisfied: setuptools in /usr/lib/python3/dist-packages (from tensorflow) (45.2.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /home/edward/.local/lib/python3.8/site-packages (from tensorflow) (1.49.1)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /home/edward/.local/lib/python3.8/site-packages (from tensorflow) (2.0.1)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /home/edward/.local/lib/python3.8/site-packages (from tensorboard<2.11,>=2.10->tensorflow) (3.4.1)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /usr/lib/python3/dist-packages (from tensorboard<2.11,>=2.10->tensorflow) (2.22.0)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /home/edward/.local/lib/python3.8/site-packages (from tensorboard<2.11,>=2.10->tensorflow) (1.8.1)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /home/edward/.local/lib/python3.8/site-packages (from tensorboard<2.11,>=2.10->tensorflow) (2.2.2)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /home/edward/.local/lib/python3.8/site-packages (from tensorboard<2.11,>=2.10->tensorflow) (0.4.6)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /home/edward/.local/lib/python3.8/site-packages (from tensorboard<2.11,>=2.10->tensorflow) (0.6.1)\n",
      "Requirement already satisfied: wheel>=0.26 in /usr/lib/python3/dist-packages (from tensorboard<2.11,>=2.10->tensorflow) (0.34.2)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /home/edward/.local/lib/python3.8/site-packages (from tensorboard<2.11,>=2.10->tensorflow) (2.12.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/edward/.local/lib/python3.8/site-packages (from packaging->tensorflow) (3.0.7)\n",
      "Requirement already satisfied: importlib-metadata>=4.4; python_version < \"3.10\" in /home/edward/.local/lib/python3.8/site-packages (from markdown>=2.6.8->tensorboard<2.11,>=2.10->tensorflow) (5.0.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /home/edward/.local/lib/python3.8/site-packages (from werkzeug>=1.0.1->tensorboard<2.11,>=2.10->tensorflow) (2.1.1)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /home/edward/.local/lib/python3.8/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.11,>=2.10->tensorflow) (1.3.1)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/lib/python3/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.11,>=2.10->tensorflow) (0.2.1)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /home/edward/.local/lib/python3.8/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.11,>=2.10->tensorflow) (5.2.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3.6\" in /home/edward/.local/lib/python3.8/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.11,>=2.10->tensorflow) (4.9)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/lib/python3/dist-packages (from importlib-metadata>=4.4; python_version < \"3.10\"->markdown>=2.6.8->tensorboard<2.11,>=2.10->tensorflow) (1.0.0)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /usr/lib/python3/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.11,>=2.10->tensorflow) (3.1.0)\n",
      "Requirement already satisfied: pyasn1>=0.1.3 in /usr/lib/python3/dist-packages (from rsa<5,>=3.1.4; python_version >= \"3.6\"->google-auth<3,>=1.6.3->tensorboard<2.11,>=2.10->tensorflow) (0.4.2)\n",
      "Requirement already satisfied: nltk in /home/edward/.local/lib/python3.8/site-packages (3.7)\n",
      "Requirement already satisfied: joblib in /home/edward/.local/lib/python3.8/site-packages (from nltk) (1.2.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /home/edward/.local/lib/python3.8/site-packages (from nltk) (2022.3.15)\n",
      "Requirement already satisfied: tqdm in /home/edward/.local/lib/python3.8/site-packages (from nltk) (4.63.1)\n",
      "Requirement already satisfied: click in /usr/lib/python3/dist-packages (from nltk) (7.0)\n",
      "Requirement already satisfied: spacy in /home/edward/.local/lib/python3.8/site-packages (3.2.3)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /home/edward/.local/lib/python3.8/site-packages (from spacy) (4.63.1)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /home/edward/.local/lib/python3.8/site-packages (from spacy) (2.0.6)\n",
      "Requirement already satisfied: pathy>=0.3.5 in /home/edward/.local/lib/python3.8/site-packages (from spacy) (0.6.1)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/lib/python3/dist-packages (from spacy) (2.22.0)\n",
      "Requirement already satisfied: thinc<8.1.0,>=8.0.12 in /home/edward/.local/lib/python3.8/site-packages (from spacy) (8.0.15)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /home/edward/.local/lib/python3.8/site-packages (from spacy) (3.3.0)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /home/edward/.local/lib/python3.8/site-packages (from spacy) (3.0.6)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.4.0 in /home/edward/.local/lib/python3.8/site-packages (from spacy) (0.7.7)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.8.1 in /home/edward/.local/lib/python3.8/site-packages (from spacy) (0.9.0)\n",
      "Requirement already satisfied: typer<0.5.0,>=0.3.0 in /home/edward/.local/lib/python3.8/site-packages (from spacy) (0.4.0)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4 in /home/edward/.local/lib/python3.8/site-packages (from spacy) (1.8.2)\n",
      "Requirement already satisfied: jinja2 in /home/edward/.local/lib/python3.8/site-packages (from spacy) (3.1.2)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /home/edward/.local/lib/python3.8/site-packages (from spacy) (1.0.6)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/edward/.local/lib/python3.8/site-packages (from spacy) (21.3)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.1 in /home/edward/.local/lib/python3.8/site-packages (from spacy) (2.4.2)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /home/edward/.local/lib/python3.8/site-packages (from spacy) (1.0.1)\n",
      "Requirement already satisfied: setuptools in /usr/lib/python3/dist-packages (from spacy) (45.2.0)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /home/edward/.local/lib/python3.8/site-packages (from spacy) (2.0.7)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /home/edward/.local/lib/python3.8/site-packages (from spacy) (1.22.3)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.8 in /home/edward/.local/lib/python3.8/site-packages (from spacy) (3.0.9)\n",
      "Requirement already satisfied: smart-open<6.0.0,>=5.0.0 in /home/edward/.local/lib/python3.8/site-packages (from pathy>=0.3.5->spacy) (5.2.1)\n",
      "Collecting click<9.0.0,>=7.1.1\n",
      "  Downloading click-8.1.3-py3-none-any.whl (96 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/edward/.local/lib/python3.8/site-packages (from pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4->spacy) (4.1.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/edward/.local/lib/python3.8/site-packages (from jinja2->spacy) (2.1.1)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/edward/.local/lib/python3.8/site-packages (from packaging>=20.0->spacy) (3.0.7)\n",
      "Installing collected packages: click\n",
      "Successfully installed click-8.1.3\n",
      "docx2csv exists.\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "# Dependencies\n",
    "pip install docx2txt\n",
    "pip install gensim\n",
    "pip install keras\n",
    "pip install nltk\n",
    "pip install -U scikit-learn\n",
    "pip install python-docx\n",
    "pip install tensorflow\n",
    "pip install nltk\n",
    "pip install spacy\n",
    "\n",
    "if ls docx2csv >/dev/null 2>&1; then\n",
    "    echo \"docx2csv exists.\"\n",
    "else\n",
    "    echo \"Folder does not exist. Cloning docx2csv.\"\n",
    "    git clone https://github.com/ivbeg/docx2csv.git\n",
    "fi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running install\n",
      "running bdist_egg\n",
      "running egg_info\n",
      "writing docx2csv.egg-info/PKG-INFO\n",
      "writing dependency_links to docx2csv.egg-info/dependency_links.txt\n",
      "writing entry points to docx2csv.egg-info/entry_points.txt\n",
      "writing requirements to docx2csv.egg-info/requires.txt\n",
      "writing top-level names to docx2csv.egg-info/top_level.txt\n",
      "reading manifest file 'docx2csv.egg-info/SOURCES.txt'\n",
      "writing manifest file 'docx2csv.egg-info/SOURCES.txt'\n",
      "installing library code to build/bdist.linux-x86_64/egg\n",
      "running install_lib\n",
      "running build_py\n",
      "creating build/bdist.linux-x86_64/egg\n",
      "creating build/bdist.linux-x86_64/egg/docx2csv\n",
      "copying build/lib/docx2csv/converter.py -> build/bdist.linux-x86_64/egg/docx2csv\n",
      "copying build/lib/docx2csv/core.py -> build/bdist.linux-x86_64/egg/docx2csv\n",
      "copying build/lib/docx2csv/__init__.py -> build/bdist.linux-x86_64/egg/docx2csv\n",
      "copying build/lib/docx2csv/__main__.py -> build/bdist.linux-x86_64/egg/docx2csv\n",
      "byte-compiling build/bdist.linux-x86_64/egg/docx2csv/converter.py to converter.cpython-38.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/docx2csv/core.py to core.cpython-38.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/docx2csv/__init__.py to __init__.cpython-38.pyc\n",
      "byte-compiling build/bdist.linux-x86_64/egg/docx2csv/__main__.py to __main__.cpython-38.pyc\n",
      "creating build/bdist.linux-x86_64/egg/EGG-INFO\n",
      "copying docx2csv.egg-info/PKG-INFO -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
      "copying docx2csv.egg-info/SOURCES.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
      "copying docx2csv.egg-info/dependency_links.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
      "copying docx2csv.egg-info/entry_points.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
      "copying docx2csv.egg-info/not-zip-safe -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
      "copying docx2csv.egg-info/requires.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
      "copying docx2csv.egg-info/top_level.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
      "creating 'dist/docx2csv-0.1.2-py3.8.egg' and adding 'build/bdist.linux-x86_64/egg' to it\n",
      "removing 'build/bdist.linux-x86_64/egg' (and everything under it)\n",
      "Processing docx2csv-0.1.2-py3.8.egg\n",
      "removing '/usr/local/lib/python3.8/dist-packages/docx2csv-0.1.2-py3.8.egg' (and everything under it)\n",
      "creating /usr/local/lib/python3.8/dist-packages/docx2csv-0.1.2-py3.8.egg\n",
      "Extracting docx2csv-0.1.2-py3.8.egg to /usr/local/lib/python3.8/dist-packages\n",
      "docx2csv 0.1.2 is already the active version in easy-install.pth\n",
      "Installing docx2csv script to /usr/local/bin\n",
      "\n",
      "Installed /usr/local/lib/python3.8/dist-packages/docx2csv-0.1.2-py3.8.egg\n",
      "Processing dependencies for docx2csv==0.1.2\n",
      "Searching for xlwt==1.3.0\n",
      "Best match: xlwt 1.3.0\n",
      "Processing xlwt-1.3.0-py3.8.egg\n",
      "xlwt 1.3.0 is already the active version in easy-install.pth\n",
      "\n",
      "Using /usr/local/lib/python3.8/dist-packages/xlwt-1.3.0-py3.8.egg\n",
      "Searching for openpyxl==3.2.0b1\n",
      "Best match: openpyxl 3.2.0b1\n",
      "Processing openpyxl-3.2.0b1-py3.8.egg\n",
      "openpyxl 3.2.0b1 is already the active version in easy-install.pth\n",
      "\n",
      "Using /usr/local/lib/python3.8/dist-packages/openpyxl-3.2.0b1-py3.8.egg\n",
      "Searching for docx==0.2.4\n",
      "Best match: docx 0.2.4\n",
      "Processing docx-0.2.4-py3.8.egg\n",
      "docx 0.2.4 is already the active version in easy-install.pth\n",
      "\n",
      "Using /usr/local/lib/python3.8/dist-packages/docx-0.2.4-py3.8.egg\n",
      "Searching for Click==7.0\n",
      "Best match: Click 7.0\n",
      "Adding Click 7.0 to easy-install.pth file\n",
      "\n",
      "Using /usr/lib/python3/dist-packages\n",
      "Searching for et-xmlfile==1.1.0\n",
      "Best match: et-xmlfile 1.1.0\n",
      "Processing et_xmlfile-1.1.0-py3.8.egg\n",
      "et-xmlfile 1.1.0 is already the active version in easy-install.pth\n",
      "\n",
      "Using /usr/local/lib/python3.8/dist-packages/et_xmlfile-1.1.0-py3.8.egg\n",
      "Searching for lxml==4.9.2\n",
      "Best match: lxml 4.9.2\n",
      "Processing lxml-4.9.2-py3.8-linux-x86_64.egg\n",
      "lxml 4.9.2 is already the active version in easy-install.pth\n",
      "\n",
      "Using /usr/local/lib/python3.8/dist-packages/lxml-4.9.2-py3.8-linux-x86_64.egg\n",
      "Searching for Pillow==9.5.0\n",
      "Best match: Pillow 9.5.0\n",
      "Processing Pillow-9.5.0-py3.8-linux-x86_64.egg\n",
      "Pillow 9.5.0 is already the active version in easy-install.pth\n",
      "\n",
      "Using /usr/local/lib/python3.8/dist-packages/Pillow-9.5.0-py3.8-linux-x86_64.egg\n",
      "Finished processing dependencies for docx2csv==0.1.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[sudo] password for edward: "
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "source .env\n",
    "cd docx2csv && echo \"$PASSWORD\" | sudo -S python3 setup.py install"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----- TEST DATA INPUT -----\n",
    "\n",
    "# Computer Science Test Data.\n",
    "# CURRENT_MAPPING=\"Lists_ComputerScience.docx\"\n",
    "# ORIGINAL_MAPPING=\"Original-Mapping-ComputerScience.csv\"\n",
    "\n",
    "# InformationSecurity Test Data.\n",
    "CURRENT_MAPPING=\"Lists_InformationSecurity.docx\"\n",
    "ORIGINAL_MAPPING=\"Original-Mapping-InfoSecurity.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/requests/__init__.py:89: RequestsDependencyWarning: urllib3 (1.26.15) or chardet (5.1.0) doesn't match a supported version!\n",
      "  warnings.warn(\"urllib3 ({}) or chardet ({}) doesn't match a supported \"\n",
      "[nltk_data] Downloading package punkt to /home/edward/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/edward/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/edward/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "from gensim.test.utils import common_texts\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract tables from word document\n",
    "from docx2csv import extract_tables, extract\n",
    "tables = extract_tables(CURRENT_MAPPING)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from docx import Document\n",
    "document = Document(CURRENT_MAPPING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Analyze',\n",
       "  'complex',\n",
       "  'computing',\n",
       "  'problem',\n",
       "  'apply',\n",
       "  'principle',\n",
       "  'computing',\n",
       "  'relevant',\n",
       "  'discipline',\n",
       "  'identify',\n",
       "  'solution',\n",
       "  '.'],\n",
       " ['Design',\n",
       "  ',',\n",
       "  'implement',\n",
       "  ',',\n",
       "  'evaluate',\n",
       "  'computing-based',\n",
       "  'solution',\n",
       "  'meet',\n",
       "  'given',\n",
       "  'set',\n",
       "  'computing',\n",
       "  'requirement',\n",
       "  'context',\n",
       "  'program',\n",
       "  '’',\n",
       "  'discipline',\n",
       "  '.'],\n",
       " ['Communicate', 'effectively', 'variety', 'professional', 'context', '.'],\n",
       " ['Recognize',\n",
       "  'professional',\n",
       "  'responsibility',\n",
       "  'make',\n",
       "  'informed',\n",
       "  'judgment',\n",
       "  'computing',\n",
       "  'practice',\n",
       "  'based',\n",
       "  'legal',\n",
       "  'ethical',\n",
       "  'principle',\n",
       "  '.'],\n",
       " ['Function',\n",
       "  'effectively',\n",
       "  'member',\n",
       "  'leader',\n",
       "  'team',\n",
       "  'engaged',\n",
       "  'activity',\n",
       "  'appropriate',\n",
       "  'program',\n",
       "  '’',\n",
       "  'discipline',\n",
       "  '.'],\n",
       " ['Apply',\n",
       "  'security',\n",
       "  'principle',\n",
       "  'practice',\n",
       "  'maintain',\n",
       "  'operation',\n",
       "  'presence',\n",
       "  'risk',\n",
       "  'threat',\n",
       "  '.'],\n",
       " ['Demonstrate',\n",
       "  'computational',\n",
       "  'thinking',\n",
       "  'skill',\n",
       "  'solve',\n",
       "  'computing',\n",
       "  'problem'],\n",
       " ['Describe',\n",
       "  'system',\n",
       "  'component',\n",
       "  'involved',\n",
       "  'building',\n",
       "  'usable',\n",
       "  'computing',\n",
       "  'platform'],\n",
       " ['Identify', 'different', 'tool', 'technique', 'used', 'system'],\n",
       " ['Discuss', 'related', 'ethical', 'issue'],\n",
       " ['Translate',\n",
       "  'problem',\n",
       "  'expressed',\n",
       "  'English',\n",
       "  ',',\n",
       "  'mathematics',\n",
       "  'diagram',\n",
       "  'computer',\n",
       "  'program'],\n",
       " ['Implement',\n",
       "  'algorithm',\n",
       "  'using',\n",
       "  'programming',\n",
       "  'construct',\n",
       "  '(',\n",
       "  'variable',\n",
       "  ',',\n",
       "  'control',\n",
       "  'structure',\n",
       "  ',',\n",
       "  'method',\n",
       "  ')'],\n",
       " ['Solve', 'problem', 'using', 'suitable', 'data', 'structure'],\n",
       " ['Implement', 'searching', ',', 'summing', 'selecting', 'algorithm'],\n",
       " ['Design', 'implement', 'algorithm', 'solve', 'simple', 'problem'],\n",
       " ['Choose', 'suitable', 'data', 'type', 'represent', 'information'],\n",
       " ['Apply',\n",
       "  'sequence',\n",
       "  ',',\n",
       "  'selection',\n",
       "  'repetition',\n",
       "  'structure',\n",
       "  'solve',\n",
       "  'problem'],\n",
       " ['Design', 'implement', 'program', 'containing', 'many', 'method'],\n",
       " ['Manipulate', 'One-Dimension', 'Two-Dimension', 'array'],\n",
       " ['Use',\n",
       "  'formal',\n",
       "  'method',\n",
       "  'symbolic',\n",
       "  'proposition',\n",
       "  'evaluate',\n",
       "  'elementary',\n",
       "  'mathematical',\n",
       "  'argument',\n",
       "  'identify',\n",
       "  'logical',\n",
       "  'reasoning'],\n",
       " ['Prove',\n",
       "  'assertion',\n",
       "  'using',\n",
       "  'basic',\n",
       "  'proof',\n",
       "  'method',\n",
       "  ',',\n",
       "  '(',\n",
       "  'eg',\n",
       "  'induction',\n",
       "  ',',\n",
       "  'contrapositive',\n",
       "  ',',\n",
       "  'contradiction',\n",
       "  ',',\n",
       "  '…etc',\n",
       "  ')'],\n",
       " ['Derive', 'closed-forms', 'summation', 'recursive', 'structure'],\n",
       " ['Apply',\n",
       "  'graph',\n",
       "  'theory',\n",
       "  'tree',\n",
       "  'model',\n",
       "  'solve',\n",
       "  'problem',\n",
       "  'connectivity'],\n",
       " ['Use',\n",
       "  'logical',\n",
       "  'notation',\n",
       "  'define',\n",
       "  'reason',\n",
       "  'fundamental',\n",
       "  'mathematical',\n",
       "  'concept',\n",
       "  'set',\n",
       "  ',',\n",
       "  'relation',\n",
       "  ',',\n",
       "  'function',\n",
       "  ',',\n",
       "  'matrix',\n",
       "  'integer'],\n",
       " ['Convert',\n",
       "  'different',\n",
       "  'number',\n",
       "  'system',\n",
       "  'represent',\n",
       "  'signed',\n",
       "  'number',\n",
       "  '1',\n",
       "  \"'s\",\n",
       "  '2',\n",
       "  \"'s\",\n",
       "  'complement',\n",
       "  'representation'],\n",
       " ['Analyze', 'combinational', 'sequential', 'circuit'],\n",
       " ['Design', 'implement', 'combinational', 'sequential', 'circuit'],\n",
       " ['Define',\n",
       "  'modern',\n",
       "  'computer',\n",
       "  'system',\n",
       "  \"'s\",\n",
       "  'major',\n",
       "  'component',\n",
       "  ',',\n",
       "  'function',\n",
       "  'inter-relationships'],\n",
       " ['Explain',\n",
       "  'memory',\n",
       "  'hierarchy',\n",
       "  'structure',\n",
       "  'importance',\n",
       "  'characteristic',\n",
       "  'level'],\n",
       " ['Describe',\n",
       "  'component',\n",
       "  'instruction',\n",
       "  'set',\n",
       "  'different',\n",
       "  'type',\n",
       "  'instruction',\n",
       "  'addressing',\n",
       "  'mode'],\n",
       " ['Explain', 'different', 'concept', 'function', 'physical', 'layer'],\n",
       " ['Apply',\n",
       "  'different',\n",
       "  'mechanism',\n",
       "  'error',\n",
       "  'control',\n",
       "  ',',\n",
       "  'flow',\n",
       "  'control',\n",
       "  'medium',\n",
       "  'access',\n",
       "  'control',\n",
       "  'data',\n",
       "  'link',\n",
       "  'layer'],\n",
       " ['Explain',\n",
       "  'main',\n",
       "  'function',\n",
       "  'network',\n",
       "  'layer',\n",
       "  'packet',\n",
       "  'switching',\n",
       "  ',',\n",
       "  'IP',\n",
       "  'addressing',\n",
       "  'fragmentation'],\n",
       " ['Discuss',\n",
       "  'operation',\n",
       "  'function',\n",
       "  'different',\n",
       "  'Transport',\n",
       "  'layer',\n",
       "  'protocol'],\n",
       " ['Implement', 'class', 'solve', 'given', 'problem'],\n",
       " ['Test', 'simple', 'class'],\n",
       " ['Design', 'class', 'using', 'existing', 'class', 'library'],\n",
       " ['Develop', 'class', 'hierarchy', 'using', 'inheritance'],\n",
       " ['Develop', 'class', 'simple', 'data', 'structure'],\n",
       " ['Design',\n",
       "  'implement',\n",
       "  'small',\n",
       "  'medium',\n",
       "  'size',\n",
       "  'software',\n",
       "  'problem',\n",
       "  'using',\n",
       "  'object'],\n",
       " ['Use', 'Arrays', 'Array-Lists', 'solving', 'problem'],\n",
       " ['Implement', 'user-defined', 'class', 'solve', 'given', 'problem'],\n",
       " ['Use',\n",
       "  'predefined',\n",
       "  'library',\n",
       "  'develop',\n",
       "  'program',\n",
       "  'graphical',\n",
       "  'user',\n",
       "  'interface'],\n",
       " ['Develop', 'class', 'hierarchy', 'using', 'inheritance'],\n",
       " ['Apply', 'project', 'lifecycle', 'process', 'project'],\n",
       " ['Apply', 'project', 'technique', 'tool', 'manage', 'project'],\n",
       " ['Identify', 'internal', 'external', 'constraint', 'given', 'project'],\n",
       " ['Produce', 'document', 'typically', 'used', 'development', 'project'],\n",
       " ['Demonstrate',\n",
       "  'verbal',\n",
       "  ',',\n",
       "  'written',\n",
       "  'communication',\n",
       "  'skill',\n",
       "  'part',\n",
       "  'team'],\n",
       " ['Explain', 'security', 'policy', ',', 'model', ',', 'mechanism'],\n",
       " ['Discuss', 'operating', 'system', 'security', 'model', 'mechanism'],\n",
       " ['Describe', 'cryptographic', 'technique', 'application'],\n",
       " ['Analyze', 'security', 'threat', 'vulnerability', 'computer', 'system'],\n",
       " ['Define', 'solution', 'defend', 'virus', 'malicious', 'program'],\n",
       " ['Explain', 'logical', 'progression', 'operating', 'system', 'development'],\n",
       " ['Explain', 'necessary', 'component', 'structure', 'operating', 'system'],\n",
       " ['Install', 'customize', 'operating', 'system'],\n",
       " ['Write', 'simple', 'shell', 'script', 'operating', 'system'],\n",
       " ['Evaluate',\n",
       "  'various',\n",
       "  'method',\n",
       "  'process',\n",
       "  'scheduling',\n",
       "  'inter-process',\n",
       "  'communication'],\n",
       " ['Explain', 'file-system', 'concept', 'operation'],\n",
       " ['Apply', 'recursion', 'solve', 'problem'],\n",
       " ['Use',\n",
       "  'APIs',\n",
       "  'implementing',\n",
       "  'moderate',\n",
       "  'size',\n",
       "  'program',\n",
       "  'data',\n",
       "  'structure'],\n",
       " ['Design', 'implement', 'linear', 'data', 'structure'],\n",
       " ['Design', 'implement', 'tree', 'data', 'structure'],\n",
       " ['Model', 'Solve', 'problem', 'using', 'graph'],\n",
       " ['Describe', 'main', 'concept', 'database', 'system'],\n",
       " ['Compare',\n",
       "  'database',\n",
       "  'system',\n",
       "  'approach',\n",
       "  'file-based',\n",
       "  'system',\n",
       "  'approach'],\n",
       " ['Design',\n",
       "  'database',\n",
       "  'using',\n",
       "  'entity-relationship',\n",
       "  'diagram',\n",
       "  '(',\n",
       "  'ERD',\n",
       "  ')'],\n",
       " ['Use',\n",
       "  'Relational',\n",
       "  'Algebra',\n",
       "  'perform',\n",
       "  'various',\n",
       "  'operation',\n",
       "  'relation'],\n",
       " ['Apply', 'normalization', 'database', 'table'],\n",
       " ['Function', 'effectively', 'team', 'create', 'query', 'database'],\n",
       " ['Analyze',\n",
       "  'issue',\n",
       "  'case',\n",
       "  'study',\n",
       "  'using',\n",
       "  'ethical',\n",
       "  'decision',\n",
       "  'making',\n",
       "  'based',\n",
       "  'code',\n",
       "  'ethic',\n",
       "  'formal',\n",
       "  'method'],\n",
       " ['Identify',\n",
       "  'privacy',\n",
       "  ',',\n",
       "  'freedom',\n",
       "  'speech',\n",
       "  'crime',\n",
       "  'issue',\n",
       "  'Cyberspace'],\n",
       " ['Discuss', 'intellectual', 'property', 'software', 'development', 'issue'],\n",
       " ['Discuss', 'implication', 'computing', 'workplace', 'worker', 'employer'],\n",
       " ['Discuss',\n",
       "  'socio-economic',\n",
       "  'implication',\n",
       "  'online',\n",
       "  'community',\n",
       "  'Digital',\n",
       "  'Divide'],\n",
       " ['Function',\n",
       "  'group',\n",
       "  'ass',\n",
       "  'current',\n",
       "  'ethical',\n",
       "  'issue',\n",
       "  'communicate',\n",
       "  'result',\n",
       "  'oral',\n",
       "  'written',\n",
       "  'form'],\n",
       " ['Describe',\n",
       "  'role',\n",
       "  'skill',\n",
       "  'entrepreneur',\n",
       "  'cultivate',\n",
       "  'entrepreneurial',\n",
       "  'mindset'],\n",
       " ['Apply',\n",
       "  'process',\n",
       "  'followed',\n",
       "  'entrepreneur',\n",
       "  'selecting',\n",
       "  'valuable',\n",
       "  'opportunity',\n",
       "  'related',\n",
       "  'business'],\n",
       " ['Explain', 'principle', 'success', 'factor', 'venture', 'planning'],\n",
       " ['Evaluate',\n",
       "  'viability',\n",
       "  'functional',\n",
       "  'planning',\n",
       "  'venture',\n",
       "  ',',\n",
       "  'considering',\n",
       "  'key',\n",
       "  'legal',\n",
       "  'intellectual',\n",
       "  'property',\n",
       "  'issue'],\n",
       " ['Create', 'business', 'model', 'financial', 'plan', 'venture'],\n",
       " ['Identify', 'structure', 'operation', 'workplace'],\n",
       " ['Recognize', 'employee', 'right', 'responsibility'],\n",
       " ['Apply', 'knowledge', 'workplace'],\n",
       " ['Function', 'effectively', ',', 'professionally', 'ethically', 'team'],\n",
       " ['Communicate', 'effectively', 'technically', 'orally', 'writing'],\n",
       " ['Recognize', 'need', 'continuing', 'professional', 'development'],\n",
       " ['Discuss', 'contemporary', 'cryptography', 'application'],\n",
       " ['Apply', 'hash', 'function', 'algorithm'],\n",
       " ['Investigate',\n",
       "  'mathematical',\n",
       "  'concept',\n",
       "  'required',\n",
       "  'cryptographic',\n",
       "  'algorithm'],\n",
       " ['Analyze', 'security', 'threat', 'associated', 'cryptographic', 'algorithm'],\n",
       " ['Evaluate', 'various', 'cryptographic', 'technique'],\n",
       " ['Analyze', 'classic', 'cryptographic', 'algorithm'],\n",
       " ['Implement', 'symmetric', 'cryptography', 'technique'],\n",
       " ['Implement', 'public-key', 'cryptosystems', 'technique'],\n",
       " ['Compare',\n",
       "  'security',\n",
       "  'property',\n",
       "  'well-known',\n",
       "  'cryptographic',\n",
       "  'protocol'],\n",
       " ['Implement', 'IPSec', 'VPN', 'solution', 'small', 'network'],\n",
       " ['Describe', 'TCP/IP', 'protocol', 'common', 'network', 'service'],\n",
       " ['Explain', 'network', 'traffic', 'service', 'filtering', 'concept'],\n",
       " ['Compare', 'stateless', 'stateful', 'firewall'],\n",
       " ['Assess', 'firewall', 'filtering', 'rule', 'consistency', 'efficiency'],\n",
       " ['Discuss', 'VPN', 'secure', 'network', 'architecture'],\n",
       " ['Demonstrate', 'teamwork', 'skill'],\n",
       " ['Describe', 'Internet', 'e-commerce', 'security', 'protocol'],\n",
       " ['Analyze',\n",
       "  'vulnerability',\n",
       "  'associated',\n",
       "  'insecure',\n",
       "  'Internet',\n",
       "  'e-commerce',\n",
       "  'protocol'],\n",
       " ['Demonstrate',\n",
       "  'security',\n",
       "  'protocol',\n",
       "  'used',\n",
       "  'achieve',\n",
       "  'internet',\n",
       "  'e-commerce',\n",
       "  'security'],\n",
       " ['Communicate',\n",
       "  'issue',\n",
       "  'relevant',\n",
       "  'public-key',\n",
       "  'infrastructure',\n",
       "  '(',\n",
       "  'PKI',\n",
       "  ')'],\n",
       " ['Explain',\n",
       "  'concept',\n",
       "  'trust',\n",
       "  '(',\n",
       "  'trust',\n",
       "  'model',\n",
       "  ')',\n",
       "  'internet',\n",
       "  'using',\n",
       "  'secure',\n",
       "  'internet',\n",
       "  'protocol'],\n",
       " ['Discuss', 'common', 'security', 'issue', 'software'],\n",
       " ['Analyze', 'software', 'vulnerability'],\n",
       " ['Explain', 'secure', 'software', 'design', '&', 'development', 'technique'],\n",
       " ['Compare',\n",
       "  'various',\n",
       "  'software',\n",
       "  'security',\n",
       "  'testing',\n",
       "  'tool',\n",
       "  'technique'],\n",
       " ['Analyze', 'web', 'application', 'security', 'threat', 'countermeasure'],\n",
       " ['Discuss', 'system', 'security', 'architecture', 'concept', 'attribute'],\n",
       " ['Design', 'secure', 'information', 'system', 'using', 'OM-AM', 'framework'],\n",
       " ['Apply', 'security', 'access', 'control', 'model'],\n",
       " ['Analyze',\n",
       "  'security',\n",
       "  'threat',\n",
       "  'associated',\n",
       "  'information',\n",
       "  'system',\n",
       "  'infrastructure'],\n",
       " ['Select', 'security', 'countermeasure', 'solution'],\n",
       " ['Evaluate', 'biometric', 'solution', 'authentication', 'protocol'],\n",
       " ['Configure', 'security', 'feature', 'network', 'device'],\n",
       " ['Analyze', 'common', 'network', 'threat', 'attack', 'vector'],\n",
       " ['Apply', 'attack', 'mitigation', 'technique'],\n",
       " ['Implement',\n",
       "  'packet',\n",
       "  'filter',\n",
       "  ',',\n",
       "  'stateful',\n",
       "  'firewall',\n",
       "  ',',\n",
       "  'application',\n",
       "  'layer',\n",
       "  'firewall'],\n",
       " ['Analyze',\n",
       "  'security',\n",
       "  'incident',\n",
       "  'using',\n",
       "  'intrusion',\n",
       "  'detection',\n",
       "  'prevention',\n",
       "  'system'],\n",
       " ['Evaluate', 'operating', 'system', 'security', 'aspect'],\n",
       " ['Configure', 'biometric', 'security', 'system'],\n",
       " ['Implement', 'lattice-based', 'access', 'control', 'model'],\n",
       " ['Design', 'active', 'response', 'security', 'architecture'],\n",
       " ['Use', 'digital', 'forensics', 'hardware', 'software', 'tool'],\n",
       " ['Analyze',\n",
       "  'designing',\n",
       "  'issue',\n",
       "  'pertaining',\n",
       "  'information',\n",
       "  'security',\n",
       "  'policy'],\n",
       " ['Investigate', 'sociological', 'legal', 'issue', 'policy', 'implementation'],\n",
       " ['Apply',\n",
       "  'principle',\n",
       "  'philosophy',\n",
       "  'underlie',\n",
       "  'successful',\n",
       "  'information',\n",
       "  'security',\n",
       "  'governance'],\n",
       " ['Discuss',\n",
       "  'interaction',\n",
       "  'information',\n",
       "  'security',\n",
       "  'concern',\n",
       "  'business',\n",
       "  'objective'],\n",
       " ['Evaluate',\n",
       "  'information',\n",
       "  'security',\n",
       "  'activity',\n",
       "  'within',\n",
       "  'implementation',\n",
       "  'project'],\n",
       " ['Analyze', 'common', 'security', 'threat', 'network', 'attack'],\n",
       " ['Evaluate', 'IPS', 'attack', 'signature', 'rule'],\n",
       " ['Compare', 'appropriate', 'countermeasure', 'common', 'network', 'attack'],\n",
       " ['Discuss', 'system', 'security', 'auditing', 'vulnerability', 'assessment'],\n",
       " ['Demonstrate', 'teamwork', 'skill'],\n",
       " ['Investigate', 'privacy', 'concern', 'different', 'context'],\n",
       " ['Apply', 'privacy', 'protection', 'solution'],\n",
       " ['Analyze',\n",
       "  'shortcoming',\n",
       "  'existing',\n",
       "  'privacy',\n",
       "  'technology',\n",
       "  'challenge',\n",
       "  'privacy',\n",
       "  'protection'],\n",
       " ['Discuss',\n",
       "  'privacy',\n",
       "  'legislation',\n",
       "  ',',\n",
       "  'policy',\n",
       "  'best',\n",
       "  'practice',\n",
       "  'different',\n",
       "  'context'],\n",
       " ['Design',\n",
       "  'information',\n",
       "  'security',\n",
       "  'risk',\n",
       "  'management',\n",
       "  'program',\n",
       "  'organization'],\n",
       " ['Implement', 'information', 'security', 'risk', 'management', 'roadmap'],\n",
       " ['Manage',\n",
       "  'information',\n",
       "  'security',\n",
       "  'risk',\n",
       "  'assessment',\n",
       "  'consulting',\n",
       "  'contract'],\n",
       " ['Compare', 'risk', 'assessment', 'technique'],\n",
       " ['Discuss', 'law', 'affecting', 'digital', 'forensics'],\n",
       " ['Analyze', 'concept', 'computer', 'digital', 'forensics'],\n",
       " ['Evaluate', 'current', 'computer', 'digital', 'forensics', 'tool'],\n",
       " ['Conduct',\n",
       "  'software',\n",
       "  'hardware',\n",
       "  'based',\n",
       "  'digital',\n",
       "  'forensic',\n",
       "  'analysis'],\n",
       " ['Evaluate', 'web', 'cloud', 'based', 'digital', 'forensic', 'tool'],\n",
       " ['Assess', 'digital', 'forensics', 'mobile', 'device', 'application'],\n",
       " ['Discuss', 'security', 'management', 'policy', 'best', 'practice'],\n",
       " ['Analyze',\n",
       "  'implement',\n",
       "  'Business',\n",
       "  'Continuity',\n",
       "  'Planning',\n",
       "  '(',\n",
       "  'BCP',\n",
       "  ')',\n",
       "  'Disaster',\n",
       "  'Recovery',\n",
       "  'Planning',\n",
       "  '(',\n",
       "  'DRP',\n",
       "  ')'],\n",
       " ['Criticize', 'legal', 'ethical', 'implication', 'security', 'management'],\n",
       " ['Apply', 'risk', 'evaluation', 'mitigation', 'strategy'],\n",
       " ['Apply', 'ISMS', 'standard'],\n",
       " ['Analyze', 'risk', 'management', 'auditing', 'technique'],\n",
       " ['Assess', 'vulnerability', 'hardware', 'device', 'system'],\n",
       " ['Analyze', 'attack', 'hardware', 'system'],\n",
       " ['Integrate', 'hardware', 'security', 'measure', 'design', 'metric'],\n",
       " ['Apply',\n",
       "  'detection',\n",
       "  ',',\n",
       "  'prevention',\n",
       "  'isolation',\n",
       "  'method',\n",
       "  'hardware',\n",
       "  'attack'],\n",
       " ['Evaluate', 'security', 'trust', 'hardware', 'system'],\n",
       " ['Describe', 'database', 'security', 'model', 'architecture'],\n",
       " ['Analyze', 'security', 'vulnerability', 'database'],\n",
       " ['Apply', 'database', 'security', 'model', 'policy', 'modern', 'database'],\n",
       " ['Discuss',\n",
       "  'data',\n",
       "  'application',\n",
       "  'auditing',\n",
       "  'procedure',\n",
       "  'database',\n",
       "  'security'],\n",
       " ['Compare', 'various', 'database', 'security', 'defense', 'mechanism'],\n",
       " ['Evaluate', 'option', 'needed', 'security', 'solution'],\n",
       " ['Compare', 'various', 'security', 'mechanism'],\n",
       " ['Apply',\n",
       "  'current',\n",
       "  'security',\n",
       "  'solution',\n",
       "  'address',\n",
       "  'certain',\n",
       "  'requirement'],\n",
       " ['Analyze', 'performance', 'applied', 'security', 'solution'],\n",
       " ['Explain', 'basic', 'principle', 'data', 'mining', 'process'],\n",
       " ['Prepare', 'data', 'mining', 'exploration'],\n",
       " ['Use',\n",
       "  'data',\n",
       "  'mining',\n",
       "  'technique',\n",
       "  'modern',\n",
       "  'tool',\n",
       "  'discover',\n",
       "  'trend',\n",
       "  'pattern',\n",
       "  'realistic',\n",
       "  'datasets'],\n",
       " ['Evaluate',\n",
       "  'different',\n",
       "  'data',\n",
       "  'mining',\n",
       "  'models/techniques',\n",
       "  'respect',\n",
       "  'performance',\n",
       "  'accuracy'],\n",
       " ['Function', 'team', 'communicate', 'effectively', 'written', 'oral', 'form']]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Array of all the PLOs and ULOs (We can couple them together as we're trying to identify Bloom/Solo level here)\n",
    "lo_sentence_array = []\n",
    "\n",
    "# TODO: train CLO classification with all data instead of just one course.\n",
    "for table in document.tables:\n",
    "    for row in table.rows:\n",
    "        for cell in row.cells[1:]:\n",
    "            tokens = nltk.word_tokenize(cell.text)\n",
    "            cleaned_tokens = [lemmatizer.lemmatize(token) for token in tokens if token.lower() not in stop_words]\n",
    "            lo_sentence_array.append(cleaned_tokens)\n",
    "\n",
    "# build the vocabulary and train the model\n",
    "# IMPORTANT, N0TE THAT sg=1 flag specifies Word2Vec to use the Skip Gram Model as designated by the LSTM paper.\n",
    "model = Word2Vec(sentences=lo_sentence_array,vector_size=100, window=5, min_count=1, workers=4, sg=1)\n",
    "\n",
    "# train the model with the course's ULOs and PLOs.\n",
    "model.train([tokens], total_examples=len([tokens]), epochs=10)\n",
    "\n",
    "lo_sentence_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-17 18:45:19.188516: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-05-17 18:45:21.891780: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2023-05-17 18:45:21.891845: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2023-05-17 18:45:22.638161: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-05-17 18:45:27.290880: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-05-17 18:45:27.291157: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-05-17 18:45:27.291169: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "[nltk_data] Downloading package punkt to /home/edward/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/edward/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/edward/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "2023-05-17 18:45:30.144320: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:966] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-05-17 18:45:30.150437: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2023-05-17 18:45:30.150801: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcublas.so.11'; dlerror: libcublas.so.11: cannot open shared object file: No such file or directory\n",
      "2023-05-17 18:45:30.151011: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcublasLt.so.11'; dlerror: libcublasLt.so.11: cannot open shared object file: No such file or directory\n",
      "2023-05-17 18:45:30.151115: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcufft.so.10'; dlerror: libcufft.so.10: cannot open shared object file: No such file or directory\n",
      "2023-05-17 18:45:30.151224: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcurand.so.10'; dlerror: libcurand.so.10: cannot open shared object file: No such file or directory\n",
      "2023-05-17 18:45:30.151329: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusolver.so.11'; dlerror: libcusolver.so.11: cannot open shared object file: No such file or directory\n",
      "2023-05-17 18:45:30.151419: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusparse.so.11'; dlerror: libcusparse.so.11: cannot open shared object file: No such file or directory\n",
      "2023-05-17 18:45:30.151483: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudnn.so.8'; dlerror: libcudnn.so.8: cannot open shared object file: No such file or directory\n",
      "2023-05-17 18:45:30.151524: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1934] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n",
      "2023-05-17 18:45:30.153726: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import docx2txt\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Embedding\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "\n",
    "# load in the Word document using docx2txt\n",
    "doc_text = docx2txt.process(CURRENT_MAPPING)\n",
    "\n",
    "# preprocess the text using NLTK\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# tokenize the text and remove stop words and non-alphabetic characters\n",
    "tokens = [word.lower() for word in word_tokenize(doc_text) if word.isalpha() and word.lower() not in stop_words]\n",
    "\n",
    "# lemmatization\n",
    "lemmatized_tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "\n",
    "\n",
    "# convert tokens back to single string format\n",
    "corpus = ' '.join(lemmatized_tokens)\n",
    "\n",
    "# create a tokenizer and fit on the corpus\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts([corpus])\n",
    "\n",
    "# convert the text to a sequence of integers\n",
    "# sequences = tokenizer.texts_to_sequences([corpus])\n",
    "\n",
    "# pad the sequences to have a fixed length\n",
    "max_length = 50\n",
    "# padded_sequences = pad_sequences(sequences, maxlen=max_length, padding='post')\n",
    "\n",
    "# LSTM model\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "embedding_dim = 50\n",
    "\n",
    "model = Sequential([\n",
    "    tf.keras.layers.Embedding(10000, 16),\n",
    "    tf.keras.layers.GlobalAveragePooling1D(),\n",
    "    tf.keras.layers.Dense(16, activation='relu'),\n",
    "    tf.keras.layers.Dropout(0.2), # Dropout rate set to 0.2 as specified from the paper\n",
    "    tf.keras.layers.Dense(6, activation='softmax')\n",
    "])\n",
    "\n",
    "model.add(Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=max_length))\n",
    "model.add(LSTM(units=64, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(units=3, activation='softmax'))\n",
    "\n",
    "# RMS Optimizer as specified by the paper.\n",
    "optimizer = RMSprop(learning_rate=0.001)\n",
    "\n",
    "# compile the model\n",
    "model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# train the model\n",
    "# X_train and y_train are assumed to be already defined\n",
    "# \n",
    "# model.fit(X_train, y_train, epochs=10, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n",
      "{'<pad>': 0, 'the': 1, 'wide': 2, 'road': 3, 'shimmered': 4, 'in': 5, 'hot': 6, 'sun': 7}\n",
      "{0: '<pad>', 1: 'the', 2: 'wide', 3: 'road', 4: 'shimmered', 5: 'in', 6: 'hot', 7: 'sun'}\n",
      "[1, 2, 3, 4, 5, 1, 6, 7]\n",
      "26\n"
     ]
    }
   ],
   "source": [
    "import io\n",
    "import re\n",
    "import string\n",
    "import tqdm\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "## training data LO\n",
    "## sentence = \"use big data streaming technologies.\"\n",
    "## word = \"apply\"\n",
    "## categories = [\"Remembering\", \"Understanding\", \"Applying\", \"Analysing\", \"Evaluating\", \"Creating\"]\n",
    "\n",
    "\n",
    "\n",
    "# Load the TensorBoard notebook extension\n",
    "%load_ext tensorboard\n",
    "\n",
    "SEED = 42\n",
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "\n",
    "sentence = \"The wide road shimmered in the hot sun\"\n",
    "tokens = list(sentence.lower().split())\n",
    "print(len(tokens))\n",
    "\n",
    "vocab, index = {}, 1  # start indexing from 1\n",
    "vocab['<pad>'] = 0  # add a padding token\n",
    "for token in tokens:\n",
    "  if token not in vocab:\n",
    "    vocab[token] = index\n",
    "    index += 1\n",
    "vocab_size = len(vocab)\n",
    "print(vocab)\n",
    "\n",
    "inverse_vocab = {index: token for token, index in vocab.items()}\n",
    "print(inverse_vocab)\n",
    "\n",
    "example_sequence = [vocab[word] for word in tokens]\n",
    "print(example_sequence)\n",
    "\n",
    "window_size = 2\n",
    "positive_skip_grams, _ = tf.keras.preprocessing.sequence.skipgrams(\n",
    "      example_sequence,\n",
    "      vocabulary_size=vocab_size,\n",
    "      window_size=window_size,\n",
    "      negative_samples=0)\n",
    "print(len(positive_skip_grams))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-17 19:31:06.151393: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-05-17 19:31:06.300076: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2023-05-17 19:31:06.300133: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2023-05-17 19:31:06.320966: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-05-17 19:31:06.846371: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-05-17 19:31:06.846483: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-05-17 19:31:06.846498: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.8/runpy.py\", line 185, in _run_module_as_main\n",
      "    mod_name, mod_spec, code = _get_module_details(mod_name, _Error)\n",
      "  File \"/usr/lib/python3.8/runpy.py\", line 144, in _get_module_details\n",
      "    return _get_module_details(pkg_main_name, error)\n",
      "  File \"/usr/lib/python3.8/runpy.py\", line 111, in _get_module_details\n",
      "    __import__(pkg_name)\n",
      "  File \"/home/edward/.local/lib/python3.8/site-packages/spacy/__init__.py\", line 15, in <module>\n",
      "    from .cli.info import info  # noqa: F401\n",
      "  File \"/home/edward/.local/lib/python3.8/site-packages/spacy/cli/__init__.py\", line 3, in <module>\n",
      "    from ._util import app, setup_cli  # noqa: F401\n",
      "  File \"/home/edward/.local/lib/python3.8/site-packages/spacy/cli/_util.py\", line 9, in <module>\n",
      "    import typer\n",
      "  File \"/home/edward/.local/lib/python3.8/site-packages/typer/__init__.py\", line 12, in <module>\n",
      "    from click.termui import get_terminal_size as get_terminal_size\n",
      "ImportError: cannot import name 'get_terminal_size' from 'click.termui' (/home/edward/.local/lib/python3.8/site-packages/click/termui.py)\n",
      "bash: line 6: import: command not found\n",
      "bash: line 7: syntax error near unexpected token `\"en_core_web_sm\"'\n",
      "bash: line 7: `spacy.cli.download(\"en_core_web_sm\")'\n"
     ]
    },
    {
     "ename": "CalledProcessError",
     "evalue": "Command 'b'## packages to install\\npython3 -m spacy download en_core_web_sm\\n\\n# %%python\\n\\nimport spacy\\nspacy.cli.download(\"en_core_web_sm\")\\n'' returned non-zero exit status 2.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mCalledProcessError\u001b[0m                        Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[34], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m get_ipython()\u001b[39m.\u001b[39;49mrun_cell_magic(\u001b[39m'\u001b[39;49m\u001b[39mbash\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39m## packages to install\u001b[39;49m\u001b[39m\\n\u001b[39;49;00m\u001b[39mpython3 -m spacy download en_core_web_sm\u001b[39;49m\u001b[39m\\n\u001b[39;49;00m\u001b[39m\\n\u001b[39;49;00m\u001b[39m# \u001b[39;49m\u001b[39m%%\u001b[39;49;00m\u001b[39mpython\u001b[39;49m\u001b[39m\\n\u001b[39;49;00m\u001b[39m\\n\u001b[39;49;00m\u001b[39mimport spacy\u001b[39;49m\u001b[39m\\n\u001b[39;49;00m\u001b[39mspacy.cli.download(\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39men_core_web_sm\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m)\u001b[39;49m\u001b[39m\\n\u001b[39;49;00m\u001b[39m'\u001b[39;49m)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/IPython/core/interactiveshell.py:2475\u001b[0m, in \u001b[0;36mInteractiveShell.run_cell_magic\u001b[0;34m(self, magic_name, line, cell)\u001b[0m\n\u001b[1;32m   2473\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbuiltin_trap:\n\u001b[1;32m   2474\u001b[0m     args \u001b[39m=\u001b[39m (magic_arg_s, cell)\n\u001b[0;32m-> 2475\u001b[0m     result \u001b[39m=\u001b[39m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   2477\u001b[0m \u001b[39m# The code below prevents the output from being displayed\u001b[39;00m\n\u001b[1;32m   2478\u001b[0m \u001b[39m# when using magics with decodator @output_can_be_silenced\u001b[39;00m\n\u001b[1;32m   2479\u001b[0m \u001b[39m# when the last Python token in the expression is a ';'.\u001b[39;00m\n\u001b[1;32m   2480\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mgetattr\u001b[39m(fn, magic\u001b[39m.\u001b[39mMAGIC_OUTPUT_CAN_BE_SILENCED, \u001b[39mFalse\u001b[39;00m):\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/IPython/core/magics/script.py:153\u001b[0m, in \u001b[0;36mScriptMagics._make_script_magic.<locals>.named_script_magic\u001b[0;34m(line, cell)\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    152\u001b[0m     line \u001b[39m=\u001b[39m script\n\u001b[0;32m--> 153\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mshebang(line, cell)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/IPython/core/magics/script.py:305\u001b[0m, in \u001b[0;36mScriptMagics.shebang\u001b[0;34m(self, line, cell)\u001b[0m\n\u001b[1;32m    300\u001b[0m \u001b[39mif\u001b[39;00m args\u001b[39m.\u001b[39mraise_error \u001b[39mand\u001b[39;00m p\u001b[39m.\u001b[39mreturncode \u001b[39m!=\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m    301\u001b[0m     \u001b[39m# If we get here and p.returncode is still None, we must have\u001b[39;00m\n\u001b[1;32m    302\u001b[0m     \u001b[39m# killed it but not yet seen its return code. We don't wait for it,\u001b[39;00m\n\u001b[1;32m    303\u001b[0m     \u001b[39m# in case it's stuck in uninterruptible sleep. -9 = SIGKILL\u001b[39;00m\n\u001b[1;32m    304\u001b[0m     rc \u001b[39m=\u001b[39m p\u001b[39m.\u001b[39mreturncode \u001b[39mor\u001b[39;00m \u001b[39m-\u001b[39m\u001b[39m9\u001b[39m\n\u001b[0;32m--> 305\u001b[0m     \u001b[39mraise\u001b[39;00m CalledProcessError(rc, cell)\n",
      "\u001b[0;31mCalledProcessError\u001b[0m: Command 'b'## packages to install\\npython3 -m spacy download en_core_web_sm\\n\\n# %%python\\n\\nimport spacy\\nspacy.cli.download(\"en_core_web_sm\")\\n'' returned non-zero exit status 2."
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "## packages to install\n",
    "\n",
    "pip install gensim\n",
    "pip install spacy\n",
    "python3 -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "spacy.cli.download(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "## This code loads the vector file into the word_vectors variable\n",
    "## Download the vector file from https://fasttext.cc/docs/en/english-vectors.html (first file on the website), unzip the file and store in your local development folder\n",
    "## Note: This piece of code may take upto an hour or two to run depending on your pc specs.\n",
    "## My i5 8th gen with 8gig ram took 58mins to run.\n",
    "\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "# Path to the downloaded .vec file\n",
    "path_to_vectors = 'wiki-news-300d-1M.vec'\n",
    "# Load the word vectors\n",
    "word_vectors = KeyedVectors.load_word2vec_format(path_to_vectors)\n",
    "\n",
    "# Find similar words\n",
    "similar_words = word_vectors.most_similar('cat')\n",
    "\n",
    "# Calculate word similarity\n",
    "similarity = word_vectors.similarity('cat', 'dog')\n",
    "\n",
    "# Perform vector arithmetic\n",
    "result = word_vectors['king'] - word_vectors['man'] + word_vectors['woman']\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tensorboard extension is already loaded. To reload it, use:\n",
      "  %reload_ext tensorboard\n"
     ]
    }
   ],
   "source": [
    "import io\n",
    "import re\n",
    "import string\n",
    "import tqdm\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "## training data LO\n",
    "## sentence = \"use big data streaming technologies.\"\n",
    "## word = \"apply\"\n",
    "## categories = [\"Remembering\", \"Understanding\", \"Applying\", \"Analysing\", \"Evaluating\", \"Creating\"]\n",
    "\n",
    "\n",
    "\n",
    "# Load the TensorBoard notebook extension\n",
    "%load_ext tensorboard\n",
    "\n",
    "SEED = 42\n",
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "\n",
    "\n",
    "def generate_skipgrams(sentence):\n",
    "    tokens = list(sentence.lower().split())\n",
    "    print(len(tokens))\n",
    "\n",
    "    vocab, index = {}, 1  # start indexing from 1\n",
    "    vocab['<pad>'] = 0  # add a padding token\n",
    "    for token in tokens:\n",
    "      if token not in vocab:\n",
    "        vocab[token] = index\n",
    "        index += 1\n",
    "    vocab_size = len(vocab)\n",
    "    print(vocab)\n",
    "\n",
    "    inverse_vocab = {index: token for token, index in vocab.items()}\n",
    "    print(inverse_vocab)\n",
    "\n",
    "    example_sequence = [vocab[word] for word in tokens]\n",
    "    print(example_sequence)\n",
    "\n",
    "    window_size = 2\n",
    "    positive_skip_grams, _ = tf.keras.preprocessing.sequence.skipgrams(\n",
    "          example_sequence,\n",
    "          vocabulary_size=vocab_size,\n",
    "          window_size=window_size,\n",
    "          negative_samples=0)\n",
    "    \n",
    "    return positive_skip_grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "## Function to identify verbs in a sentence\n",
    "def identify_verbs(sentence):\n",
    "    # Load the English language model in spaCy\n",
    "    nlp = spacy.load('en_core_web_sm')\n",
    "    \n",
    "    # Process the sentence using spaCy\n",
    "    doc = nlp(sentence)\n",
    "    \n",
    "    # Extract the verbs from the processed sentence\n",
    "    verbs = [token.lemma_ for token in doc if token.pos_ == 'VERB']\n",
    "    \n",
    "    return verbs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['apply']\n",
      "Sentence:  apply common data analytics and machine learning algorithms in a big data environment.  Identified blooms level:  Applying\n",
      "['use']\n",
      "Sentence:  use big data streaming technologies.  Identified blooms level:  Applying\n"
     ]
    }
   ],
   "source": [
    "## Main piece of code that performs the mapping \n",
    "\n",
    "sentences = [\n",
    "    \"apply common data analytics and machine learning algorithms in a big data environment.\",\n",
    "    \"use big data streaming technologies.\"\n",
    "]\n",
    "bloom_levels = [\"Remembering\", \"Understanding\", \"Applying\", \"Analysing\", \"Evaluating\", \"Creating\"]\n",
    "\n",
    "# identified_levels = []\n",
    "final_level = None\n",
    "\n",
    "for i in range(len(sentences)):\n",
    "    verbs = identify_verbs(sentences[i])\n",
    "    print(verbs)\n",
    "    score = 0\n",
    "    for j in range(len(verbs)):\n",
    "        for k in range(len(bloom_levels)):\n",
    "            similarity_score = word_vectors.similarity(verbs[j], bloom_levels[k])\n",
    "            if similarity_score >= score:\n",
    "                score=similarity_score\n",
    "                final_level = bloom_levels[k]\n",
    "    print(\"Sentence: \", sentences[i], \" Identified blooms level: \", final_level)\n",
    "\n",
    "\n",
    "### Todos\n",
    "# Find a way to use skipgrams\n",
    "# This method only works for blooms since this paper is only based on blooms mapping\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
