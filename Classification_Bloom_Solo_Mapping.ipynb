{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "# Dependencies\n",
    "pip install docx2txt\n",
    "pip install gensim\n",
    "pip install keras\n",
    "pip install nltk\n",
    "pip install -U scikit-learn\n",
    "pip install python-docx\n",
    "pip install tensorflow\n",
    "pip install pandas\n",
    "pip install openpyxl\n",
    "pip install nltk\n",
    "pip install spacy\n",
    "pip install csv\n",
    "python3 -m spacy download en_core_web_sm\n",
    "python3 -m spacy download en_core_web_trf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.test.utils import common_texts\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "import pandas as pd\n",
    "import csv\n",
    "import math\n",
    "import pprint\n",
    "import re\n",
    "\n",
    "import spacy\n",
    "# Use if en_core_web_sm not installable via python3 in terminal:\n",
    "# spacy.cli.download(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def verb_classifier(verbs_file_path): # verbs_file_path - string value that contains the file path\n",
    "\n",
    "    xlsx = pd.ExcelFile(verbs_file_path, engine='openpyxl') \n",
    "\n",
    "    sheet_names = xlsx.sheet_names  # Get a list of sheet names\n",
    "\n",
    "    # Create an empty dictionary to store DataFrames for each sheet\n",
    "    dfs = {}\n",
    "\n",
    "    for sheet_name in sheet_names:\n",
    "        df = xlsx.parse(sheet_name)  # For XLSX files\n",
    "        \n",
    "        # Store the DataFrame in the dictionary\n",
    "        dfs[sheet_name] = df\n",
    "\n",
    "    domain_levels = pd.concat(dfs)\n",
    "    duplicate_checklist = []\n",
    "\n",
    "    for i in range(domain_levels.shape[0]):\n",
    "        for j in range(domain_levels.shape[1]):\n",
    "            cell_value = domain_levels.iloc[i, j]\n",
    "            if not pd.isna(cell_value):\n",
    "                cell_value_lower = cell_value.lower()\n",
    "                verb = lemmatizer.lemmatize(cell_value_lower, pos=\"v\")\n",
    "                if verb not in duplicate_checklist:\n",
    "                    domain_levels.iloc[i, j] = verb\n",
    "                    duplicate_checklist.append(verb)\n",
    "                else:\n",
    "                    domain_levels.iloc[i, j] = float('nan')\n",
    "\n",
    "    domain_levels = domain_levels.dropna(how='all')\n",
    "    return domain_levels\n",
    "\n",
    "\n",
    "# Paths\n",
    "solo_file_path = 'SOLO.xlsx'\n",
    "bloom_cognitive_file_path = 'Bloom_cognitive.xlsx'\n",
    "bloom_psychomotor_file_path = 'Bloom_psychomotor.xlsx'\n",
    "bloom_affective_file_path = 'Bloom_affective.xlsx'\n",
    "\n",
    "# Verbs\n",
    "mapped_verbs = {\n",
    "    \"Cognitive\": verb_classifier(bloom_cognitive_file_path),\n",
    "    \"Affective\": verb_classifier(bloom_affective_file_path),\n",
    "    \"Psychomotor\": verb_classifier(bloom_psychomotor_file_path),\n",
    "    \"SOLO\": verb_classifier(solo_file_path)\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Check the dataframe for duplicates\n",
    "\n",
    "# verbs_df - The dataframe that consists of the classified verbs\n",
    "# domain_name - string name of the domain. (SOLO, Blooms Cognitive, Blooms Affective or Blooms Psychomotor)\n",
    "def check_duplicates(verbs_df, domain_name): \n",
    "\n",
    "    ## Checking duplicates for SOLO\n",
    "    list_of_lists = verbs_df.values.tolist()\n",
    "    merged_list = [item for sublist in list_of_lists for item in sublist]\n",
    "\n",
    "    # Remove nan\n",
    "    cleaned_list = list(filter(lambda x: not pd.isna(x), merged_list))\n",
    "\n",
    "    # Check for duplicate\n",
    "    if len(cleaned_list) != len(set(cleaned_list)):\n",
    "        print(\"Duplicates Found in \" + domain_name)\n",
    "        print(sorted(cleaned_list))\n",
    "    else:\n",
    "        print(\"No duplicates exist in \" + domain_name)\n",
    "    \n",
    "    list_of_lists.clear()\n",
    "    merged_list.clear()\n",
    "    cleaned_list.clear()\n",
    "\n",
    "\n",
    "# Check for duplicates\n",
    "for taxonomy_key, taxonomy_item in mapped_verbs.items():\n",
    "    check_duplicates(taxonomy_item, taxonomy_key)\n",
    "    taxonomy_item.to_csv(\"./outputs/mapped_\" + taxonomy_key + \".csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the English language model in spaCy\n",
    "# nlp = spacy.load('en_core_web_sm')\n",
    "nlp = spacy.load('en_core_web_trf', exclude=['ner'])\n",
    "\n",
    "## Function to identify verbs in a sentence\n",
    "def identify_verbs(sentence):\n",
    "    # Process the sentence using spaCy\n",
    "    doc = nlp(sentence)\n",
    "    \n",
    "    # Extract the verbs from the processed sentence\n",
    "    verbs = [token.lemma_ for token in doc if token.pos_ == 'VERB']\n",
    "    \n",
    "    return verbs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_columns(csv_file, columns):\n",
    "    extracted_data = {}\n",
    "    \n",
    "    with open(csv_file, 'r') as file:\n",
    "        reader = csv.reader(file)\n",
    "        headers = next(reader)  # Read the headers\n",
    "        \n",
    "        # Check if all specified columns exist in the CSV file\n",
    "        for column in columns:\n",
    "            if column not in headers:\n",
    "                raise ValueError(f\"Column '{column}' not found in the CSV file.\")\n",
    "        \n",
    "        # Initialize separate arrays for each column\n",
    "        for column in columns:\n",
    "            extracted_data[column] = []\n",
    "        \n",
    "        # Extract data from specified columns\n",
    "        for row in reader:\n",
    "            for column in columns:\n",
    "                column_index = headers.index(column)\n",
    "                extracted_data[column].append(row[column_index])\n",
    "    \n",
    "    return extracted_data\n",
    "\n",
    "# Example usage\n",
    "# csv_file = 'Learning outcomes manual mapping - Mappings.csv'\n",
    "csv_file = 'Learning outcomes manual mapping - Mappings.csv'\n",
    "# columns_to_extract = ['Learning outcomes', 'Final Bloom Level', 'Final SOLO Level']\n",
    "columns_to_extract = ['LO', 'Cognitive', 'Affective', 'Psychomotor', 'SOLO']\n",
    "extracted_data = extract_columns(csv_file, columns_to_extract)\n",
    "sentences = extracted_data['LO']\n",
    "final_levels = {\n",
    "    \"Cognitive\": extracted_data['Cognitive'],\n",
    "    \"Affective\": extracted_data['Affective'],\n",
    "    \"Psychomotor\": extracted_data['Psychomotor'],\n",
    "    \"SOLO\": extracted_data['SOLO']\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Main piece of code that performs the mapping - Approach 2 - 12% accuracy\n",
    "\n",
    "# wiki word vectors no uppercase\n",
    "# TODO: Modify this section of the code to use the bloom level verbs from Arragon's spreadsheet, will also need to modify\n",
    "# Ideas for improving accuracy\n",
    "#### Reduce the number of verbs.\n",
    "#### Take more learning outcomes from the monash handbook website(need big dataset for this part) and identify verbs that are appearing multiple times \n",
    "#### The nummber of times that it appears could be set to a certain number ex: 5. \n",
    "#### If the verb doesnt appear atleast 5 times, we could remove the verb from our list of predefined verbs which will result in a shorter verb list\n",
    "\n",
    "output_bloom_levels = []\n",
    "\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"stopwords\")\n",
    "nltk.download(\"wordnet\")\n",
    "\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "# Array of all the PLOs and ULOs (We can couple them together as we're trying to identify Bloom/Solo level here)\n",
    "lo_sentence_array = []\n",
    "bloom_levels_str = [\n",
    "    \"Remembering\",\n",
    "    \"Understanding\",\n",
    "    \"Applying\",\n",
    "    \"Analysing\",\n",
    "    \"Evaluating\",\n",
    "    \"Creating\",\n",
    "]\n",
    "\n",
    "# TODO: train CLO classification with all data instead of just one course.\n",
    "for sentence in sentences:\n",
    "    sentence = re.sub(r'[^\\w\\s]', '', sentence)\n",
    "    # Process the sentence using spaCy\n",
    "    doc = nlp(sentence)\n",
    "    \n",
    "    # Extract the verbs from the processed sentence\n",
    "    cleaned_tokens = [token.lemma_ for token in doc if token.lower_ not in stop_words]\n",
    "    lo_sentence_array.append(cleaned_tokens)\n",
    "\n",
    "# build the vocabulary and train the model\n",
    "# IMPORTANT, N0TE THAT sg=1 flag specifies Word2Vec to use the Skip Gram Model as designated by the LSTM paper.\n",
    "model = Word2Vec(\n",
    "    sentences=lo_sentence_array, vector_size=100, window=5, min_count=1, workers=4, sg=1, epochs=30\n",
    ")\n",
    "\n",
    "word_vectors = model.wv\n",
    "# train the model with the course's ULOs and PLOs.\n",
    "# model.train([tokens], total_examples=len([tokens]), epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bloom_mapping(sentences, final_levels):\n",
    "    passed_mappings = {\n",
    "            \"Cognitive\": 0,\n",
    "            \"Affective\": 0,\n",
    "            \"Psychomotor\": 0,\n",
    "            \"SOLO\": 0\n",
    "        }\n",
    "    failed_mappings = {\n",
    "            \"Cognitive\": 0,\n",
    "            \"Affective\": 0,\n",
    "            \"Psychomotor\": 0,\n",
    "            \"SOLO\": 0\n",
    "        }\n",
    "    failed_cases = []\n",
    "    min_score = 1\n",
    "\n",
    "    for i in range(len(sentences)): # Iterates over the LOs\n",
    "        identified_verbs = identify_verbs(sentences[i])\n",
    "\n",
    "        score_list = {\n",
    "            \"Cognitive\": {\n",
    "                \"Remembering\": 0,\n",
    "                \"Understanding\": 0,\n",
    "                \"Applying\": 0,\n",
    "                \"Analysing\": 0,\n",
    "                \"Evaluating\": 0,\n",
    "                \"Creating\": 0\n",
    "            },\n",
    "            \"Affective\": {\n",
    "                \"Receiving\": 0,\n",
    "                \"Responding\": 0,\n",
    "                \"Valuing\": 0,\n",
    "                \"Organisation\": 0,\n",
    "                \"Characterisation\": 0\n",
    "            },\n",
    "            \"Psychomotor\": {\n",
    "                \"Perception\": 0,\n",
    "                \"Set\": 0,\n",
    "                \"Guided Response\": 0,\n",
    "                \"Mechanism\": 0,\n",
    "                \"Complex Overt Response\": 0,\n",
    "                \"Adaptation\": 0,\n",
    "                \"Origination\": 0    \n",
    "            },\n",
    "            \"SOLO\": {\n",
    "                \"Prestructural\": 0,\n",
    "                \"Unistructural\": 0,\n",
    "                \"Multistructural\": 0,\n",
    "                \"Relational\": 0,\n",
    "                \"Extended Abstract\": 0\n",
    "            }\n",
    "        }\n",
    "\n",
    "        for taxonomy_key, taxonomy_item in mapped_verbs.items():\n",
    "            # if final_levels[taxonomy_key][i] is None or not final_levels[taxonomy_key][i] or final_levels[taxonomy_key][i] == '-': continue\n",
    "            for j in range(taxonomy_item.shape[0]): # Level\n",
    "                for k in range(taxonomy_item.shape[1]): # Verb\n",
    "                    verb = taxonomy_item.iloc[j, k]\n",
    "                    \n",
    "                    if verb is None or not verb or pd.isna(verb): continue\n",
    "\n",
    "                    similarity_score = 0\n",
    "                    for l in range(len(identified_verbs)):\n",
    "                        try:    # Currently some of the 'verbs' identified are phrases rather than words and it was throwing errors so this is a temp solution \n",
    "                            sim_score = word_vectors.similarity(identified_verbs[l], verb)\n",
    "                            if sim_score > 0.9999: similarity_score += sim_score\n",
    "                        except:\n",
    "                            pass\n",
    "                    score_list[taxonomy_key][taxonomy_item.columns[k]] += similarity_score\n",
    "\n",
    "        # Identify level based on similarity\n",
    "        max_score = {\n",
    "            \"Cognitive\": { \"Level\": None, \"Score\": 0 },\n",
    "            \"Affective\": { \"Level\": None, \"Score\": 0 },\n",
    "            \"Psychomotor\": { \"Level\": None, \"Score\": 0 },\n",
    "            \"SOLO\": { \"Level\": None, \"Score\": 0 }\n",
    "        }\n",
    "        for t_key, t_item in score_list.items():\n",
    "            for l in t_item:\n",
    "                if max_score[t_key][\"Score\"] < score_list[t_key][l]:\n",
    "                    max_score[t_key] = { \"Level\": l, \"Score\": score_list[t_key][l] }\n",
    "\n",
    "        for t_key, t_item in final_levels.items():\n",
    "            if final_levels[t_key][i] is None or not final_levels[t_key][i] or final_levels[t_key][i] == '-': continue\n",
    "            \n",
    "            if max_score[t_key][\"Level\"] != None and final_levels[t_key][i].lower() == max_score[t_key][\"Level\"].lower():\n",
    "                passed_mappings[t_key] += 1\n",
    "            else:\n",
    "                failed_mappings[t_key] += 1\n",
    "                sentence_data = {\n",
    "                    \"Domain\": t_key,\n",
    "                    \"manually identified level\": final_levels[t_key][i],\n",
    "                    \"automatically identified level\": max_score[t_key][\"Level\"],\n",
    "                    \"verbs identified\": identified_verbs,\n",
    "                    \"sentence\": sentences[i],\n",
    "                }\n",
    "                failed_cases.append(sentence_data)\n",
    "\n",
    "    # for case in failed_cases:\n",
    "    #     print(case)\n",
    "    with open('./outputs/failed_lo_mappings.csv', 'w', newline='') as file: \n",
    "        writer = csv.DictWriter(file, fieldnames = sentence_data.keys())\n",
    "        writer.writeheader()\n",
    "        writer.writerows(failed_cases)\n",
    "\n",
    "\n",
    "\n",
    "    total_passed = 0\n",
    "    total_failed = 0\n",
    "    for taxonomy, passed in passed_mappings.items():\n",
    "        total_passed += passed\n",
    "        total_failed += failed_mappings[taxonomy]\n",
    "        tot = passed + failed_mappings[taxonomy] if passed + failed_mappings[taxonomy] > 0 else 1\n",
    "        mapping_percentage = math.ceil((passed/(tot))*100)\n",
    "        print(\"Percentage of \", taxonomy, \" mappings passed: \", mapping_percentage, \"%\")\n",
    "        \n",
    "    total_mapping_percentage = math.ceil((total_passed/(total_passed + total_failed))*100)\n",
    "    print(\"Total percentage of mappings passed: \", total_mapping_percentage, \"%\")\n",
    "    print(min_score)\n",
    "    pass\n",
    "\n",
    "\n",
    "### Todos\n",
    "# Find a way to use skipgrams\n",
    "# This method only works for blooms since this paper is only based on blooms mapping\n",
    "\n",
    "bloom_mapping(sentences, final_levels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_keys_by_value(dictionary, target_value):\n",
    "    key_value = \"\"\n",
    "    for key, value in dictionary.items():\n",
    "        if value == target_value:\n",
    "           key_value = key\n",
    "    \n",
    "    if key_value == \"\":\n",
    "        key_value = list(dictionary.keys())[-1]\n",
    "    return key_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "solo = mapped_verbs['SOLO']\n",
    "\n",
    "## Rankings are manually derived using the followinng sources\n",
    "## https://davenport.libguides.com/learningoutcomes/domains#:~:text=Bloom%20identified%20three%20domains%2C%20or,Psychomotor%20Skills%20or%20Physical%20Skills\n",
    "## https://www.vectorsolutions.com/resources/blogs/teaching-skills-the-psychomotor-domain-of-learning-and-learning-objectives/\n",
    "\n",
    "solo_levels_ranks = {\n",
    "    \"Prestructural\": 0,\n",
    "    \"Unistructural\": 1,\n",
    "    \"Multistructural\": 2,\n",
    "    \"Relational\": 3,\n",
    "    \"Extended Abstract\": 4\n",
    "}\n",
    "\n",
    "bloom_cognitive_levels_ranks = {\n",
    "    \"Remembering\": 0,\n",
    "    \"Understanding\": 0,\n",
    "    \"Applying\": 1,\n",
    "    \"Analysing\": 2,\n",
    "    \"Evaluating\": 3,\n",
    "    \"Creating\": 4\n",
    "}\n",
    "\n",
    "bloom_affective_levels_ranks = {\n",
    "    \"Receiving\": 0,\n",
    "    \"Responding\": 1,\n",
    "    \"Valuing\": 2,\n",
    "    \"Organisation\": 3,\n",
    "    \"Characterisation\": 4\n",
    "}\n",
    "\n",
    "bloom_psychomotor_levels_ranks = {\n",
    "    \"Perception\": 0,\n",
    "    \"Set\": 0,\n",
    "    \"Guided Response\": 1,\n",
    "    \"Mechanism\": 2,\n",
    "    \"Complex Overt Response\": 3,\n",
    "    \"Adaptation\": 3,\n",
    "    \"Origination\": 4\n",
    "}\n",
    "\n",
    "# bloom_cognitive_copy = bloom_cognitive.copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def verb_ranking_identifier(current_taxonomy, comparison_taxonomy, verb, current_taxonomy_ranking_table, comparison_taxonomy_ranking_table):\n",
    "    if verb in comparison_taxonomy.values:\n",
    "        current_taxonomy_level = current_taxonomy[current_taxonomy == verb].stack().index[0][1]\n",
    "        comparison_taxonomy_level = comparison_taxonomy[comparison_taxonomy == verb].stack().index[0][1]\n",
    "\n",
    "        current_taxonomy_rank = current_taxonomy_ranking_table[current_taxonomy_level]\n",
    "        comparison_taxonomy_rank = comparison_taxonomy_ranking_table[comparison_taxonomy_level]\n",
    "        if verb=='relate':\n",
    "            print(\"fail\")\n",
    "\n",
    "        ranking_range = current_taxonomy_rank - comparison_taxonomy_rank\n",
    "\n",
    "\n",
    "        if abs(ranking_range) > 1:\n",
    "            # print(verb + '-' + current_taxonomy_level + '-' + str(current_taxonomy_rank) + '-' + str(comparison_taxonomy_rank))\n",
    "            new_current_taxonomy_rank = (current_taxonomy_rank + comparison_taxonomy_rank) // 2\n",
    "            new_current_taxonomy_level = get_keys_by_value(current_taxonomy_ranking_table, new_current_taxonomy_rank)\n",
    "            new_comparison_taxonomy_level = get_keys_by_value(comparison_taxonomy_ranking_table, new_current_taxonomy_rank)\n",
    "            current_taxonomy_dict = {\n",
    "                verb: new_current_taxonomy_level\n",
    "            }\n",
    "            comparison_taxonomy_dict = {\n",
    "                verb: new_comparison_taxonomy_level\n",
    "            }\n",
    "            current_taxonomy_modifications.append(current_taxonomy_dict)\n",
    "            comparison_taxonomy_modifications.append(comparison_taxonomy_dict)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def bloom_cog_remapping():\n",
    "    global current_taxonomy_modifications\n",
    "    current_taxonomy_modifications = []\n",
    "    global comparison_taxonomy_modifications\n",
    "    comparison_taxonomy_modifications = []\n",
    "\n",
    "    global solo_copy \n",
    "    solo_copy = solo.copy(deep=True)\n",
    "    solo_copy.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    global bloom_cognitive_copy\n",
    "    bloom_cognitive_copy = mapped_verbs['Cognitive']\n",
    "    bloom_cognitive_copy.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    global bloom_affective_copy\n",
    "    bloom_affective = mapped_verbs['Affective']\n",
    "    bloom_affective_copy = bloom_affective.copy(deep=True)\n",
    "    bloom_affective_copy.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    global bloom_psychomotor_copy\n",
    "    bloom_psychomotor = mapped_verbs['Psychomotor']\n",
    "    bloom_psychomotor_copy = bloom_psychomotor.copy(deep=True)\n",
    "    bloom_psychomotor_copy.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    # Check for solo verbs against other taxonomies. Identify new levels to put verbs into\n",
    "    for i in range(bloom_cognitive_copy.shape[0]): # Level\n",
    "        for j in range(bloom_cognitive_copy.shape[1]): # Verb\n",
    "            verb = bloom_cognitive_copy.iloc[i, j]\n",
    "            \n",
    "            if verb is None or not verb or pd.isna(verb): continue\n",
    "\n",
    "            verb_ranking_identifier(bloom_cognitive_copy, solo_copy, verb, bloom_cognitive_levels_ranks, solo_levels_ranks)\n",
    "            verb_ranking_identifier(bloom_cognitive_copy, bloom_affective_copy, verb, bloom_cognitive_levels_ranks, bloom_affective_levels_ranks)\n",
    "            verb_ranking_identifier(bloom_cognitive_copy, bloom_psychomotor_copy, verb, bloom_cognitive_levels_ranks, bloom_psychomotor_levels_ranks)\n",
    "\n",
    "\n",
    "    for i in range(len(current_taxonomy_modifications)):\n",
    "        current_item = current_taxonomy_modifications[i]\n",
    "        verb = list(current_item.keys())[0]\n",
    "        level = list(current_item.values())[0]\n",
    "        row, col = bloom_cognitive_copy[bloom_cognitive_copy == verb].stack().index[0]\n",
    "        bloom_cognitive_copy.at[row, col] = None\n",
    "        temp_df = pd.DataFrame({\n",
    "            level: verb\n",
    "        }, index=[0])\n",
    "        bloom_cognitive_copy = pd.concat([bloom_cognitive_copy, temp_df], ignore_index=True)\n",
    "\n",
    "\n",
    "    for i in range(len(comparison_taxonomy_modifications)):\n",
    "        current_item = comparison_taxonomy_modifications[i]\n",
    "        verb = list(current_item.keys())[0]\n",
    "        level = list(current_item.values())[0]\n",
    "        temp_df = pd.DataFrame({\n",
    "            level: verb\n",
    "        }, index=[0])\n",
    "        if level in solo_copy.columns:\n",
    "            row, col = solo_copy[solo_copy == verb].stack().index[0]\n",
    "            solo_copy.at[row, col] = None\n",
    "            solo_copy = pd.concat([solo_copy, temp_df], ignore_index=True)\n",
    "        elif level in bloom_affective_copy.columns:\n",
    "            row, col = bloom_affective_copy[bloom_affective_copy == verb].stack().index[0]\n",
    "            bloom_affective_copy.at[row, col] = None\n",
    "            bloom_affective_copy = pd.concat([bloom_affective_copy, temp_df], ignore_index=True)\n",
    "        elif level in bloom_psychomotor_copy.columns:\n",
    "            row, col = bloom_psychomotor_copy[bloom_psychomotor_copy == verb].stack().index[0]\n",
    "            bloom_psychomotor_copy.at[row, col] = None\n",
    "            bloom_psychomotor_copy = pd.concat([bloom_psychomotor_copy, temp_df], ignore_index=True)\n",
    "        else:\n",
    "            print(level)\n",
    "            print('level doesnt exist')\n",
    "\n",
    "bloom_cog_remapping()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def check_verb_rankings(current_taxonomy, comparison_taxonomy, verb, current_taxonomy_ranking_table, comparison_taxonomy_ranking_table):\n",
    "    if verb in comparison_taxonomy.values:\n",
    "        current_taxonomy_level = current_taxonomy[current_taxonomy == verb].stack().index[0][1]\n",
    "        comparison_taxonomy_level = comparison_taxonomy[comparison_taxonomy == verb].stack().index[0][1]\n",
    "\n",
    "        current_taxonomy_rank = current_taxonomy_ranking_table[current_taxonomy_level]\n",
    "        comparison_taxonomy_rank = comparison_taxonomy_ranking_table[comparison_taxonomy_level]\n",
    "\n",
    "        ranking_range = current_taxonomy_rank - comparison_taxonomy_rank\n",
    "\n",
    "\n",
    "        if abs(ranking_range) > 1:\n",
    "            print(verb + '-' + current_taxonomy_level + '-' + str(current_taxonomy_rank) + '-' + str(comparison_taxonomy_rank) + '-' + comparison_taxonomy_level)\n",
    "\n",
    "\n",
    "        \n",
    "for i in range(bloom_cognitive_copy.shape[0]): # Level\n",
    "    for j in range(bloom_cognitive_copy.shape[1]): # Verb\n",
    "        verb = bloom_cognitive_copy.iloc[i, j]\n",
    "        \n",
    "        if verb is None or not verb or pd.isna(verb): continue\n",
    "        \n",
    "        check_verb_rankings(bloom_cognitive_copy, solo_copy, verb, bloom_cognitive_levels_ranks, solo_levels_ranks)\n",
    "        check_verb_rankings(bloom_cognitive_copy, bloom_affective_copy, verb, bloom_cognitive_levels_ranks, bloom_affective_levels_ranks)\n",
    "        check_verb_rankings(bloom_cognitive_copy, bloom_psychomotor_copy, verb, bloom_cognitive_levels_ranks, bloom_psychomotor_levels_ranks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "solo_copy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_nan_values(taxonomy_verb_df):\n",
    "    dict = {}\n",
    "    for i in range(taxonomy_verb_df.shape[0]): # Level\n",
    "        for j in range(taxonomy_verb_df.shape[1]): # Verb\n",
    "            verb = taxonomy_verb_df.iloc[i, j]\n",
    "\n",
    "            if verb is None or not verb or pd.isna(verb): continue\n",
    "\n",
    "            verb_level = taxonomy_verb_df[taxonomy_verb_df == verb].stack().index[0][1]\n",
    "            dict[verb_level] = verb\n",
    "\n",
    "    \n",
    "    new_df = pd.DataFrame(dict)\n",
    "    return new_df\n",
    "\n",
    "\n",
    "df = remove_nan_values(solo_copy)\n",
    "df\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
